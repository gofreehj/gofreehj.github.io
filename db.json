{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/affix.js","path":"js/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/exturl.js","path":"js/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/js.cookie.js","path":"js/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/post-details.js","path":"js/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/scroll-cookie.js","path":"js/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/scrollspy.js","path":"js/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/images/header-background.jpg","path":"images/header-background.jpg","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"030db4fc07c3e71ede6b88d266ec56b864cc8cc5","modified":1562656786921},{"_id":"themes/next/.DS_Store","hash":"487ae5c80846383da4b356705eebff9886775d4b","modified":1562754426240},{"_id":"themes/next/.all-contributorsrc","hash":"50392ce05ce8f4cb3a3ee6c65eeb05d3533c16b2","modified":1562468952019},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1562468952020},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1562468952020},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1562468952020},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1562468952020},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1562468952024},{"_id":"themes/next/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1562468952024},{"_id":"themes/next/.gitignore","hash":"b80cec1d5e6a73d1cec382aad8046d1352a1e963","modified":1562468952024},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1562468952024},{"_id":"themes/next/README.md","hash":"bae0d85db8338f39f7fe9ba254d9026e7de51cf2","modified":1562468952025},{"_id":"themes/next/bower.json","hash":"e6a80b9ed2d618d1cca5781952c67167a7cfac07","modified":1562468952026},{"_id":"themes/next/_config.yml","hash":"6db4f1931bfee2b157605638f7833e940b127727","modified":1562722757308},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1562468952026},{"_id":"themes/next/gulpfile.coffee","hash":"23bd9587807edc4dbecb5c5a29ab96ade24458b5","modified":1562468952031},{"_id":"themes/next/package.json","hash":"5870ec2b6d2159a57d84c67bad0a535b76398d5a","modified":1562468952056},{"_id":"source/_posts/.DS_Store","hash":"543373fc2f7b7f780cb38e08614574d7309284e9","modified":1562656666244},{"_id":"source/about/index.md","hash":"cb02958ba986dd6bf4bf79d828c5d7baeaac56e6","modified":1562565858512},{"_id":"source/categories/index.md","hash":"503b000acc40beb6933e6db58ab49e1ecf00175a","modified":1562565817878},{"_id":"source/tags/index.md","hash":"10fb8cffa7c791a9ffb41940d1af39096ec4b404","modified":1562565847234},{"_id":"themes/next/.git/FETCH_HEAD","hash":"a126fdbb3b85e80a5934eb70ff69ecd48eb5612d","modified":1562687286796},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1562468952012},{"_id":"themes/next/.git/ORIG_HEAD","hash":"3b87d0a5aac1ecf31e0f85364cdca99e11d5aa7c","modified":1562687286808},{"_id":"themes/next/.git/config","hash":"e2ca9fa6f115d4406d24bf0df53fc26ce13e0c9b","modified":1562468952014},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1562468839867},{"_id":"themes/next/.git/index","hash":"37abe37ede8bc0b3dff9732ea3db1e847d95c701","modified":1562468952111},{"_id":"themes/next/.git/packed-refs","hash":"994daac6d9a306583b06acc2062ed9528eebbaa9","modified":1562468952011},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"c149f003d03501565e7688915cd8f2e99fbf8f42","modified":1562468952021},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"7ce6cdc8adcbfda68fcbcc54c8b9fd3434a37993","modified":1562468952021},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1562468952021},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"3239625bb2573e61f7bcce27a74882a9ff7021e9","modified":1562468952022},{"_id":"themes/next/.github/auto_assign.yml","hash":"cb68a1dca1c4623448c2ca899614a9f21df1b036","modified":1562468952022},{"_id":"themes/next/.github/config.yml","hash":"8a5cbf5aa9529390fe0a782758aca9c3a02f9dcf","modified":1562468952022},{"_id":"themes/next/.github/eslint-disable-bot.yml","hash":"16541fb7b80f5ab90135db96285badb63c4d7d3e","modified":1562468952022},{"_id":"themes/next/.github/lock.yml","hash":"585d2c471047be320aa62f2b74dad797bf09c530","modified":1562468952023},{"_id":"themes/next/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1562468952023},{"_id":"themes/next/.github/release-drafter.yml","hash":"c9fdbbdf712327a8ae1ed5972973a75802e245bc","modified":1562468952023},{"_id":"themes/next/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1562468952023},{"_id":"themes/next/.github/stale.yml","hash":"41bf97ee86b8940a0b2e754499ec77fd2b44b717","modified":1562468952023},{"_id":"themes/next/.github/weekly-digest.yml","hash":"404e4ccb7fcd6587bc9b0247a7a7ff256d21f2cb","modified":1562468952024},{"_id":"themes/next/.github/topissuebot.yml","hash":"10665bf2b5aba351725715c71e94ad183a0e8f18","modified":1562468952024},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1562468952027},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"0d2f22ea09dd1ef63c66164e048d8239d2ccb2b8","modified":1562468952027},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1562468952027},{"_id":"themes/next/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1562468952027},{"_id":"themes/next/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1562468952027},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"212a36d57495990b5f56e46ca8dce1d76c199660","modified":1562468952028},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1562468952028},{"_id":"themes/next/docs/MATH.md","hash":"026d2cff73c22a30ea39c50783557ff4913aceac","modified":1562468952028},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1562468952028},{"_id":"themes/next/languages/de.yml","hash":"88dcfa3e53cef1b7f858f98ca9f980179169ae4c","modified":1562468952031},{"_id":"themes/next/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1562468952032},{"_id":"themes/next/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1562468952032},{"_id":"themes/next/languages/es.yml","hash":"a5203c7bbae70bc40f2ee526f9e8105ca9be851e","modified":1562468952032},{"_id":"themes/next/languages/fa.yml","hash":"cc1f3a13e020e2cc666ddc57aaebc4c1ebd669d0","modified":1562468952032},{"_id":"themes/next/languages/fr.yml","hash":"c1e2e892c678920854e1f3df409118398523849e","modified":1562468952032},{"_id":"themes/next/languages/hu.yml","hash":"3b4c10c86a228da70dc4b1a1784a6f942e186032","modified":1562468952033},{"_id":"themes/next/languages/id.yml","hash":"3a9f4485e6801e0e6fae749133a52e3797760795","modified":1562468952033},{"_id":"themes/next/languages/it.yml","hash":"28ff9197a3d21e838e33bb026d8adb544320cb1a","modified":1562468952033},{"_id":"themes/next/languages/ja.yml","hash":"8f85a6500716191159f16c7f484ba61ddd16eeb6","modified":1562468952033},{"_id":"themes/next/languages/ko.yml","hash":"1df31bf037bcb6868a4bd60c49ff55eec5b8167f","modified":1562468952034},{"_id":"themes/next/languages/nl.yml","hash":"6f4a339ecc67a140f3f9c7bec369cbda6b45afd7","modified":1562468952034},{"_id":"themes/next/languages/pt-BR.yml","hash":"301a0535df5de7b585c7c9752053c41c6ef26f9b","modified":1562468952034},{"_id":"themes/next/languages/pt.yml","hash":"6d87701443e33a13574049e613f064f1eb250c95","modified":1562468952034},{"_id":"themes/next/languages/ru.yml","hash":"93872ac01074159566ee3e1738eea6e9216bab8e","modified":1562468952035},{"_id":"themes/next/languages/tr.yml","hash":"5489606e6c40c0b226a3414c8e5037aac965211d","modified":1562468952035},{"_id":"themes/next/languages/uk.yml","hash":"765ba405778f07d7ec3713606568852b04e1a862","modified":1562468952035},{"_id":"themes/next/languages/vi.yml","hash":"6a812db8606498980cd64f001e9ef2f50e124809","modified":1562468952036},{"_id":"themes/next/languages/zh-CN.yml","hash":"f311ad2cc2edba144764c36c0035b6ed0d356a53","modified":1562468952036},{"_id":"themes/next/languages/zh-HK.yml","hash":"7a5e47f561d4b6e132f7f3b09676afbf8520264e","modified":1562468952036},{"_id":"themes/next/languages/zh-TW.yml","hash":"3f3674cac8f47a9a509a7557ea1557bbfbd027e8","modified":1562468952036},{"_id":"themes/next/layout/_layout.swig","hash":"74701fcf2303d59400587436ab4c244e04df7ad9","modified":1562468952037},{"_id":"themes/next/layout/archive.swig","hash":"7e8f3a41a68e912f2b2aaba905d314306ccaf794","modified":1562468952055},{"_id":"themes/next/layout/category.swig","hash":"dda0e6b2139decaf5e865d22ec9d45fdb615a703","modified":1562468952055},{"_id":"themes/next/layout/index.swig","hash":"9b4733d037c360e8504645b1d6c6dd17817c9d7b","modified":1562468952055},{"_id":"themes/next/layout/page.swig","hash":"29c64c7031aaf276d3d11cdf2e95025996fd6eed","modified":1562468952056},{"_id":"themes/next/layout/post.swig","hash":"f74929fd792541916eb25c2addfb35431be071ba","modified":1562468952056},{"_id":"themes/next/layout/schedule.swig","hash":"3268dd3d90d8b0e142cfa1a2ebb23355baeda148","modified":1562468952056},{"_id":"themes/next/layout/tag.swig","hash":"a6be69a90924c9d2f4d90fb4867234859bd2c2e9","modified":1562468952056},{"_id":"themes/next/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1562468952057},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1562468952058},{"_id":"themes/next/source/.DS_Store","hash":"bc532c0a026687f5a626bfcea7ad7ec2a41ce6fc","modified":1562754426237},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1562468952110},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1562468952111},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1562468952111},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562468952081},{"_id":"source/_posts/Hive/.DS_Store","hash":"c0246501f635bb99eae8fd6a6458be82f5bd6105","modified":1562659613088},{"_id":"source/_posts/Hive/collect.md","hash":"3cc82e3ba82ba561e31ae5d7463c9c9024e8495b","modified":1562593297125},{"_id":"source/_posts/Hive/Hive SQL 的编译过程.md","hash":"16249ff0a12d0dce871ad9e29eab6d2ac6de9900","modified":1562932002031},{"_id":"source/_posts/Hive/join.md","hash":"3f1d76d065923e7841aa686a3e2fcab4544b21e4","modified":1562658549822},{"_id":"source/_posts/Hive/join_task.md","hash":"abf55cd808baa2c2e832a7e497e5c769e00f385a","modified":1562658469230},{"_id":"source/_posts/Hive/over.md","hash":"92f9dfb869f60b1b690a177d4dee5cccfeba9404","modified":1562921407464},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1562468839868},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1562468839867},{"_id":"themes/next/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1562468839868},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1562468839868},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1562468839868},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1562468839867},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1562468839867},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1562468839869},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1562468839868},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1562468839868},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1562468839869},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1562468839867},{"_id":"themes/next/.git/logs/HEAD","hash":"5f89e425f7012c93a473fb36ecc8cc1cb85a24bb","modified":1562468952013},{"_id":"themes/next/.github/ISSUE_TEMPLATE/custom-issue-template.md","hash":"57e1e06e845193e80c7df4a4454af28352526f7a","modified":1562468952021},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"c37a60580c901c79ccb22564b228a46e06207445","modified":1562468952021},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"07c423cce4157b8e2dbf60907ccbf3f18c4cf98a","modified":1562468952022},{"_id":"themes/next/.github/ISSUE_TEMPLATE/non-english.md","hash":"0b0727ff4d5180ae67f930fb4f8e9488e33eda9f","modified":1562468952022},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1562468952028},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1562468952028},{"_id":"themes/next/docs/ru/README.md","hash":"4d7ef717d0b57288e606996ee56c20ffd59d5a99","modified":1562468952029},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1562468952029},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"caa624092175d44e3d3a8c6ca23922718da2354c","modified":1562468952029},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"b218e30df4126b6adc87684775ac4c86ea7f7958","modified":1562468952029},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"650fcb9135b6f09d48e866c19e0dbccd831367f1","modified":1562468952029},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1562468952030},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1562468952030},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"115ffbde2b3ce01ef1f8c2b3833e6f6794650132","modified":1562468952030},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"5da70d7fa0c988a66a469b9795d33d471a4a4433","modified":1562468952031},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"83feca62190abcca0332915ffe0eefe582573085","modified":1562468952030},{"_id":"themes/next/docs/zh-CN/README.md","hash":"cdd7a8bdcf4a83ff4c74ee6c95c6bcc0b8c1831c","modified":1562468952031},{"_id":"themes/next/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1562468952037},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1562468952037},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1562468952037},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"89b0a0e64637bf5b0cfea0a23642df3d95eedfa4","modified":1562468952038},{"_id":"themes/next/layout/_macro/post.swig","hash":"7920866b88d2c6c2ad0e2e7201e58d37fb0d7cff","modified":1562468952039},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"480d93619479dcfcbec6906803bb38b2dfbeae53","modified":1562468952039},{"_id":"themes/next/layout/_partials/comments.swig","hash":"784356dd77fe96ea1bc4cb0008e2b40de71bf2f0","modified":1562468952039},{"_id":"themes/next/layout/_partials/footer.swig","hash":"589f545333e21a8c7823bce89ab45cf1eb7db6e2","modified":1562468952040},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"6357537ac0bb114aed4d61bafb39e6690a413697","modified":1562468952040},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1562468952042},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"06dac109504812b63766a80ede9ddacbd42d227d","modified":1562468952042},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"50be1762f60222379a8bef5e42ab1a0f3872b7ff","modified":1562468952045},{"_id":"themes/next/layout/_scripts/exturl.swig","hash":"61ae10d41f67ece004a025077fdb28724af05090","modified":1562468952045},{"_id":"themes/next/layout/_scripts/next-boot.swig","hash":"012e3ece672cc3b13d5e032139f328d3426d7d65","modified":1562468952045},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"edaff4766e0c05fd5c889d9dd32884d376bef9d9","modified":1562468952045},{"_id":"themes/next/layout/_scripts/scroll-cookie.swig","hash":"ccd13d73429ef91ef5e8b7d9fa43c8188facdf41","modified":1562468952047},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9cd491b8ff2dc9d6976cd9e89c4e56678e3bcefa","modified":1562468952047},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"4ccf2abbfd070874265b0436a3eff21f7c998dfb","modified":1562468952050},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"10b61a8bac671e375916a4d234c120117098a78f","modified":1562468952050},{"_id":"themes/next/layout/_third-party/chatra.swig","hash":"aa0893cddc803bd3fd34ab78d7d003bd86be86b6","modified":1562468952050},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"a7126355227236f9433615edfd89e86fd51ed676","modified":1562468952052},{"_id":"themes/next/layout/_third-party/mermaid.swig","hash":"d6e6ddda836bd9e2e8d9767a910c7d3280080e81","modified":1562468952053},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"2c4a66be4677d3e4dec3f169ac8a769098dad1fe","modified":1562468952053},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"c28f9dc96ab735daeb7f599f86470aa5a83c03cf","modified":1562468952053},{"_id":"themes/next/layout/_third-party/pdf.swig","hash":"810a9b2a6059f46c4a2ddb178f1eaa4c5e23750b","modified":1562468952053},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"7757bd285732e857996b99af9d917953589fac5e","modified":1562468952053},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"cbe40cb67dad15ade967b0f396c1a95b6871f76a","modified":1562468952054},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"2398e5cd0cb466953b6e7a42c2b2caddebf3c348","modified":1562468952054},{"_id":"themes/next/layout/_third-party/tidio.swig","hash":"912368c41de675f458b267a49a99ae3e7e420ebb","modified":1562468952055},{"_id":"themes/next/scripts/filters/exturl.js","hash":"79ad823ca803cb00e0bfc648aa6c9d59711e0519","modified":1562468952057},{"_id":"themes/next/scripts/helpers/engine.js","hash":"60eb1554456d9d0e5afc4a2d16f1580a0aa02da8","modified":1562468952057},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"799a042bbf497a4c7a2981aa2014ff28fa1bb382","modified":1562468952057},{"_id":"themes/next/scripts/tags/button.js","hash":"f3b4f7ae7e58072bbf410d950a99a0b53cbc866d","modified":1562468952058},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f13430d9d1c9773b390787c2f046bb1f12a79878","modified":1562468952058},{"_id":"themes/next/scripts/tags/exturl.js","hash":"d605918cf819887e9555212dbe12da97fd887a0b","modified":1562468952058},{"_id":"themes/next/scripts/tags/full-image.js","hash":"fcb41c1c81560ed49dc4024654388a28ee7d32b0","modified":1562468952059},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"8fc05f22b88553bc1d96e0c925799cd97920fc6a","modified":1562468952059},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"5db59d56f4f4082382bf1c16722e6c383892b0c5","modified":1562468952059},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1562468952060},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1562468952060},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1562468952060},{"_id":"themes/next/scripts/tags/pdf.js","hash":"f780cc72bff91d2720626e7af69eed25e9c12a29","modified":1562468952061},{"_id":"themes/next/scripts/tags/tabs.js","hash":"00ca6340d4fe0ccdae7525373e4729117775bbfa","modified":1562468952061},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1562468952061},{"_id":"themes/next/source/css/.DS_Store","hash":"bcdaf2bf0551873251c8ffbc8bcbaa3c61fc966a","modified":1562754426241},{"_id":"themes/next/source/css/main.styl","hash":"e010ec8ac73268a0f137204c89e0080ab8d59b3d","modified":1562468952081},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1562468952082},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1562468952082},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1562468952082},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1562468952082},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1562468952082},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1562468952083},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1562468952083},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1562468952083},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1562468952083},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1562468952084},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1562468952084},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1562468952084},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1562468952084},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1562468952084},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1562468952084},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1562468952085},{"_id":"themes/next/source/images/searchicon.png","hash":"025d64ba0160a3a2257dd2b3032b5f7c9dd9b82b","modified":1562468952085},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1562468952085},{"_id":"themes/next/source/js/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1562468952085},{"_id":"themes/next/source/js/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1562468952086},{"_id":"themes/next/source/js/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1562468952086},{"_id":"themes/next/source/js/js.cookie.js","hash":"e0afce539f1fb81d59e3c6f0a68d736e2fb45d93","modified":1562468952086},{"_id":"themes/next/source/js/motion.js","hash":"a16bc0b701646bf6653484675f4d5dc0f892d184","modified":1562468952086},{"_id":"themes/next/source/js/next-boot.js","hash":"e0615efab5f81ba0fd39c0527eac31144deac7ce","modified":1562468952086},{"_id":"themes/next/source/js/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1562468952087},{"_id":"themes/next/source/js/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1562468952087},{"_id":"themes/next/source/js/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1562468952087},{"_id":"themes/next/source/js/utils.js","hash":"81913c5f75d0949443833cf4269ad63bd7f9be6f","modified":1562468952088},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562468952076},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562468952076},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562468952076},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1562468952080},{"_id":"themes/next/.git/objects/pack/pack-be773cb15aff0e6f31a58b324ec6cc02547d7838.idx","hash":"78bd0dd9ecdbf4792ecf3d92f08e27ea55d4f6be","modified":1562687286785},{"_id":"themes/next/.git/refs/heads/master","hash":"3b87d0a5aac1ecf31e0f85364cdca99e11d5aa7c","modified":1562468952013},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1562468952038},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"9257da95bd032bb3bd1da670e302fd2c7d5610b6","modified":1562468952038},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"fc6bafc8c633afadc538c5afa5620ea2a1cdcb84","modified":1562468952040},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"02bb5748e8540b024e7f4008a9e640890b45280f","modified":1562468952040},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1562468952041},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"f537846ace6a1afdacf122848dd01a32ceb66006","modified":1562468952041},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"648bf7eda66629592cb915c4004534b3913cbc22","modified":1562468952041},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"71af31fea5913fd30c233e555ef13cf2c9768f72","modified":1562468952041},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"5adc60100e129c1d0307bdcaa0c7b8e8375a6ea4","modified":1562468952042},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"0fa4fadb39467b01cede49f21b22e86b1a2da805","modified":1562468952042},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"2940df694fff28e8bf71b6546b4162f1e38227db","modified":1562468952042},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"3615db591dd910fb9fa96542734c7ec0ef05019c","modified":1562468952043},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"eea95b785c9c36d28e1839619793f66e89773bee","modified":1562468952043},{"_id":"themes/next/layout/_partials/post/reward.swig","hash":"d44f025eb93c99ddf90202d8293ccf80689a00c7","modified":1562468952043},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"ef11b5be5bfb2f0affe82cf521c002b37fef9819","modified":1562468952043},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"15b542f5b06b7532234af367340b9ed9fcebb0ac","modified":1562468952044},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"6f181cc188ecbe5e607fd989756e470d4cb9765d","modified":1562468952044},{"_id":"themes/next/layout/_partials/share/likely.swig","hash":"b45e934d24d76ec6b6a790e92bdb3d56186b0e2a","modified":1562468952045},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"d9fe715fee716f78c7976c4e8838da71439ee0e0","modified":1562468952043},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"7d1693416a5dc098f4723a53da2e2d1fc2d6e075","modified":1562468952044},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1562468952044},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a5587bd1f60d35e58618576cec45e662aa44ea1f","modified":1562468952044},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"5b05f165547391bf231e52f56f3d925efc09bc44","modified":1562468952046},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"ffc8e8836714ea79abeb77b75859634615652877","modified":1562468952046},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"108b157fbd1ac3baaf19ae87234fa8728ab79556","modified":1562468952046},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"0097e45e7b671f8006b8b2d3c4f95cacc76a983c","modified":1562468952046},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"e42604fbb17648484e5f12afe230d826de089388","modified":1562468952047},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"a5723950c343d220270bfd27bd30050eda6c3fb3","modified":1562468952047},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"798d67e4a736613ab899eabe6529091bbcda7850","modified":1562468952047},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"591b2ccd9713ccb922b9fcf5e278b6de9c5ec30b","modified":1562468952047},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"8eadb929c9e50e58502ccad2dc2657746f8c592a","modified":1562468952048},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"08cd47ef8572121b7811342d3c9a84a338a18191","modified":1562468952048},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"3d01fa6edc0ad73f81813613f2e8a610777f1852","modified":1562468952048},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"fae69a0e1a1d42f7bb44e594a29857d94594698b","modified":1562468952048},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"9fa1ca7059243197d8fbbd35108c36629a254570","modified":1562468952048},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"4a966b7ffe2d80ff1b3dd0fd14b355766dc5c70f","modified":1562468952048},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"438c6f5e6665d72f4ea7ee206011d669246f6102","modified":1562468952049},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"a09d2af2a8470555eeb265b0eb14dc678079e870","modified":1562468952049},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"f240a50cd9b627620d9a374a29cf95f0c5e99d7c","modified":1562468952049},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"92e04a2b9e0c3df594bc22235d1894e5ad458dfc","modified":1562468952049},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"0dd5b315d1da55dbfc10f51a1f8952f72eba2720","modified":1562468952050},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"3533167c4295637b91d90f3bae7c651cd128bb6e","modified":1562468952050},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"074a995cd630f56fc4a3135173515c86f2cb34b6","modified":1562468952051},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"1a00b1b78c429721d6477c2d8f6f68f005285cc8","modified":1562468952051},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"e8f91c571ceb4b80aafebc4d36b89fb41b1ae040","modified":1562468952051},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"53a59cba82cad49f15a90e1a18007aaac525bddd","modified":1562468952051},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"40bab84a4a7a368fa31f0f8ce49af6ec3e5983c9","modified":1562468952052},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"15a4d60d3ecc59db2f23629477f8e7b8324981ed","modified":1562468952052},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"a7e304b05a44279d3e4f611908d7faef9dc14d7c","modified":1562468952052},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"c2cb2f384bc30d31cdccf9794a729c03e687b45c","modified":1562468952052},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"601774d8672577aefbcefac82c94b01f0338da31","modified":1562468952053},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"0a13dfd2de52a96901039098c6fc7b515edfc50b","modified":1562468952054},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"ea94aa85034c6d1b6bb865aecea55c73f8a14501","modified":1562468952054},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"b3eaab6a269aa3fcbafe24fd06f0c9206dc12716","modified":1562468952055},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"69620e1fb28b5d85e5123751271d4dd2b7119914","modified":1562723092454},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1562468952076},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"2e8fb29aa92325df39054b5450757858c6cebc41","modified":1562468952076},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"2036bbb73afd43251982ce824f06c6e88d35a2ef","modified":1562468952076},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"a8aa41625b94cf17a7f473ed10dcbe683b1db705","modified":1562716517601},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1562468952080},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"fc15e277d1504532a09b7b1bd31f900ad95ec4b8","modified":1562468952081},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"a66374cb9a1b72a8e0280ea2d0f7fc6c002133b7","modified":1562722172439},{"_id":"themes/next/source/js/schemes/muse.js","hash":"e9bfa6b343b67625f58757efce46ccdaac8f308c","modified":1562468952087},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"9eb63cba0327d3d11b6cbfcbe40b88e97a8378a3","modified":1562468952087},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1562468952088},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1562468952088},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1562468952088},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1562468952088},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1562468952089},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1562468952110},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1562468952110},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1562468952110},{"_id":"themes/next/.git/objects/pack/pack-be773cb15aff0e6f31a58b324ec6cc02547d7838.pack","hash":"3fccf6f16e0fa1b7a9317ddfb719e677283f5fe8","modified":1562687286785},{"_id":"themes/next/source/css/_variables/base.styl","hash":"640f25a63770af5566ccc9cec79c40a4f1c0b29e","modified":1562468952081},{"_id":"themes/next/source/lib/jquery/index.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1562468952107},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"5f89e425f7012c93a473fb36ecc8cc1cb85a24bb","modified":1562468952013},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1562468952012},{"_id":"themes/next/.git/refs/remotes/origin/master","hash":"3ff711436292ce841c8ad40e5f65ce6087f357e5","modified":1562687286794},{"_id":"themes/next/.git/refs/remotes/origin/readme","hash":"8aa15097c2062f17e458d581933dd9112daa8f55","modified":1562687286795},{"_id":"themes/next/.git/refs/remotes/origin/reward","hash":"2fee0e89b04b2dee8257e75601392666d77bb5e2","modified":1562687286795},{"_id":"themes/next/.git/refs/remotes/origin/three","hash":"cf4586388f473a05f8dfcedf7ae55cb42d26d5cb","modified":1562687286796},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"fe5ff961b86004a306778c7d33a85b32e5e00e48","modified":1562468952061},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"c8b3225396cb444d8baeb94bac78e5216b992a81","modified":1562468952062},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"b98c65006e2546fbf3870c16fbbcbc009dbaab15","modified":1562468952062},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1562468952062},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"9d71f34fa13a41b8c8cd2fbdf3fdea608385277c","modified":1562468952062},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"ce826aedf42b9eca424a044452f5d193866726a6","modified":1562468952066},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"ce2aae8f3ed8ceac3a2417e0481044cf69c788aa","modified":1562468952069},{"_id":"themes/next/source/css/_common/components/scrollbar.styl","hash":"d7b8bcf2a6031296c84bb4f4ecfb037af01d2d82","modified":1562468952069},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"7e51ea64611ab5d678c112b4688d4db4fd2737e2","modified":1562468952074},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"6d900b4159eeb869196a619602578bf4d83a117b","modified":1562468952074},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"8e0740a9ad349ce5555122325da872923135a698","modified":1562468952075},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"9a190ef2f49bdbf69604b48ad1dc7197895ee9b6","modified":1562468952075},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"7ffde343bdf10add1f052f3c4308a15180eb4404","modified":1562468952075},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1562468952075},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"33456264a74d1bba38264d14713544d67d003733","modified":1562468952075},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"9a2d298dbdcbfd758518fd74b63897bc80ce15a5","modified":1562468952077},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1562468952077},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"24230e46fc9fb7b8551f97bb36e9bc1f7423098e","modified":1562468952077},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1562468952077},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"75d2d92af070eb10273558b2436972d3f12b361c","modified":1562468952077},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fa33213aceed7bf4bf25437ca9c1a00f7734ae65","modified":1562468952077},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"7359880e8d85312861fe0871f58b662e627dae0c","modified":1562468952078},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a96e46a6ae86c423f932bc2bc78b9f7453e4e4e5","modified":1562468952078},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"6565b4a309325596768d0d32e022c80ef23066cb","modified":1562468952078},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"fc160583f742c94316a0fee05c18468033173534","modified":1562468952078},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"7359880e8d85312861fe0871f58b662e627dae0c","modified":1562468952079},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"1edf4e69d0ec0dc9cefed6c35d3e803e0da4093d","modified":1562468952079},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1562468952079},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"57044a6d19eb418c1c3d28787e82c69efa9e0ca6","modified":1562468952079},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"75737591682a2bafa71db4c03fb79e970ac0e7aa","modified":1562468952079},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"b6dac5bbf20f090cf4b67d156f030d7170dfb39c","modified":1562468952080},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"6400c98a9fd2b9a8502269f33355bd7ab3ff793b","modified":1562468952080},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b5b936dddb7b4de4720cd1e8428b30a2f06d63fb","modified":1562468952080},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"232aedbd44243b3b80c4503c947060d3269c1afc","modified":1562468952080},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1562468952103},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1562468952103},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1562468952104},{"_id":"themes/next/.git/objects/pack/pack-213e8734b96d65c2d6150244ad3b7d2cf6afbd77.idx","hash":"1b61ee8b13825da6f11d4581f0d3f4ae14898163","modified":1562468951994},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1562468952107},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1562468952109},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"5f89e425f7012c93a473fb36ecc8cc1cb85a24bb","modified":1562468952012},{"_id":"themes/next/.git/logs/refs/remotes/origin/master","hash":"876e15674a7dcde674436d76f58bad20fff0caeb","modified":1562687286794},{"_id":"themes/next/.git/logs/refs/remotes/origin/readme","hash":"e1793056bcc421dbf7ae8e3c234d7717e2e928fc","modified":1562687286795},{"_id":"themes/next/.git/logs/refs/remotes/origin/reward","hash":"34ca42c29912ee63a62e323809be216b34a8497d","modified":1562687286795},{"_id":"themes/next/.git/logs/refs/remotes/origin/three","hash":"32e6c1bb1ddeecf59382d5170f9e861d04b47dea","modified":1562687286796},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"4cfeec9434a72d5efc6ca225d3445d084d4590f7","modified":1562468952062},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"ca97f0b6990eef947039faede80c56d9c4381ee1","modified":1562468952063},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"6c4990d375b640ee4551e62c48c1cbe4c3d62212","modified":1562468952063},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1562468952063},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"a410ed529afd46ddf4a96ecf0de6599488716887","modified":1562468952063},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"c0d9e18a9210fdcaf33e488518b3b288eb58c0a1","modified":1562468952063},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"cc6ee18f47f2e1e06df6fa0eadb37079e580fd11","modified":1562468952064},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"71d8d1cc22a2a7627a6db7240f0c4902a14f9bea","modified":1562468952064},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1562468952064},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"352796ec0a0cbbdb45d2351711d136ae6112b757","modified":1562468952064},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"6904fd7ea6455e008d9884558b68254608af9a3c","modified":1562468952065},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"2d142c6f39853916256ad8fc79eb6b85f4001ae8","modified":1562468952065},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1562468952065},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f1d52954b9a5d1ca8e224382349f525e598dd923","modified":1562468952065},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1562468952065},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"80addb9b725e329915c05c27b9fadaf56457a9b3","modified":1562468952066},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"61ca40856e5cacd48e0fa9728fde4605c7dd4c94","modified":1562468952066},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1562468952066},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"6a75bb1f2435f4e895cbbb5abbddf6e8f7257804","modified":1562468952067},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"2356226157e8068b0e9bbe2f7d0f74e1ab49199b","modified":1562468952067},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"c961d37190d9bec58a36306c7e716c4e72c4582f","modified":1562468952067},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"df3c19fd447da6d4a807683345007a41338f9a04","modified":1562468952067},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"0bf899fab331add63f0c8ead31ca3a3db2ad74d9","modified":1562468952067},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"67165cd8836e03c289162b96ef06f8b024afe9af","modified":1562468952067},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"496f931e3a7e313ba8088fb91bb20789cace72c9","modified":1562468952068},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"3f33bb862c2aa993f54987fbb345da067b79b112","modified":1562468952068},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"5440013a081201ca791582db98159dce93ea9e75","modified":1562468952068},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1562468952068},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1562468952068},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"8e058c99dd7d41f0bd34c7c28b6ac9fbb17dcb5e","modified":1562468952069},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"d5c8ffed7f2c701052b7a53abaf5ef437374ea72","modified":1562468952069},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"a6c24393dffbdd94dd5c01cdbec5e180b0bfbbbd","modified":1562468952069},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"a3170630d8e085889a4bdc20eb7f09c5a0479c47","modified":1562468952069},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"707527c9950a7459355c8abcf4751c0964de0bc1","modified":1562468952070},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"cc83816614f21c7e1d8d3f867d547ff7c658cec4","modified":1562468952070},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"c2d9c3b6fbfa65544e6b5a55d3cb2149df04a8a9","modified":1562468952070},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-button.styl","hash":"517d541a80d59ad99a3f648be74891e0c7bc72a8","modified":1562468952070},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"9224b566cd2632f64c1a964e2c786cee93b93286","modified":1562468952070},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"9a3bfc878ca797946815bed23cd6f92b24a16358","modified":1562468952070},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"8a24b56524a388fbabd408ffc8ba9b56eb9e01ce","modified":1562468952071},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"c01609176929590f8f347075a9a12b661acd661e","modified":1562468952071},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"8e5c884fb950937afa350c608545455c87aa6129","modified":1562468952071},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"967fb3a3c6c851b34ec5df2d945dc266ed63d146","modified":1562468952071},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"58ec00eebe68d0eebd2eea435c710063877447df","modified":1562468952071},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"6ec8ea7b11a146777b6b8da0f71f0cc1dbd129df","modified":1562468952071},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"c85df3ecc0b37095cac14114c308e5829c66b5a3","modified":1562468952072},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"d7501ae01fc45fa15b00d1bc5233b9fffa20a3c9","modified":1562468952072},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"21b32840d8b3a14b10770153114778304ba6d1b0","modified":1562468952072},{"_id":"themes/next/source/css/_common/components/tags/pdf.styl","hash":"da8d34729fb6eb0fcb8ee81e67d2be3c02bc1bc4","modified":1562468952072},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"6e4400d6704dee076434726b7a03ac464eb7bcb4","modified":1562468952072},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"cbc0be5a3285b469858ec9ead48e2ea90bd47ae1","modified":1562468952072},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fc58498d4f5081fcf6218e9e18c5bf2328275bef","modified":1562468952073},{"_id":"themes/next/source/css/_common/components/third-party/copy-code.styl","hash":"688ca3eccc26727d050ad098b32b40934719588a","modified":1562468952073},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"ac7753d536341aa824d7bce0332735e838916995","modified":1562468952073},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"9fac89c8146eb2675721a26f528d7d0f8be7debe","modified":1562468952073},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"ef66c0a08e4243a25e41408d70ca66682b8dcea1","modified":1562468952073},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"61466e3e5459960b5802a267751a0c8018918b0b","modified":1562468952074},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"3ae3f3c276d444862033fd3434c632ad0d2f84e6","modified":1562468952074},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"dd44d8ca93ad5d366a96d797a0a8b4f3b46f9a77","modified":1562468952074},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1562468952078},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1562468952078},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1562468952079},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1562468952106},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1562468952105},{"_id":"themes/next/source/images/header-background.jpg","hash":"5d5232dad22f57ff2739170fcca077e7d73424ce","modified":1557254108000},{"_id":"themes/next/.git/objects/pack/pack-213e8734b96d65c2d6150244ad3b7d2cf6afbd77.pack","hash":"827521f5b009cf1306b12813f9454dadd9ca33f7","modified":1562468951992},{"_id":"public/search.xml","hash":"f9c19f374474ccc262c9c7b19a2590f04515cfb3","modified":1562932066864},{"_id":"public/about/index.html","hash":"d7518fff901b85c323d94987ea9b79b29bf80df4","modified":1562932066874},{"_id":"public/categories/index.html","hash":"6a7acfd473c1d7bc075987e734dd4aa56e95a4a7","modified":1562932066874},{"_id":"public/tags/index.html","hash":"a68fba6c94e0e129093c8d73dbbcfee4ae7e1c65","modified":1562932066874},{"_id":"public/2019/07/12/Hive/Hive SQL 的编译过程/index.html","hash":"e821a175ad5dde5392f9a5ec1769f05dc5d9efb6","modified":1562932066874},{"_id":"public/2019/07/09/Hive/join/index.html","hash":"ecde23e99c51cae68feab16be823addaddcd0d77","modified":1562932066874},{"_id":"public/2019/07/09/Hive/join_task/index.html","hash":"f57e4abd256a9108dd6df1ce7ed2cedc860a60d7","modified":1562932066874},{"_id":"public/2019/07/07/Hive/collect/index.html","hash":"e71488875da92ad157348b5602600e91a02bfd33","modified":1562932066874},{"_id":"public/2019/07/07/Hive/over/index.html","hash":"8e16a9b2d560a6f89a225cdf0135391dc6fd5c1e","modified":1562932066874},{"_id":"public/archives/index.html","hash":"d239aaaa775ee131add8b5cd32af3809cb228b84","modified":1562932066874},{"_id":"public/archives/2019/index.html","hash":"adbe142771fd2d599bd3ebf0762a4e734cd04ecb","modified":1562932066875},{"_id":"public/archives/2019/07/index.html","hash":"9ea470673cfc23c741aade24daea74a5a6ba28af","modified":1562932066875},{"_id":"public/categories/Hive/index.html","hash":"b7322ecb1bf8b8d1fe26c6118b8bb5007d1e6a6f","modified":1562932066875},{"_id":"public/tags/sql/index.html","hash":"b4dafce473cf4fc11aa2d1ab66cc4316af225808","modified":1562932066875},{"_id":"public/index.html","hash":"1554031ac49d445b784635ec293becd3943dbf7f","modified":1562932066875},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1562932066878},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1562932066878},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1562932066878},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1562932066878},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1562932066878},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1562932066878},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1562932066878},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1562932066879},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1562932066879},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1562932066879},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1562932066879},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1562932066879},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1562932066879},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1562932066879},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1562932066879},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1562932066879},{"_id":"public/images/searchicon.png","hash":"025d64ba0160a3a2257dd2b3032b5f7c9dd9b82b","modified":1562932066879},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1562932066879},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1562932066879},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1562932066879},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1562932067199},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1562932067202},{"_id":"public/js/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1562932067205},{"_id":"public/js/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1562932067205},{"_id":"public/js/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1562932067205},{"_id":"public/js/js.cookie.js","hash":"e0afce539f1fb81d59e3c6f0a68d736e2fb45d93","modified":1562932067205},{"_id":"public/js/motion.js","hash":"a16bc0b701646bf6653484675f4d5dc0f892d184","modified":1562932067205},{"_id":"public/js/next-boot.js","hash":"e0615efab5f81ba0fd39c0527eac31144deac7ce","modified":1562932067205},{"_id":"public/js/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1562932067205},{"_id":"public/js/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1562932067205},{"_id":"public/js/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1562932067206},{"_id":"public/js/utils.js","hash":"81913c5f75d0949443833cf4269ad63bd7f9be6f","modified":1562932067206},{"_id":"public/js/schemes/muse.js","hash":"e9bfa6b343b67625f58757efce46ccdaac8f308c","modified":1562932067206},{"_id":"public/js/schemes/pisces.js","hash":"9eb63cba0327d3d11b6cbfcbe40b88e97a8378a3","modified":1562932067206},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1562932067206},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1562932067206},{"_id":"public/css/main.css","hash":"8e1dafad2f1daaa767f5344abb9b17b9e200cc22","modified":1562932067206},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1562932067206},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1562932067206},{"_id":"public/lib/jquery/index.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1562932067206},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1562932067206},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1562932067206},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1562932067206},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1562932067206},{"_id":"public/images/header-background.jpg","hash":"5d5232dad22f57ff2739170fcca077e7d73424ce","modified":1562932067218}],"Category":[{"name":"Hive","_id":"cjy01ec5j0005m5873mww3gt3"}],"Data":[],"Page":[{"title":"关于我","date":"2019-07-07T04:50:12.000Z","_content":"Love snow!! ","source":"about/index.md","raw":"---\ntitle: 关于我\ndate: 2019-07-07 12:50:12\n---\nLove snow!! ","updated":"2019-07-08T06:04:18.512Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjy01ec130000m587zlgv1jyn","content":"<p>Love snow!! </p>\n","site":{"data":{}},"length":13,"excerpt":"","more":"<p>Love snow!! </p>\n"},{"title":"分类","date":"2019-07-07T05:05:22.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2019-07-07 13:05:22\ntype: \"categories\"\ncomments: false\n---\n","updated":"2019-07-08T06:03:37.878Z","path":"categories/index.html","layout":"page","_id":"cjy01ec4s0001m5876r8blz1f","content":"","site":{"data":{}},"length":0,"excerpt":"","more":""},{"title":"标签","date":"2019-07-07T05:52:35.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle:  标签\ndate: 2019-07-07 13:52:35\ntype: \"tags\"\ncomments: false\n---\n","updated":"2019-07-08T06:04:07.234Z","path":"tags/index.html","layout":"page","_id":"cjy01ec4t0002m58739b83qli","content":"","site":{"data":{}},"length":0,"excerpt":"","more":""}],"Post":[{"title":"Hive中集合数据类型Struct，Map和Array","date":"2019-07-07T04:33:48.000Z","_content":"\nHive中的列支持使用struct，map和array集合数据类型。下表中的数据类型实际上调用的是内置函数。\n\nHive集合数据类型:\n\n| **数据类型** | **描述**                                                     | **字面语法示例**                 |\n| ------------ | ------------------------------------------------------------ | -------------------------------- |\n| STRUCT       | 数据类型描述字面语法示例和C语言中的struct或者“对象”类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 STRUCT { first STRING , last STRING} ，那么第 1 个元素可以通过字段名.first来引用 | struct('John','Doe')             |\n| MAP          | MAP 是一组键一值对元组集合，使用数组表示法(例如['key']) 可以访问元素。例如，如果某个列的数据类型是 MAP ，其中键 值对是'first' -> 'John' 和'last' -> 'Doe'，那么可以通过字段名['last']获取最后 1 个元素 | map('first','JOIN','last','Doe') |\n| ARRAY        | 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为［'John', 'Doe'] , 那么第 2 个元素可以通过数组名[1]进行引用 | Array('John','Doe')              |\n\n和基本数据类型一样，这些类型的名称同样是保留字。\n\n大多数的关系型数据库并不支持这些集合数据类型，因此使用它们会趋向于破坏标准格式。例如，在传统数据模型中，structs可能需要由多个不同的表拼装而成，表间需要适当地使用外键来进行连接。\n\n破坏标准格式所带来的一个实际问题是会增大数据冗余的风险，进而导致消耗不必要的磁盘空间，还有可能造成数据不一致，因此当数据发生改变时冗余的拷贝数据可能无法进行相应的同步。\n\n然而，在大数据系统中，不遵循标准格式的一个好处就是可以提高更高吞吐量的数据。当处理的数据的数量级是TB或者PB时，以最少的“头部寻址”来从磁盘上扫描数据是非常必要的。按数据进行封装的话可以通过减少寻址次数来提供查询的速度。而如果根据外键关系关联的话则需要进行磁盘间的寻址操作，这样会有非常高的性能消耗。\n\n建表：\n\n```sql\ncreate\n\ttable collect_test\n\t(\n\t\tid INT,\n\t\tname STRING,\n\t\thobby ARRAY < STRING >,                      -- array中元素为String类型\n\t\tfriend MAP < STRING,STRING >,                -- map中键和值均为String类型\n\t\tmark struct < math:int,english:int >         -- Struct中元素为Int类型\n\t)\n\trow format delimited fields terminated by ','  -- 字段之间用','分隔\n\tcollection items terminated by '_'             -- 集合中的元素用'_'分隔\n\tmap keys terminated by ':'                     -- map中键值对之间用':'分隔\n\tlines terminated by '\\n                        -- 行之间用'\\n'分隔 默认一般不指定\n```\n\n2、向表test_set中插入数据\n\n1）对于数据量较大，常用的一种方法是通过文件批量导入的方法，比如我现在要将如下的文本中的数据插入到表中\n\n```sql\n1,xiaoming,basketball_game,xiaohong:yes_xiaohua:no,99_75\n\n1,xiaohong,watch_study,xiaoming:no_xiaohua:not,95_95\n```\n\n可以采用如下语句来实现\n\n```sql\nhive -e \"load data local inpath '/path/data.txt' overwrite into table collect_test\"\n```\n\n2）对于想插入几条数据时，可以采取insert语句来插入数据，比如我们想插入数据\n\n2,xiaohua,basketball_read,xiaoming:no_xiaohong:no,90_90\n可以采用如下语句来实现，分别通过array,str_to_map,named_struct来包装插入的三种集合数据\n\n```sql\nINSERT INTO collect_test\nSELECT\n\t2,\n\t'xiaohua',\n\tarray('basketball', 'read'),\n\tstr_to_map('xiaoming:no,xiaohong:no'),\n\tnamed_struct('math', 90, 'english', 90)\n```\n\n对于集合类型的查询，我们还有一种经常使用的方法，查询语句如下\n\n```sql\nselect\n\tid,\n\tname,\n\thobby[0], \t\t\t\t\t-- 查询第一个hobby\n\tfriend['xiaohong'], -- 查询map键为xiaohong的value\n\tmark.math \t\t\t\t\t-- 查询struct中math的值\nfrom\n\ttest_set\nwhere\n\tname = 'xiaoming'\n```\n\n----\n\n参考：\n\n1.https://blog.csdn.net/qq_41973536/article/details/81627918\n\n2.https://blog.csdn.net/u014414323/article/details/83616361","source":"_posts/Hive/collect.md","raw":"---\ntitle: Hive中集合数据类型Struct，Map和Array\ndate: 2019-07-07 12:33:48\ntags: \n    - sql\ncategories: \n    - Hive\n---\n\nHive中的列支持使用struct，map和array集合数据类型。下表中的数据类型实际上调用的是内置函数。\n\nHive集合数据类型:\n\n| **数据类型** | **描述**                                                     | **字面语法示例**                 |\n| ------------ | ------------------------------------------------------------ | -------------------------------- |\n| STRUCT       | 数据类型描述字面语法示例和C语言中的struct或者“对象”类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 STRUCT { first STRING , last STRING} ，那么第 1 个元素可以通过字段名.first来引用 | struct('John','Doe')             |\n| MAP          | MAP 是一组键一值对元组集合，使用数组表示法(例如['key']) 可以访问元素。例如，如果某个列的数据类型是 MAP ，其中键 值对是'first' -> 'John' 和'last' -> 'Doe'，那么可以通过字段名['last']获取最后 1 个元素 | map('first','JOIN','last','Doe') |\n| ARRAY        | 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为［'John', 'Doe'] , 那么第 2 个元素可以通过数组名[1]进行引用 | Array('John','Doe')              |\n\n和基本数据类型一样，这些类型的名称同样是保留字。\n\n大多数的关系型数据库并不支持这些集合数据类型，因此使用它们会趋向于破坏标准格式。例如，在传统数据模型中，structs可能需要由多个不同的表拼装而成，表间需要适当地使用外键来进行连接。\n\n破坏标准格式所带来的一个实际问题是会增大数据冗余的风险，进而导致消耗不必要的磁盘空间，还有可能造成数据不一致，因此当数据发生改变时冗余的拷贝数据可能无法进行相应的同步。\n\n然而，在大数据系统中，不遵循标准格式的一个好处就是可以提高更高吞吐量的数据。当处理的数据的数量级是TB或者PB时，以最少的“头部寻址”来从磁盘上扫描数据是非常必要的。按数据进行封装的话可以通过减少寻址次数来提供查询的速度。而如果根据外键关系关联的话则需要进行磁盘间的寻址操作，这样会有非常高的性能消耗。\n\n建表：\n\n```sql\ncreate\n\ttable collect_test\n\t(\n\t\tid INT,\n\t\tname STRING,\n\t\thobby ARRAY < STRING >,                      -- array中元素为String类型\n\t\tfriend MAP < STRING,STRING >,                -- map中键和值均为String类型\n\t\tmark struct < math:int,english:int >         -- Struct中元素为Int类型\n\t)\n\trow format delimited fields terminated by ','  -- 字段之间用','分隔\n\tcollection items terminated by '_'             -- 集合中的元素用'_'分隔\n\tmap keys terminated by ':'                     -- map中键值对之间用':'分隔\n\tlines terminated by '\\n                        -- 行之间用'\\n'分隔 默认一般不指定\n```\n\n2、向表test_set中插入数据\n\n1）对于数据量较大，常用的一种方法是通过文件批量导入的方法，比如我现在要将如下的文本中的数据插入到表中\n\n```sql\n1,xiaoming,basketball_game,xiaohong:yes_xiaohua:no,99_75\n\n1,xiaohong,watch_study,xiaoming:no_xiaohua:not,95_95\n```\n\n可以采用如下语句来实现\n\n```sql\nhive -e \"load data local inpath '/path/data.txt' overwrite into table collect_test\"\n```\n\n2）对于想插入几条数据时，可以采取insert语句来插入数据，比如我们想插入数据\n\n2,xiaohua,basketball_read,xiaoming:no_xiaohong:no,90_90\n可以采用如下语句来实现，分别通过array,str_to_map,named_struct来包装插入的三种集合数据\n\n```sql\nINSERT INTO collect_test\nSELECT\n\t2,\n\t'xiaohua',\n\tarray('basketball', 'read'),\n\tstr_to_map('xiaoming:no,xiaohong:no'),\n\tnamed_struct('math', 90, 'english', 90)\n```\n\n对于集合类型的查询，我们还有一种经常使用的方法，查询语句如下\n\n```sql\nselect\n\tid,\n\tname,\n\thobby[0], \t\t\t\t\t-- 查询第一个hobby\n\tfriend['xiaohong'], -- 查询map键为xiaohong的value\n\tmark.math \t\t\t\t\t-- 查询struct中math的值\nfrom\n\ttest_set\nwhere\n\tname = 'xiaoming'\n```\n\n----\n\n参考：\n\n1.https://blog.csdn.net/qq_41973536/article/details/81627918\n\n2.https://blog.csdn.net/u014414323/article/details/83616361","slug":"Hive/collect","published":1,"updated":"2019-07-08T13:41:37.125Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy01ec5f0003m58781hbbf8i","content":"<p>Hive中的列支持使用struct，map和array集合数据类型。下表中的数据类型实际上调用的是内置函数。</p>\n<p>Hive集合数据类型:</p>\n<table>\n<thead>\n<tr>\n<th><strong>数据类型</strong></th>\n<th><strong>描述</strong></th>\n<th><strong>字面语法示例</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>STRUCT</td>\n<td>数据类型描述字面语法示例和C语言中的struct或者“对象”类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 STRUCT { first STRING , last STRING} ，那么第 1 个元素可以通过字段名.first来引用</td>\n<td>struct(‘John’,’Doe’)</td>\n</tr>\n<tr>\n<td>MAP</td>\n<td>MAP 是一组键一值对元组集合，使用数组表示法(例如[‘key’]) 可以访问元素。例如，如果某个列的数据类型是 MAP ，其中键 值对是’first’ -&gt; ‘John’ 和’last’ -&gt; ‘Doe’，那么可以通过字段名[‘last’]获取最后 1 个元素</td>\n<td>map(‘first’,’JOIN’,’last’,’Doe’)</td>\n</tr>\n<tr>\n<td>ARRAY</td>\n<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为［’John’, ‘Doe’] , 那么第 2 个元素可以通过数组名[1]进行引用</td>\n<td>Array(‘John’,’Doe’)</td>\n</tr>\n</tbody></table>\n<p>和基本数据类型一样，这些类型的名称同样是保留字。</p>\n<p>大多数的关系型数据库并不支持这些集合数据类型，因此使用它们会趋向于破坏标准格式。例如，在传统数据模型中，structs可能需要由多个不同的表拼装而成，表间需要适当地使用外键来进行连接。</p>\n<p>破坏标准格式所带来的一个实际问题是会增大数据冗余的风险，进而导致消耗不必要的磁盘空间，还有可能造成数据不一致，因此当数据发生改变时冗余的拷贝数据可能无法进行相应的同步。</p>\n<p>然而，在大数据系统中，不遵循标准格式的一个好处就是可以提高更高吞吐量的数据。当处理的数据的数量级是TB或者PB时，以最少的“头部寻址”来从磁盘上扫描数据是非常必要的。按数据进行封装的话可以通过减少寻址次数来提供查询的速度。而如果根据外键关系关联的话则需要进行磁盘间的寻址操作，这样会有非常高的性能消耗。</p>\n<p>建表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">create</span></span><br><span class=\"line\">\t<span class=\"keyword\">table</span> collect_test</span><br><span class=\"line\">\t(</span><br><span class=\"line\">\t\t<span class=\"keyword\">id</span> <span class=\"built_in\">INT</span>,</span><br><span class=\"line\">\t\t<span class=\"keyword\">name</span> <span class=\"keyword\">STRING</span>,</span><br><span class=\"line\">\t\thobby <span class=\"built_in\">ARRAY</span> &lt; <span class=\"keyword\">STRING</span> &gt;,                      <span class=\"comment\">-- array中元素为String类型</span></span><br><span class=\"line\">\t\tfriend <span class=\"keyword\">MAP</span> &lt; <span class=\"keyword\">STRING</span>,<span class=\"keyword\">STRING</span> &gt;,                <span class=\"comment\">-- map中键和值均为String类型</span></span><br><span class=\"line\">\t\tmark <span class=\"keyword\">struct</span> &lt; math:<span class=\"built_in\">int</span>,english:<span class=\"built_in\">int</span> &gt;         <span class=\"comment\">-- Struct中元素为Int类型</span></span><br><span class=\"line\">\t)</span><br><span class=\"line\">\t<span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>  <span class=\"comment\">-- 字段之间用','分隔</span></span><br><span class=\"line\">\tcollection items <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">'_'</span>             <span class=\"comment\">-- 集合中的元素用'_'分隔</span></span><br><span class=\"line\">\t<span class=\"keyword\">map</span> <span class=\"keyword\">keys</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">':'</span>                     <span class=\"comment\">-- map中键值对之间用':'分隔</span></span><br><span class=\"line\">\t<span class=\"keyword\">lines</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">'\\n                        -- 行之间用'</span>\\n<span class=\"string\">'分隔 默认一般不指定</span></span><br></pre></td></tr></table></figure>\n\n<p>2、向表test_set中插入数据</p>\n<p>1）对于数据量较大，常用的一种方法是通过文件批量导入的方法，比如我现在要将如下的文本中的数据插入到表中</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1,xiaoming,basketball_game,xiaohong:yes_xiaohua:no,99_75</span><br><span class=\"line\"></span><br><span class=\"line\">1,xiaohong,watch_study,xiaoming:no_xiaohua:not,95_95</span><br></pre></td></tr></table></figure>\n\n<p>可以采用如下语句来实现</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive -e \"<span class=\"keyword\">load</span> <span class=\"keyword\">data</span> <span class=\"keyword\">local</span> inpath <span class=\"string\">'/path/data.txt'</span> overwrite <span class=\"keyword\">into</span> <span class=\"keyword\">table</span> collect_test<span class=\"string\">\"</span></span><br></pre></td></tr></table></figure>\n\n<p>2）对于想插入几条数据时，可以采取insert语句来插入数据，比如我们想插入数据</p>\n<p>2,xiaohua,basketball_read,xiaoming:no_xiaohong:no,90_90<br>可以采用如下语句来实现，分别通过array,str_to_map,named_struct来包装插入的三种集合数据</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> collect_test</span><br><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">\t<span class=\"number\">2</span>,</span><br><span class=\"line\">\t<span class=\"string\">'xiaohua'</span>,</span><br><span class=\"line\">\t<span class=\"built_in\">array</span>(<span class=\"string\">'basketball'</span>, <span class=\"string\">'read'</span>),</span><br><span class=\"line\">\tstr_to_map(<span class=\"string\">'xiaoming:no,xiaohong:no'</span>),</span><br><span class=\"line\">\tnamed_struct(<span class=\"string\">'math'</span>, <span class=\"number\">90</span>, <span class=\"string\">'english'</span>, <span class=\"number\">90</span>)</span><br></pre></td></tr></table></figure>\n\n<p>对于集合类型的查询，我们还有一种经常使用的方法，查询语句如下</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">\t<span class=\"keyword\">id</span>,</span><br><span class=\"line\">\t<span class=\"keyword\">name</span>,</span><br><span class=\"line\">\thobby[<span class=\"number\">0</span>], \t\t\t\t\t<span class=\"comment\">-- 查询第一个hobby</span></span><br><span class=\"line\">\tfriend[<span class=\"string\">'xiaohong'</span>], <span class=\"comment\">-- 查询map键为xiaohong的value</span></span><br><span class=\"line\">\tmark.math \t\t\t\t\t<span class=\"comment\">-- 查询struct中math的值</span></span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">\ttest_set</span><br><span class=\"line\"><span class=\"keyword\">where</span></span><br><span class=\"line\">\t<span class=\"keyword\">name</span> = <span class=\"string\">'xiaoming'</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<p>参考：</p>\n<p>1.<a href=\"https://blog.csdn.net/qq_41973536/article/details/81627918\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/qq_41973536/article/details/81627918</a></p>\n<p>2.<a href=\"https://blog.csdn.net/u014414323/article/details/83616361\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/u014414323/article/details/83616361</a></p>\n","site":{"data":{}},"length":2347,"excerpt":"","more":"<p>Hive中的列支持使用struct，map和array集合数据类型。下表中的数据类型实际上调用的是内置函数。</p>\n<p>Hive集合数据类型:</p>\n<table>\n<thead>\n<tr>\n<th><strong>数据类型</strong></th>\n<th><strong>描述</strong></th>\n<th><strong>字面语法示例</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>STRUCT</td>\n<td>数据类型描述字面语法示例和C语言中的struct或者“对象”类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 STRUCT { first STRING , last STRING} ，那么第 1 个元素可以通过字段名.first来引用</td>\n<td>struct(‘John’,’Doe’)</td>\n</tr>\n<tr>\n<td>MAP</td>\n<td>MAP 是一组键一值对元组集合，使用数组表示法(例如[‘key’]) 可以访问元素。例如，如果某个列的数据类型是 MAP ，其中键 值对是’first’ -&gt; ‘John’ 和’last’ -&gt; ‘Doe’，那么可以通过字段名[‘last’]获取最后 1 个元素</td>\n<td>map(‘first’,’JOIN’,’last’,’Doe’)</td>\n</tr>\n<tr>\n<td>ARRAY</td>\n<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为［’John’, ‘Doe’] , 那么第 2 个元素可以通过数组名[1]进行引用</td>\n<td>Array(‘John’,’Doe’)</td>\n</tr>\n</tbody></table>\n<p>和基本数据类型一样，这些类型的名称同样是保留字。</p>\n<p>大多数的关系型数据库并不支持这些集合数据类型，因此使用它们会趋向于破坏标准格式。例如，在传统数据模型中，structs可能需要由多个不同的表拼装而成，表间需要适当地使用外键来进行连接。</p>\n<p>破坏标准格式所带来的一个实际问题是会增大数据冗余的风险，进而导致消耗不必要的磁盘空间，还有可能造成数据不一致，因此当数据发生改变时冗余的拷贝数据可能无法进行相应的同步。</p>\n<p>然而，在大数据系统中，不遵循标准格式的一个好处就是可以提高更高吞吐量的数据。当处理的数据的数量级是TB或者PB时，以最少的“头部寻址”来从磁盘上扫描数据是非常必要的。按数据进行封装的话可以通过减少寻址次数来提供查询的速度。而如果根据外键关系关联的话则需要进行磁盘间的寻址操作，这样会有非常高的性能消耗。</p>\n<p>建表：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">create</span></span><br><span class=\"line\">\t<span class=\"keyword\">table</span> collect_test</span><br><span class=\"line\">\t(</span><br><span class=\"line\">\t\t<span class=\"keyword\">id</span> <span class=\"built_in\">INT</span>,</span><br><span class=\"line\">\t\t<span class=\"keyword\">name</span> <span class=\"keyword\">STRING</span>,</span><br><span class=\"line\">\t\thobby <span class=\"built_in\">ARRAY</span> &lt; <span class=\"keyword\">STRING</span> &gt;,                      <span class=\"comment\">-- array中元素为String类型</span></span><br><span class=\"line\">\t\tfriend <span class=\"keyword\">MAP</span> &lt; <span class=\"keyword\">STRING</span>,<span class=\"keyword\">STRING</span> &gt;,                <span class=\"comment\">-- map中键和值均为String类型</span></span><br><span class=\"line\">\t\tmark <span class=\"keyword\">struct</span> &lt; math:<span class=\"built_in\">int</span>,english:<span class=\"built_in\">int</span> &gt;         <span class=\"comment\">-- Struct中元素为Int类型</span></span><br><span class=\"line\">\t)</span><br><span class=\"line\">\t<span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>  <span class=\"comment\">-- 字段之间用','分隔</span></span><br><span class=\"line\">\tcollection items <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">'_'</span>             <span class=\"comment\">-- 集合中的元素用'_'分隔</span></span><br><span class=\"line\">\t<span class=\"keyword\">map</span> <span class=\"keyword\">keys</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">':'</span>                     <span class=\"comment\">-- map中键值对之间用':'分隔</span></span><br><span class=\"line\">\t<span class=\"keyword\">lines</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">'\\n                        -- 行之间用'</span>\\n<span class=\"string\">'分隔 默认一般不指定</span></span><br></pre></td></tr></table></figure>\n\n<p>2、向表test_set中插入数据</p>\n<p>1）对于数据量较大，常用的一种方法是通过文件批量导入的方法，比如我现在要将如下的文本中的数据插入到表中</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1,xiaoming,basketball_game,xiaohong:yes_xiaohua:no,99_75</span><br><span class=\"line\"></span><br><span class=\"line\">1,xiaohong,watch_study,xiaoming:no_xiaohua:not,95_95</span><br></pre></td></tr></table></figure>\n\n<p>可以采用如下语句来实现</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive -e \"<span class=\"keyword\">load</span> <span class=\"keyword\">data</span> <span class=\"keyword\">local</span> inpath <span class=\"string\">'/path/data.txt'</span> overwrite <span class=\"keyword\">into</span> <span class=\"keyword\">table</span> collect_test<span class=\"string\">\"</span></span><br></pre></td></tr></table></figure>\n\n<p>2）对于想插入几条数据时，可以采取insert语句来插入数据，比如我们想插入数据</p>\n<p>2,xiaohua,basketball_read,xiaoming:no_xiaohong:no,90_90<br>可以采用如下语句来实现，分别通过array,str_to_map,named_struct来包装插入的三种集合数据</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> collect_test</span><br><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">\t<span class=\"number\">2</span>,</span><br><span class=\"line\">\t<span class=\"string\">'xiaohua'</span>,</span><br><span class=\"line\">\t<span class=\"built_in\">array</span>(<span class=\"string\">'basketball'</span>, <span class=\"string\">'read'</span>),</span><br><span class=\"line\">\tstr_to_map(<span class=\"string\">'xiaoming:no,xiaohong:no'</span>),</span><br><span class=\"line\">\tnamed_struct(<span class=\"string\">'math'</span>, <span class=\"number\">90</span>, <span class=\"string\">'english'</span>, <span class=\"number\">90</span>)</span><br></pre></td></tr></table></figure>\n\n<p>对于集合类型的查询，我们还有一种经常使用的方法，查询语句如下</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">\t<span class=\"keyword\">id</span>,</span><br><span class=\"line\">\t<span class=\"keyword\">name</span>,</span><br><span class=\"line\">\thobby[<span class=\"number\">0</span>], \t\t\t\t\t<span class=\"comment\">-- 查询第一个hobby</span></span><br><span class=\"line\">\tfriend[<span class=\"string\">'xiaohong'</span>], <span class=\"comment\">-- 查询map键为xiaohong的value</span></span><br><span class=\"line\">\tmark.math \t\t\t\t\t<span class=\"comment\">-- 查询struct中math的值</span></span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">\ttest_set</span><br><span class=\"line\"><span class=\"keyword\">where</span></span><br><span class=\"line\">\t<span class=\"keyword\">name</span> = <span class=\"string\">'xiaoming'</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<p>参考：</p>\n<p>1.<a href=\"https://blog.csdn.net/qq_41973536/article/details/81627918\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/qq_41973536/article/details/81627918</a></p>\n<p>2.<a href=\"https://blog.csdn.net/u014414323/article/details/83616361\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/u014414323/article/details/83616361</a></p>\n"},{"title":"Hive SQL 的编译过程","date":"2019-07-12T04:33:48.000Z","_content":"\n> 原文地址 https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html\n\nHive 是基于 Hadoop 的一个数据仓库系统，在各大公司都有广泛的应用。美团数据仓库也是基于 Hive 搭建，每天执行近万次的 Hive ETL 计算流程，负责每天数百 GB 的数据存储和分析。Hive 的稳定性和性能对我们的数据分析非常关键。\n\n在几次升级 Hive 的过程中，我们遇到了一些大大小小的问题。通过向社区的咨询和自己的努力，在解决这些问题的同时我们对 Hive 将 SQL 编译为 MapReduce 的过程有了比较深入的理解。对这一过程的理解不仅帮助我们解决了一些 Hive 的 bug，也有利于我们优化 Hive SQL，提升我们对 Hive 的掌控力，同时有能力去定制一些需要的功能。\n\nMapReduce 实现基本 SQL 操作的原理\n------------------------\n\n详细讲解 SQL 编译为 MapReduce 之前，我们先来看看 MapReduce 框架实现 SQL 基本操作的原理\n\n### Join 的实现原理\n\n```\nselect u.name, o.orderid from order o join user u on o.uid = u.uid;\n\n\n```\n\n在 map 的输出 value 中为不同表的数据打上 tag 标记，在 reduce 阶段根据 tag 判断数据来源。MapReduce 的过程如下（这里只是说明最基本的 Join 的实现，还有其他的实现方式）\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/73cd82b9.png)\n\nMapReduce CommonJoin 的实现\n\n### Group By 的实现原理\n\n```\nselect rank, isonline, count(*) from city group by rank, isonline;\n\n\n```\n\n将 GroupBy 的字段组合为 map 的输出 key 值，利用 MapReduce 的排序，在 reduce 阶段保存 LastKey 区分不同的 key。MapReduce 的过程如下（当然这里只是说明 Reduce 端的非 Hash 聚合过程）\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/bcb10088.png)\n\nMapReduce Group By 的实现\n\n### Distinct 的实现原理\n\n```\nselect dealid, count(distinct uid) num from order group by dealid;\n\n\n```\n\n当只有一个 distinct 字段时，如果不考虑 Map 阶段的 Hash GroupBy，只需要将 GroupBy 字段和 Distinct 字段组合为 map 输出 key，利用 mapreduce 的排序，同时将 GroupBy 字段作为 reduce 的 key，在 reduce 阶段保存 LastKey 即可完成去重\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/d7816cf9.png)\n\nMapReduce Distinct 的实现\n\n如果有多个 distinct 字段呢，如下面的 SQL\n\n```\nselect dealid, count(distinct uid), count(distinct date) from order group by dealid;\n\n\n```\n\n实现方式有两种：\n\n（1）如果仍然按照上面一个 distinct 字段的方法，即下图这种实现方式，无法跟据 uid 和 date 分别排序，也就无法通过 LastKey 去重，仍然需要在 reduce 阶段在内存中通过 Hash 去重\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/16d67006.png)\n\nMapReduce Multi Distinct 的实现\n\n（2）第二种实现方式，可以对所有的 distinct 字段编号，每行数据生成 n 行数据，那么相同字段就会分别排序，这时只需要在 reduce 阶段记录 LastKey 即可去重。\n\n这种实现方式很好的利用了 MapReduce 的排序，节省了 reduce 阶段去重的内存消耗，但是缺点是增加了 shuffle 的数据量。\n\n需要注意的是，在生成 reduce value 时，除第一个 distinct 字段所在行需要保留 value 值，其余 distinct 数据行 value 字段均可为空。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/68fce9bb.png)\n\nMapReduce Multi Distinct 的实现\n\nSQL 转化为 MapReduce 的过程\n---------------------\n\n了解了 MapReduce 实现 SQL 基本操作之后，我们来看看 Hive 是如何将 SQL 转化为 MapReduce 任务的，整个编译过程分为六个阶段：\n\n1.  Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象语法树 AST Tree\n2.  遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock\n3.  遍历 QueryBlock，翻译为执行操作树 OperatorTree\n4.  逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量\n5.  遍历 OperatorTree，翻译为 MapReduce 任务\n6.  物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划\n\n下面分别对这六个阶段进行介绍\n\n### Phase1 SQL 词法，语法解析\n\n#### Antlr\n\nHive 使用 Antlr 实现 SQL 的词法和语法解析。Antlr 是一种语言识别的工具，可以用来构造领域语言。 这里不详细介绍 Antlr，只需要了解使用 Antlr 构造特定的语言只需要编写一个语法文件，定义词法和语法替换规则即可，Antlr 完成了词法分析、语法分析、语义分析、中间代码生成的过程。\n\nHive 中语法规则的定义文件在 0.10 版本以前是 Hive.g 一个文件，随着语法规则越来越复杂，由语法规则生成的 Java 解析类可能超过 Java 类文件的最大上限，0.11 版本将 Hive.g 拆成了 5 个文件，词法规则 HiveLexer.g 和语法规则的 4 个文件 SelectClauseParser.g，FromClauseParser.g，IdentifiersParser.g，HiveParser.g。\n\n#### 抽象语法树 AST Tree\n\n经过词法和语法解析后，如果需要对表达式做进一步的处理，使用 Antlr 的抽象语法树语法 Abstract Syntax Tree，在语法分析的同时将输入语句转换成抽象语法树，后续在遍历语法树时完成进一步的处理。\n\n下面的一段语法是 Hive SQL 中 SelectStatement 的语法规则，从中可以看出，SelectStatement 包含 select, from, where, groupby, having, orderby 等子句。 （在下面的语法规则中，箭头表示对于原语句的改写，改写后会加入一些特殊词标示特定语法，比如 TOK_QUERY 标示一个查询块）\n\n```SPARQL\nselectStatement\n   :\n   selectClause\n   fromClause\n   whereClause?\n   groupByClause?\n   havingClause?\n   orderByClause?\n   clusterByClause?\n   distributeByClause?\n   sortByClause?\n   limitClause? -> ^(TOK_QUERY fromClause ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE))\n                     selectClause whereClause? groupByClause? havingClause? orderByClause? clusterByClause?\n                     distributeByClause? sortByClause? limitClause?))\n   ;\n```\n\n#### 样例 SQL\n\n为了详细说明 SQL 翻译为 MapReduce 的过程，这里以一条简单的 SQL 为例，SQL 中包含一个子查询，最终将数据写入到一张表中\n\n```sql\nFROM\n( \n  SELECT\n    p.datekey datekey,\n    p.userid userid,\n    c.clienttype\n  FROM\n    detail.usersequence_client c\n    JOIN fact.orderpayment p ON p.orderid = c.orderid\n    JOIN default.user du ON du.userid = p.userid\n  WHERE p.datekey = 20131118 \n) base\nINSERT OVERWRITE TABLE `test`.`customer_kpi`\nSELECT\n  base.datekey,\n  base.clienttype,\n  count(distinct base.userid) buyer_count\nGROUP BY base.datekey, base.clienttype\n\n\n```\n\n#### SQL 生成 AST Tree\n\nAntlr 对 Hive SQL 解析的代码如下，HiveLexerX，HiveParser 分别是 Antlr 对语法文件 Hive.g 编译后自动生成的词法解析和语法解析类，在这两个类中进行复杂的解析。\n\n```sql\nHiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));    \nTokenRewriteStream tokens = new TokenRewriteStream(lexer);\nif (ctx != null) {\n  ctx.setTokenRewriteStream(tokens);\n}\nHiveParser parser = new HiveParser(tokens);                                 \nparser.setTreeAdaptor(adaptor);\nHiveParser.statement_return r = null;\ntry {\n  r = parser.statement();                                                   \n} catch (RecognitionException e) {\n  e.printStackTrace();\n  throw new ParseException(parser.errors);\n}\n\n\n```\n\n最终生成的 AST Tree 如下图右侧（使用 Antlr Works 生成，Antlr Works 是 Antlr 提供的编写语法文件的编辑器），图中只是展开了骨架的几个节点，没有完全展开。 子查询 1/2，分别对应右侧第 1/2 两个部分。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/53c6802d.png)\n\nSQL 生成 AST Tree\n\n这里注意一下内层子查询也会生成一个 TOK_DESTINATION 节点。请看上面 SelectStatement 的语法规则，这个节点是在语法改写中特意增加了的一个节点。原因是 Hive 中所有查询的数据均会保存在 HDFS 临时的文件中，无论是中间的子查询还是查询最终的结果，Insert 语句最终会将数据写入表所在的 HDFS 目录下。\n\n详细来看，将内存子查询的 from 子句展开后，得到如下 AST Tree，每个表生成一个 TOK_TABREF 节点，Join 条件生成一个 “=” 节点。其他 SQL 部分类似，不一一详述。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/f7ebe4b8.png)\n\nAST Tree\n\n### Phase2 SQL 基本组成单元 QueryBlock\n\nAST Tree 仍然非常复杂，不够结构化，不方便直接翻译为 MapReduce 程序，AST Tree 转化为 QueryBlock 就是将 SQL 进一部抽象和结构化。\n\n#### QueryBlock\n\nQueryBlock 是一条 SQL 最基本的组成单元，包括三个部分：输入源，计算过程，输出。简单来讲一个 QueryBlock 就是一个子查询。\n\n下图为 Hive 中 QueryBlock 相关对象的类图，解释图中几个重要的属性\n\n*   QB#aliasToSubq（表示 QB 类的 aliasToSubq 属性）保存子查询的 QB 对象，aliasToSubq key 值是子查询的别名\n*   QB#qbp 即 QBParseInfo 保存一个基本 SQL 单元中的给个操作部分的 AST Tree 结构，QBParseInfo#nameToDest 这个 HashMap 保存查询单元的输出，key 的形式是 inclause-i（由于 Hive 支持 Multi Insert 语句，所以可能有多个输出），value 是对应的 ASTNode 节点，即 TOK_DESTINATION 节点。类 QBParseInfo 其余 HashMap 属性分别保存输出和各个操作的 ASTNode 节点的对应关系。\n*   QBParseInfo#JoinExpr 保存 TOK_JOIN 节点。QB#QBJoinTree 是对 Join 语法树的结构化。\n*   QB#qbm 保存每个输入表的元信息，比如表在 HDFS 上的路径，保存表数据的文件格式等。\n*   QBExpr 这个对象是为了表示 Union 操作。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/fc74a4ea.png)\n\nQueryBlock\n\n#### AST Tree 生成 QueryBlock\n\nAST Tree 生成 QueryBlock 的过程是一个递归的过程，先序遍历 AST Tree，遇到不同的 Token 节点，保存到相应的属性中，主要包含以下几个过程\n\n*   TOK_QUERY => 创建 QB 对象，循环递归子节点\n*   TOK_FROM => 将表名语法部分保存到 QB 对象的`aliasToTabs`等属性中\n*   TOK_INSERT => 循环递归子节点\n*   TOK_DESTINATION => 将输出目标的语法部分保存在 QBParseInfo 对象的 nameToDest 属性中\n*   TOK_SELECT => 分别将查询表达式的语法部分保存在`destToSelExpr`、`destToAggregationExprs`、`destToDistinctFuncExprs`三个属性中\n*   TOK_WHERE => 将 Where 部分的语法保存在 QBParseInfo 对象的 destToWhereExpr 属性中\n\n最终样例 SQL 生成两个 QB 对象，QB 对象的关系如下，QB1 是外层查询，QB2 是子查询\n\n```\nQB1\n\n  \\\n\n   QB2\n```\n\n### Phase3 逻辑操作符 Operator\n\n#### Operator\n\nHive 最终生成的 MapReduce 任务，Map 阶段和 Reduce 阶段均由 OperatorTree 组成。逻辑操作符，就是在 Map 阶段或者 Reduce 阶段完成单一特定的操作。\n\n基本的操作符包括 TableScanOperator，SelectOperator，FilterOperator，JoinOperator，GroupByOperator，ReduceSinkOperator\n\n从名字就能猜出各个操作符完成的功能，TableScanOperator 从 MapReduce 框架的 Map 接口原始输入表的数据，控制扫描表的数据行数，标记是从原表中取数据。JoinOperator 完成 Join 操作。FilterOperator 完成过滤操作\n\nReduceSinkOperator 将 Map 端的字段组合序列化为 Reduce Key/value, Partition Key，只可能出现在 Map 阶段，同时也标志着 Hive 生成的 MapReduce 程序中 Map 阶段的结束。\n\nOperator 在 Map Reduce 阶段之间的数据传递都是一个流式的过程。每一个 Operator 对一行数据完成操作后之后将数据传递给 childOperator 计算。\n\nOperator 类的主要属性和方法如下\n\n*   RowSchema 表示 Operator 的输出字段\n*   InputObjInspector outputObjInspector 解析输入和输出字段\n*   processOp 接收父 Operator 传递的数据，forward 将处理好的数据传递给子 Operator 处理\n*   Hive 每一行数据经过一个 Operator 处理之后，会对字段重新编号，colExprMap 记录每个表达式经过当前 Operator 处理前后的名称对应关系，在下一个阶段逻辑优化阶段用来回溯字段名\n*   由于 Hive 的 MapReduce 程序是一个动态的程序，即不确定一个 MapReduce Job 会进行什么运算，可能是 Join，也可能是 GroupBy，所以 Operator 将所有运行时需要的参数保存在 OperatorDesc 中，OperatorDesc 在提交任务前序列化到 HDFS 上，在 MapReduce 任务执行前从 HDFS 读取并反序列化。Map 阶段 OperatorTree 在 HDFS 上的位置在 Job.getConf(“hive.exec.plan”) + “/map.xml”\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/bb36a793.png)\n\nQueryBlock\n\n#### QueryBlock 生成 Operator Tree\n\nQueryBlock 生成 Operator Tree 就是遍历上一个过程中生成的 QB 和 QBParseInfo 对象的保存语法的属性，包含如下几个步骤：\n\n*   QB#aliasToSubq => 有子查询，递归调用\n*   QB#aliasToTabs => TableScanOperator\n*   QBParseInfo#joinExpr => QBJoinTree => ReduceSinkOperator + JoinOperator\n*   QBParseInfo#destToWhereExpr => FilterOperator\n*   QBParseInfo#destToGroupby => ReduceSinkOperator + GroupByOperator\n*   QBParseInfo#destToOrderby => ReduceSinkOperator + ExtractOperator\n\n**_由于 Join/GroupBy/OrderBy 均需要在 Reduce 阶段完成，所以在生成相应操作的 Operator 之前都会先生成一个 ReduceSinkOperator，将字段组合并序列化为 Reduce Key/value, Partition Key_**\n\n接下来详细分析样例 SQL 生成 OperatorTree 的过程\n\n先序遍历上一个阶段生成的 QB 对象\n\n1.  首先根据子 QueryBlock `QB2#aliasToTabs {du=dim.user, c=detail.usersequence_client, p=fact.orderpayment}`生成 TableScanOperator\n  \n    ```\n    TableScanOperator(“dim.user”) TS[0]\n    TableScanOperator(“detail.usersequence_client”) TS[1]       TableScanOperator(“fact.orderpayment”) TS[2]\n    \n    \n    ```\n    \n2.  先序遍历`QBParseInfo#joinExpr`生成`QBJoinTree`，类`QBJoinTree`也是一个树状结构，`QBJoinTree`保存左右表的 ASTNode 和这个查询的别名，最终生成的查询树如下\n  \n    ```\n       base\n       /  \\\n      p    du\n     /      \\\n    c        p\n    ```\n    \n3.  前序遍历`QBJoinTree`，先生成`detail.usersequence_client`和`fact.orderpayment`的 Join 操作树\n  \n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/a720d129.png)\n\nJoin to Operator\n\n**_图中 TS=TableScanOperator RS=ReduceSinkOperator JOIN=JoinOperator_**\n\n1.  生成中间表与 dim.user 的 Join 操作树\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/d368c7d0.png)\n\nJoin to Operator\n\n1.  根据 QB2 `QBParseInfo#destToWhereExpr` 生成`FilterOperator`。此时 QB2 遍历完成。\n\n下图中 SelectOperator 在某些场景下会根据一些条件判断是否需要解析字段。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/383ce9ae.png)\n\nWhere to Operator\n\n**_图中 FIL= FilterOperator SEL= SelectOperator_**\n\n1.  根据 QB1 的 QBParseInfo#destToGroupby 生成 ReduceSinkOperator + GroupByOperator\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/fa192406.png)\n\nGroupBy to Operator\n\n**_图中 GBY= GroupByOperator_** **_GBY[12] 是 HASH 聚合，即在内存中通过 Hash 进行聚合运算_**\n\n1.  最终都解析完后，会生成一个 FileSinkOperator，将数据写入 HDFS\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/5cb009cc.png)\n\nFileSinkOperator\n\n**_图中 FS=FileSinkOperator_**\n\n### Phase4 逻辑层优化器\n\n大部分逻辑层优化器通过变换 OperatorTree，合并操作符，达到减少 MapReduce Job，减少 shuffle 数据量的目的。\n\n| 名称                      | 作用                                                   |\n| :------------------------ | :----------------------------------------------------- |\n| ② SimpleFetchOptimizer    | 优化没有GroupBy表达式的聚合查询                        |\n| ② MapJoinProcessor        | MapJoin，需要SQL中提供hint，0.11版本已不用             |\n| ② BucketMapJoinOptimizer  | BucketMapJoin                                          |\n| ② GroupByOptimizer        | Map端聚合                                              |\n| ① ReduceSinkDeDuplication | 合并线性的OperatorTree中partition/sort key相同的reduce |\n| ① PredicatePushDown       | 谓词前置                                               |\n| ① CorrelationOptimizer    | 利用查询中的相关性，合并有相关性的Job，HIVE-2206       |\n| ColumnPruner              | 字段剪枝                                               |\n\n表格中①的优化器均是一个 Job 干尽可能多的事情 / 合并。②的都是减少 shuffle 数据量，甚至不做 Reduce。\n\nCorrelationOptimizer 优化器非常复杂，都能利用查询中的相关性，合并有相关性的 Job，参考 [Hive Correlation Optimizer](https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer)\n\n对于样例 SQL，有两个优化器对其进行优化。下面分别介绍这两个优化器的作用，并补充一个优化器 ReduceSinkDeDuplication 的作用\n\n#### PredicatePushDown 优化器\n\n断言判断提前优化器将 OperatorTree 中的 FilterOperator 提前到 TableScanOperator 之后\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/8c45914c.png)\n\nPredicatePushDown\n\n#### NonBlockingOpDeDupProc 优化器\n\n`NonBlockingOpDeDupProc`优化器合并 SEL-SEL 或者 FIL-FIL 为一个 Operator\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/943f0313.png)\n\nNonBlockingOpDeDupProc\n\n#### ReduceSinkDeDuplication 优化器\n\nReduceSinkDeDuplication 可以合并线性相连的两个 RS。实际上 CorrelationOptimizer 是 ReduceSinkDeDuplication 的超集，能合并线性和非线性的操作 RS，但是 Hive 先实现的 ReduceSinkDeDuplication\n\n譬如下面这条 SQL 语句\n\n```sql\nfrom (select key, value from src group by key, value) s select s.key group by s.key;\n```\n\n经过前面几个阶段之后，会生成如下的 OperatorTree，两个 Tree 是相连的，这里没有画到一起\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/67f4d79f.png)\n\nReduceSinkDeDuplication\n\n这时候遍历 OperatorTree 后能发现前前后两个 RS 输出的 Key 值和 PartitionKey 如下\n\n|          | Key       | PartitionKey |\n| :------- | :-------- | ------------ |\n| childRS  | key       | key          |\n| parentRS | key,value | key,value    |\n\nReduceSinkDeDuplication 优化器检测到：1. pRS Key 完全包含 cRS Key，且排序顺序一致；2. pRS PartitionKey 完全包含 cRS PartitionKey。符合优化条件，会对执行计划进行优化。\n\nReduceSinkDeDuplication 将 childRS 和 parentheRS 与 childRS 之间的 Operator 删掉，保留的 RS 的 Key 为 key,value 字段，PartitionKey 为 key 字段。合并后的 OperatorTree 如下：\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/9ef61667.png)\n\nReduceSinkDeDuplication\n\n### Phase5 OperatorTree 生成 MapReduce Job 的过程\n\nOperatorTree 转化为 MapReduce Job 的过程分为下面几个阶段\n\n1.  对输出表生成 MoveTask\n2.  从 OperatorTree 的其中一个根节点向下深度优先遍历\n3.  ReduceSinkOperator 标示 Map/Reduce 的界限，多个 Job 间的界限\n4.  遍历其他根节点，遇过碰到 JoinOperator 合并 MapReduceTask\n5.  生成 StatTask 更新元数据\n6.  剪断 Map 与 Reduce 间的 Operator 的关系\n\n#### 对输出表生成 MoveTask\n\n由上一步 OperatorTree 只生成了一个 FileSinkOperator，直接生成一个 MoveTask，完成将最终生成的 HDFS 临时文件移动到目标表目录下\n\n```\nMoveTask[Stage-0]\nMove Operator\n```\n\n#### 开始遍历\n\n将 OperatorTree 中的所有根节点保存在一个 toWalk 的数组中，循环取出数组中的元素（省略 QB1，未画出）\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/7088b447.png)\n\n开始遍历\n\n取出最后一个元素 TS[p] 放入栈 opStack{TS[p]} 中\n\n#### Rule #1 TS% 生成 MapReduceTask 对象，确定 MapWork\n\n发现栈中的元素符合下面规则 R1（这里用 python 代码简单表示）\n\n```\n\"\".join([t + \"%\" for t in opStack]) == \"TS%\"\n```\n\n生成一个`MapReduceTask[Stage-1]`对象，`MapReduceTask[Stage-1]`对象的`MapWork`属性保存 Operator 根节点的引用。由于 OperatorTree 之间之间的 Parent Child 关系，这个时候`MapReduceTask[Stage-1]`包含了以`TS[p]`为根的所有 Operator\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/38fce288.png)\n\nStage-1 生成 Map 阶段\n\n#### Rule #2 TS%.*RS% 确定 ReduceWork\n\n继续遍历 TS[p] 的子 Operator，将子 Operator 存入栈 opStack 中 当第一个 RS 进栈后，即栈 opStack = {TS[p], FIL[18], RS[4]} 时，就会满足下面的规则 R2\n\n```\n\"\".join([t + \"%\" for t in opStack]) == \"TS%.*RS%\"\n```\n\n这时候在`MapReduceTask[Stage-1]`对象的`ReduceWork`属性保存`JOIN[5]`的引用\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/f8a8803e.png)\n\nStage-1 生成 Reduce 阶段\n\n#### Rule #3 RS%.*RS% 生成新 MapReduceTask 对象，切分 MapReduceTask\n\n继续遍历 JOIN[5] 的子 Operator，将子 Operator 存入栈 opStack 中\n\n当第二个 RS 放入栈时，即当栈`opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6]}`时，就会满足下面的规则 R3\n\n```\n\"\".join([t + \"%\" for t in opStack]) == “RS%.*RS%” //循环遍历opStack的每一个后缀数组\n```\n\n这时候创建一个新的`MapReduceTask[Stage-2]`对象，将 OperatorTree 从`JOIN[5]`和`RS[6]`之间剪开，并为`JOIN[5]`生成一个子 Operator `FS[19]`，`RS[6]`生成一个`TS[20]`，`MapReduceTask[Stage-2]`对象的`MapWork`属性保存`TS[20]`的引用。\n\n新生成的`FS[19]`将中间数据落地，存储在 HDFS 临时文件中。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/cc1c7642.png)\n\nStage-2\n\n继续遍历 RS[6] 的子 Operator，将子 Operator 存入栈 opStack 中\n\n当`opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13]}`时，又会满足 R3 规则\n\n同理生成`MapReduceTask[Stage-3]`对象，并切开 Stage-2 和 Stage-3 的 OperatorTree\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6e876f9e.png)\n\nStage-3\n\n#### R4 FS% 连接 MapReduceTask 与 MoveTask\n\n最终将所有子 Operator 存入栈中之后，`opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13], GBY[14], SEL[15], FS[17]}` 满足规则 R4\n\n```\n\"\".join([t + \"%\" for t in opStack]) == “FS%”\n```\n\n这时候将`MoveTask`与`MapReduceTask[Stage-3]`连接起来，并生成一个`StatsTask`，修改表的元信息\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6d3b67c9.png)\n\nMoveTask\n\n#### 合并 Stage\n\n此时并没有结束，还有两个根节点没有遍历。\n\n将 opStack 栈清空，将 toWalk 的第二个元素加入栈。会发现`opStack = {TS[du]}`继续满足 R1 TS%，生成`MapReduceTask[Stage-5]`\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6799e54e.png)\n\nStage-5\n\n继续从`TS[du]`向下遍历，当`opStack={TS[du], RS[7]}`时，满足规则 R2 TS%.*RS%\n\n此时将`JOIN[8]`保存为`MapReduceTask[Stage-5]`的`ReduceWork`时，发现在一个 Map 对象保存的 Operator 与 MapReduceWork 对象关系的`Map<Operator, MapReduceWork>`对象中发现，`JOIN[8]`已经存在。此时将`MapReduceTask[Stage-2]`和`MapReduceTask[Stage-5]`合并为一个 MapReduceTask\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/164ea8fd.png)\n\n合并 Stage-2 和 Stage-5\n\n同理从最后一个根节点`TS[c]`开始遍历，也会对 MapReduceTask 进行合并\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/9259eee5.png)\n\n合并 Stage-1 和 Stage-6\n\n#### 切分 Map Reduce 阶段\n\n最后一个阶段，将 MapWork 和 ReduceWork 中的 OperatorTree 以 RS 为界限剪开\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/129684ea.png)\n\n切分 Map Reduce 阶段\n\n#### OperatorTree 生成 MapReduceTask 全貌\n\n最终共生成 3 个 MapReduceTask，如下图\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/13308564.png)\n\nOperatorTree 生成 MapReduceTask 全貌\n\n### Phase6 物理层优化器\n\n这里不详细介绍每个优化器的原理，单独介绍一下 MapJoin 的优化器\n\n| 名称                                 | 作用                             |\n| :----------------------------------- | :------------------------------- |\n| Vectorizer                           | HIVE-4160，将在0.13中发布        |\n| SortMergeJoinResolver                | 与bucket配合，类似于归并排序     |\n| SamplingOptimizer                    | 并行order by优化器，在0.12中发布 |\n| CommonJoinResolver + MapJoinResolver | MapJoin优化器                    |\n\n#### MapJoin 原理\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/a4cd80c9.png)\n\nmapjoin 原理\n\nMapJoin 简单说就是在 Map 阶段将小表读入内存，顺序扫描大表完成 Join。\n\n上图是 Hive MapJoin 的原理图，出自 Facebook 工程师 Liyin Tang 的一篇介绍 Join 优化的 slice，从图中可以看出 MapJoin 分为两个阶段：\n\n1.  通过 MapReduce Local Task，将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会对 HashTableFiles 进行压缩。\n  \n2.  MapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在 Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务。\n  \n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/678dfd68.png)\n\nconditionaltask\n\n如果 Join 的两张表一张表是临时表，就会生成一个 ConditionalTask，在运行期间判断是否使用 MapJoin\n\n#### CommonJoinResolver 优化器\n\nCommonJoinResolver 优化器就是将 CommonJoin 转化为 MapJoin，转化过程如下\n\n1.  深度优先遍历 Task Tree\n2.  找到 JoinOperator，判断左右表数据量大小\n3.  对与小表 + 大表 => MapJoinTask，对于小 / 大表 + 中间表 => ConditionalTask\n\n遍历上一个阶段生成的 MapReduce 任务，发现`MapReduceTask[Stage-2]` `JOIN[8]`中有一张表为临时表，先对 Stage-2 进行深度拷贝（由于需要保留原始执行计划为 Backup Plan，所以这里将执行计划拷贝了一份），生成一个 MapJoinOperator 替代 JoinOperator，然后生成一个 MapReduceLocalWork 读取小表生成 HashTableFiles 上传至 DistributedCache 中。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/4e209569.png)\n\nmapjoin 变换\n\nMapReduceTask 经过变换后的执行计划如下图所示\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/136eb26d.png)\n\nmapjoin 变换\n\n#### MapJoinResolver 优化器\n\nMapJoinResolver 优化器遍历 Task Tree，将所有有 local work 的 MapReduceTask 拆成两个 Task\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/1278f2fa.png)\n\nMapJoinResolver\n\n最终 MapJoinResolver 处理完之后，执行计划如下图所示\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/2a6ea0e2.png)\n\nMapJoinResolver\n\n### Hive SQL 编译过程的设计\n\n从上述整个 SQL 编译的过程，可以看出编译过程的设计有几个优点值得学习和借鉴\n\n*   使用 Antlr 开源软件定义语法规则，大大简化了词法和语法的编译解析过程，仅仅需要维护一份语法文件即可。\n*   整体思路很清晰，分阶段的设计使整个编译过程代码容易维护，使得后续各种优化器方便的以可插拔的方式开关，譬如 Hive 0.13 最新的特性 Vectorization 和对 Tez 引擎的支持都是可插拔的。\n*   每个 Operator 只完成单一的功能，简化了整个 MapReduce 程序。\n\n### 社区发展方向\n\nHive 依然在迅速的发展中，为了提升 Hive 的性能，hortonworks 公司主导的 Stinger 计划提出了一系列对 Hive 的改进，比较重要的改进有：\n\n*   Vectorization - 使 Hive 从单行单行处理数据改为批量处理方式，大大提升了指令流水线和缓存的利用率\n*   Hive on Tez - 将 Hive 底层的 MapReduce 计算框架替换为 Tez 计算框架。Tez 不仅可以支持多 Reduce 阶段的任务 MRR，还可以一次性提交执行计划，因而能更好的分配资源。\n*   Cost Based Optimizer - 使 Hive 能够自动选择最优的 Join 顺序，提高查询速度\n*   Implement insert, update, and delete in Hive with full ACID support - 支持表按主键的增量更新\n\n我们也将跟进社区的发展，结合自身的业务需要，提升 Hive 型 ETL 流程的性能\n\n### 参考\n\nAntlr: [http://www.antlr.org/](http://www.antlr.org/) \n\nWiki Antlr 介绍: [http://en.wikipedia.org/wiki/ANTLR](http://en.wikipedia.org/wiki/ANTLR) \n\nHive Wiki: [https://cwiki.apache.org/confluence/display/Hive/Home](https://cwiki.apache.org/confluence/display/Hive/Home) \n\nHiveSQL 编译过程: [http://www.slideshare.net/recruitcojp/internal-hive](http://www.slideshare.net/recruitcojp/internal-hive) \n\nJoin Optimization in Hive: [Join Strategies in Hive from the 2011 Hadoop Summit (Liyin Tang, Namit Jain)](https://cwiki.apache.org/confluence/download/attachments/27362054/Hive+Summit+2011-join.pdf?version=1&modificationDate=1309986642000) Hive Design Docs: [https://cwiki.apache.org/confluence/display/Hive/DesignDocs](https://cwiki.apache.org/confluence/display/Hive/DesignDocs)\n","source":"_posts/Hive/Hive SQL 的编译过程.md","raw":"---\ntitle: Hive SQL 的编译过程\ndate: 2019-07-12 12:33:48\ntags: \n    - sql\ncategories: \n    - Hive\n---\n\n> 原文地址 https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html\n\nHive 是基于 Hadoop 的一个数据仓库系统，在各大公司都有广泛的应用。美团数据仓库也是基于 Hive 搭建，每天执行近万次的 Hive ETL 计算流程，负责每天数百 GB 的数据存储和分析。Hive 的稳定性和性能对我们的数据分析非常关键。\n\n在几次升级 Hive 的过程中，我们遇到了一些大大小小的问题。通过向社区的咨询和自己的努力，在解决这些问题的同时我们对 Hive 将 SQL 编译为 MapReduce 的过程有了比较深入的理解。对这一过程的理解不仅帮助我们解决了一些 Hive 的 bug，也有利于我们优化 Hive SQL，提升我们对 Hive 的掌控力，同时有能力去定制一些需要的功能。\n\nMapReduce 实现基本 SQL 操作的原理\n------------------------\n\n详细讲解 SQL 编译为 MapReduce 之前，我们先来看看 MapReduce 框架实现 SQL 基本操作的原理\n\n### Join 的实现原理\n\n```\nselect u.name, o.orderid from order o join user u on o.uid = u.uid;\n\n\n```\n\n在 map 的输出 value 中为不同表的数据打上 tag 标记，在 reduce 阶段根据 tag 判断数据来源。MapReduce 的过程如下（这里只是说明最基本的 Join 的实现，还有其他的实现方式）\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/73cd82b9.png)\n\nMapReduce CommonJoin 的实现\n\n### Group By 的实现原理\n\n```\nselect rank, isonline, count(*) from city group by rank, isonline;\n\n\n```\n\n将 GroupBy 的字段组合为 map 的输出 key 值，利用 MapReduce 的排序，在 reduce 阶段保存 LastKey 区分不同的 key。MapReduce 的过程如下（当然这里只是说明 Reduce 端的非 Hash 聚合过程）\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/bcb10088.png)\n\nMapReduce Group By 的实现\n\n### Distinct 的实现原理\n\n```\nselect dealid, count(distinct uid) num from order group by dealid;\n\n\n```\n\n当只有一个 distinct 字段时，如果不考虑 Map 阶段的 Hash GroupBy，只需要将 GroupBy 字段和 Distinct 字段组合为 map 输出 key，利用 mapreduce 的排序，同时将 GroupBy 字段作为 reduce 的 key，在 reduce 阶段保存 LastKey 即可完成去重\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/d7816cf9.png)\n\nMapReduce Distinct 的实现\n\n如果有多个 distinct 字段呢，如下面的 SQL\n\n```\nselect dealid, count(distinct uid), count(distinct date) from order group by dealid;\n\n\n```\n\n实现方式有两种：\n\n（1）如果仍然按照上面一个 distinct 字段的方法，即下图这种实现方式，无法跟据 uid 和 date 分别排序，也就无法通过 LastKey 去重，仍然需要在 reduce 阶段在内存中通过 Hash 去重\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/16d67006.png)\n\nMapReduce Multi Distinct 的实现\n\n（2）第二种实现方式，可以对所有的 distinct 字段编号，每行数据生成 n 行数据，那么相同字段就会分别排序，这时只需要在 reduce 阶段记录 LastKey 即可去重。\n\n这种实现方式很好的利用了 MapReduce 的排序，节省了 reduce 阶段去重的内存消耗，但是缺点是增加了 shuffle 的数据量。\n\n需要注意的是，在生成 reduce value 时，除第一个 distinct 字段所在行需要保留 value 值，其余 distinct 数据行 value 字段均可为空。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/68fce9bb.png)\n\nMapReduce Multi Distinct 的实现\n\nSQL 转化为 MapReduce 的过程\n---------------------\n\n了解了 MapReduce 实现 SQL 基本操作之后，我们来看看 Hive 是如何将 SQL 转化为 MapReduce 任务的，整个编译过程分为六个阶段：\n\n1.  Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象语法树 AST Tree\n2.  遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock\n3.  遍历 QueryBlock，翻译为执行操作树 OperatorTree\n4.  逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量\n5.  遍历 OperatorTree，翻译为 MapReduce 任务\n6.  物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划\n\n下面分别对这六个阶段进行介绍\n\n### Phase1 SQL 词法，语法解析\n\n#### Antlr\n\nHive 使用 Antlr 实现 SQL 的词法和语法解析。Antlr 是一种语言识别的工具，可以用来构造领域语言。 这里不详细介绍 Antlr，只需要了解使用 Antlr 构造特定的语言只需要编写一个语法文件，定义词法和语法替换规则即可，Antlr 完成了词法分析、语法分析、语义分析、中间代码生成的过程。\n\nHive 中语法规则的定义文件在 0.10 版本以前是 Hive.g 一个文件，随着语法规则越来越复杂，由语法规则生成的 Java 解析类可能超过 Java 类文件的最大上限，0.11 版本将 Hive.g 拆成了 5 个文件，词法规则 HiveLexer.g 和语法规则的 4 个文件 SelectClauseParser.g，FromClauseParser.g，IdentifiersParser.g，HiveParser.g。\n\n#### 抽象语法树 AST Tree\n\n经过词法和语法解析后，如果需要对表达式做进一步的处理，使用 Antlr 的抽象语法树语法 Abstract Syntax Tree，在语法分析的同时将输入语句转换成抽象语法树，后续在遍历语法树时完成进一步的处理。\n\n下面的一段语法是 Hive SQL 中 SelectStatement 的语法规则，从中可以看出，SelectStatement 包含 select, from, where, groupby, having, orderby 等子句。 （在下面的语法规则中，箭头表示对于原语句的改写，改写后会加入一些特殊词标示特定语法，比如 TOK_QUERY 标示一个查询块）\n\n```SPARQL\nselectStatement\n   :\n   selectClause\n   fromClause\n   whereClause?\n   groupByClause?\n   havingClause?\n   orderByClause?\n   clusterByClause?\n   distributeByClause?\n   sortByClause?\n   limitClause? -> ^(TOK_QUERY fromClause ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE))\n                     selectClause whereClause? groupByClause? havingClause? orderByClause? clusterByClause?\n                     distributeByClause? sortByClause? limitClause?))\n   ;\n```\n\n#### 样例 SQL\n\n为了详细说明 SQL 翻译为 MapReduce 的过程，这里以一条简单的 SQL 为例，SQL 中包含一个子查询，最终将数据写入到一张表中\n\n```sql\nFROM\n( \n  SELECT\n    p.datekey datekey,\n    p.userid userid,\n    c.clienttype\n  FROM\n    detail.usersequence_client c\n    JOIN fact.orderpayment p ON p.orderid = c.orderid\n    JOIN default.user du ON du.userid = p.userid\n  WHERE p.datekey = 20131118 \n) base\nINSERT OVERWRITE TABLE `test`.`customer_kpi`\nSELECT\n  base.datekey,\n  base.clienttype,\n  count(distinct base.userid) buyer_count\nGROUP BY base.datekey, base.clienttype\n\n\n```\n\n#### SQL 生成 AST Tree\n\nAntlr 对 Hive SQL 解析的代码如下，HiveLexerX，HiveParser 分别是 Antlr 对语法文件 Hive.g 编译后自动生成的词法解析和语法解析类，在这两个类中进行复杂的解析。\n\n```sql\nHiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));    \nTokenRewriteStream tokens = new TokenRewriteStream(lexer);\nif (ctx != null) {\n  ctx.setTokenRewriteStream(tokens);\n}\nHiveParser parser = new HiveParser(tokens);                                 \nparser.setTreeAdaptor(adaptor);\nHiveParser.statement_return r = null;\ntry {\n  r = parser.statement();                                                   \n} catch (RecognitionException e) {\n  e.printStackTrace();\n  throw new ParseException(parser.errors);\n}\n\n\n```\n\n最终生成的 AST Tree 如下图右侧（使用 Antlr Works 生成，Antlr Works 是 Antlr 提供的编写语法文件的编辑器），图中只是展开了骨架的几个节点，没有完全展开。 子查询 1/2，分别对应右侧第 1/2 两个部分。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/53c6802d.png)\n\nSQL 生成 AST Tree\n\n这里注意一下内层子查询也会生成一个 TOK_DESTINATION 节点。请看上面 SelectStatement 的语法规则，这个节点是在语法改写中特意增加了的一个节点。原因是 Hive 中所有查询的数据均会保存在 HDFS 临时的文件中，无论是中间的子查询还是查询最终的结果，Insert 语句最终会将数据写入表所在的 HDFS 目录下。\n\n详细来看，将内存子查询的 from 子句展开后，得到如下 AST Tree，每个表生成一个 TOK_TABREF 节点，Join 条件生成一个 “=” 节点。其他 SQL 部分类似，不一一详述。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/f7ebe4b8.png)\n\nAST Tree\n\n### Phase2 SQL 基本组成单元 QueryBlock\n\nAST Tree 仍然非常复杂，不够结构化，不方便直接翻译为 MapReduce 程序，AST Tree 转化为 QueryBlock 就是将 SQL 进一部抽象和结构化。\n\n#### QueryBlock\n\nQueryBlock 是一条 SQL 最基本的组成单元，包括三个部分：输入源，计算过程，输出。简单来讲一个 QueryBlock 就是一个子查询。\n\n下图为 Hive 中 QueryBlock 相关对象的类图，解释图中几个重要的属性\n\n*   QB#aliasToSubq（表示 QB 类的 aliasToSubq 属性）保存子查询的 QB 对象，aliasToSubq key 值是子查询的别名\n*   QB#qbp 即 QBParseInfo 保存一个基本 SQL 单元中的给个操作部分的 AST Tree 结构，QBParseInfo#nameToDest 这个 HashMap 保存查询单元的输出，key 的形式是 inclause-i（由于 Hive 支持 Multi Insert 语句，所以可能有多个输出），value 是对应的 ASTNode 节点，即 TOK_DESTINATION 节点。类 QBParseInfo 其余 HashMap 属性分别保存输出和各个操作的 ASTNode 节点的对应关系。\n*   QBParseInfo#JoinExpr 保存 TOK_JOIN 节点。QB#QBJoinTree 是对 Join 语法树的结构化。\n*   QB#qbm 保存每个输入表的元信息，比如表在 HDFS 上的路径，保存表数据的文件格式等。\n*   QBExpr 这个对象是为了表示 Union 操作。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/fc74a4ea.png)\n\nQueryBlock\n\n#### AST Tree 生成 QueryBlock\n\nAST Tree 生成 QueryBlock 的过程是一个递归的过程，先序遍历 AST Tree，遇到不同的 Token 节点，保存到相应的属性中，主要包含以下几个过程\n\n*   TOK_QUERY => 创建 QB 对象，循环递归子节点\n*   TOK_FROM => 将表名语法部分保存到 QB 对象的`aliasToTabs`等属性中\n*   TOK_INSERT => 循环递归子节点\n*   TOK_DESTINATION => 将输出目标的语法部分保存在 QBParseInfo 对象的 nameToDest 属性中\n*   TOK_SELECT => 分别将查询表达式的语法部分保存在`destToSelExpr`、`destToAggregationExprs`、`destToDistinctFuncExprs`三个属性中\n*   TOK_WHERE => 将 Where 部分的语法保存在 QBParseInfo 对象的 destToWhereExpr 属性中\n\n最终样例 SQL 生成两个 QB 对象，QB 对象的关系如下，QB1 是外层查询，QB2 是子查询\n\n```\nQB1\n\n  \\\n\n   QB2\n```\n\n### Phase3 逻辑操作符 Operator\n\n#### Operator\n\nHive 最终生成的 MapReduce 任务，Map 阶段和 Reduce 阶段均由 OperatorTree 组成。逻辑操作符，就是在 Map 阶段或者 Reduce 阶段完成单一特定的操作。\n\n基本的操作符包括 TableScanOperator，SelectOperator，FilterOperator，JoinOperator，GroupByOperator，ReduceSinkOperator\n\n从名字就能猜出各个操作符完成的功能，TableScanOperator 从 MapReduce 框架的 Map 接口原始输入表的数据，控制扫描表的数据行数，标记是从原表中取数据。JoinOperator 完成 Join 操作。FilterOperator 完成过滤操作\n\nReduceSinkOperator 将 Map 端的字段组合序列化为 Reduce Key/value, Partition Key，只可能出现在 Map 阶段，同时也标志着 Hive 生成的 MapReduce 程序中 Map 阶段的结束。\n\nOperator 在 Map Reduce 阶段之间的数据传递都是一个流式的过程。每一个 Operator 对一行数据完成操作后之后将数据传递给 childOperator 计算。\n\nOperator 类的主要属性和方法如下\n\n*   RowSchema 表示 Operator 的输出字段\n*   InputObjInspector outputObjInspector 解析输入和输出字段\n*   processOp 接收父 Operator 传递的数据，forward 将处理好的数据传递给子 Operator 处理\n*   Hive 每一行数据经过一个 Operator 处理之后，会对字段重新编号，colExprMap 记录每个表达式经过当前 Operator 处理前后的名称对应关系，在下一个阶段逻辑优化阶段用来回溯字段名\n*   由于 Hive 的 MapReduce 程序是一个动态的程序，即不确定一个 MapReduce Job 会进行什么运算，可能是 Join，也可能是 GroupBy，所以 Operator 将所有运行时需要的参数保存在 OperatorDesc 中，OperatorDesc 在提交任务前序列化到 HDFS 上，在 MapReduce 任务执行前从 HDFS 读取并反序列化。Map 阶段 OperatorTree 在 HDFS 上的位置在 Job.getConf(“hive.exec.plan”) + “/map.xml”\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/bb36a793.png)\n\nQueryBlock\n\n#### QueryBlock 生成 Operator Tree\n\nQueryBlock 生成 Operator Tree 就是遍历上一个过程中生成的 QB 和 QBParseInfo 对象的保存语法的属性，包含如下几个步骤：\n\n*   QB#aliasToSubq => 有子查询，递归调用\n*   QB#aliasToTabs => TableScanOperator\n*   QBParseInfo#joinExpr => QBJoinTree => ReduceSinkOperator + JoinOperator\n*   QBParseInfo#destToWhereExpr => FilterOperator\n*   QBParseInfo#destToGroupby => ReduceSinkOperator + GroupByOperator\n*   QBParseInfo#destToOrderby => ReduceSinkOperator + ExtractOperator\n\n**_由于 Join/GroupBy/OrderBy 均需要在 Reduce 阶段完成，所以在生成相应操作的 Operator 之前都会先生成一个 ReduceSinkOperator，将字段组合并序列化为 Reduce Key/value, Partition Key_**\n\n接下来详细分析样例 SQL 生成 OperatorTree 的过程\n\n先序遍历上一个阶段生成的 QB 对象\n\n1.  首先根据子 QueryBlock `QB2#aliasToTabs {du=dim.user, c=detail.usersequence_client, p=fact.orderpayment}`生成 TableScanOperator\n  \n    ```\n    TableScanOperator(“dim.user”) TS[0]\n    TableScanOperator(“detail.usersequence_client”) TS[1]       TableScanOperator(“fact.orderpayment”) TS[2]\n    \n    \n    ```\n    \n2.  先序遍历`QBParseInfo#joinExpr`生成`QBJoinTree`，类`QBJoinTree`也是一个树状结构，`QBJoinTree`保存左右表的 ASTNode 和这个查询的别名，最终生成的查询树如下\n  \n    ```\n       base\n       /  \\\n      p    du\n     /      \\\n    c        p\n    ```\n    \n3.  前序遍历`QBJoinTree`，先生成`detail.usersequence_client`和`fact.orderpayment`的 Join 操作树\n  \n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/a720d129.png)\n\nJoin to Operator\n\n**_图中 TS=TableScanOperator RS=ReduceSinkOperator JOIN=JoinOperator_**\n\n1.  生成中间表与 dim.user 的 Join 操作树\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/d368c7d0.png)\n\nJoin to Operator\n\n1.  根据 QB2 `QBParseInfo#destToWhereExpr` 生成`FilterOperator`。此时 QB2 遍历完成。\n\n下图中 SelectOperator 在某些场景下会根据一些条件判断是否需要解析字段。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/383ce9ae.png)\n\nWhere to Operator\n\n**_图中 FIL= FilterOperator SEL= SelectOperator_**\n\n1.  根据 QB1 的 QBParseInfo#destToGroupby 生成 ReduceSinkOperator + GroupByOperator\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/fa192406.png)\n\nGroupBy to Operator\n\n**_图中 GBY= GroupByOperator_** **_GBY[12] 是 HASH 聚合，即在内存中通过 Hash 进行聚合运算_**\n\n1.  最终都解析完后，会生成一个 FileSinkOperator，将数据写入 HDFS\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/5cb009cc.png)\n\nFileSinkOperator\n\n**_图中 FS=FileSinkOperator_**\n\n### Phase4 逻辑层优化器\n\n大部分逻辑层优化器通过变换 OperatorTree，合并操作符，达到减少 MapReduce Job，减少 shuffle 数据量的目的。\n\n| 名称                      | 作用                                                   |\n| :------------------------ | :----------------------------------------------------- |\n| ② SimpleFetchOptimizer    | 优化没有GroupBy表达式的聚合查询                        |\n| ② MapJoinProcessor        | MapJoin，需要SQL中提供hint，0.11版本已不用             |\n| ② BucketMapJoinOptimizer  | BucketMapJoin                                          |\n| ② GroupByOptimizer        | Map端聚合                                              |\n| ① ReduceSinkDeDuplication | 合并线性的OperatorTree中partition/sort key相同的reduce |\n| ① PredicatePushDown       | 谓词前置                                               |\n| ① CorrelationOptimizer    | 利用查询中的相关性，合并有相关性的Job，HIVE-2206       |\n| ColumnPruner              | 字段剪枝                                               |\n\n表格中①的优化器均是一个 Job 干尽可能多的事情 / 合并。②的都是减少 shuffle 数据量，甚至不做 Reduce。\n\nCorrelationOptimizer 优化器非常复杂，都能利用查询中的相关性，合并有相关性的 Job，参考 [Hive Correlation Optimizer](https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer)\n\n对于样例 SQL，有两个优化器对其进行优化。下面分别介绍这两个优化器的作用，并补充一个优化器 ReduceSinkDeDuplication 的作用\n\n#### PredicatePushDown 优化器\n\n断言判断提前优化器将 OperatorTree 中的 FilterOperator 提前到 TableScanOperator 之后\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/8c45914c.png)\n\nPredicatePushDown\n\n#### NonBlockingOpDeDupProc 优化器\n\n`NonBlockingOpDeDupProc`优化器合并 SEL-SEL 或者 FIL-FIL 为一个 Operator\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/943f0313.png)\n\nNonBlockingOpDeDupProc\n\n#### ReduceSinkDeDuplication 优化器\n\nReduceSinkDeDuplication 可以合并线性相连的两个 RS。实际上 CorrelationOptimizer 是 ReduceSinkDeDuplication 的超集，能合并线性和非线性的操作 RS，但是 Hive 先实现的 ReduceSinkDeDuplication\n\n譬如下面这条 SQL 语句\n\n```sql\nfrom (select key, value from src group by key, value) s select s.key group by s.key;\n```\n\n经过前面几个阶段之后，会生成如下的 OperatorTree，两个 Tree 是相连的，这里没有画到一起\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/67f4d79f.png)\n\nReduceSinkDeDuplication\n\n这时候遍历 OperatorTree 后能发现前前后两个 RS 输出的 Key 值和 PartitionKey 如下\n\n|          | Key       | PartitionKey |\n| :------- | :-------- | ------------ |\n| childRS  | key       | key          |\n| parentRS | key,value | key,value    |\n\nReduceSinkDeDuplication 优化器检测到：1. pRS Key 完全包含 cRS Key，且排序顺序一致；2. pRS PartitionKey 完全包含 cRS PartitionKey。符合优化条件，会对执行计划进行优化。\n\nReduceSinkDeDuplication 将 childRS 和 parentheRS 与 childRS 之间的 Operator 删掉，保留的 RS 的 Key 为 key,value 字段，PartitionKey 为 key 字段。合并后的 OperatorTree 如下：\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/9ef61667.png)\n\nReduceSinkDeDuplication\n\n### Phase5 OperatorTree 生成 MapReduce Job 的过程\n\nOperatorTree 转化为 MapReduce Job 的过程分为下面几个阶段\n\n1.  对输出表生成 MoveTask\n2.  从 OperatorTree 的其中一个根节点向下深度优先遍历\n3.  ReduceSinkOperator 标示 Map/Reduce 的界限，多个 Job 间的界限\n4.  遍历其他根节点，遇过碰到 JoinOperator 合并 MapReduceTask\n5.  生成 StatTask 更新元数据\n6.  剪断 Map 与 Reduce 间的 Operator 的关系\n\n#### 对输出表生成 MoveTask\n\n由上一步 OperatorTree 只生成了一个 FileSinkOperator，直接生成一个 MoveTask，完成将最终生成的 HDFS 临时文件移动到目标表目录下\n\n```\nMoveTask[Stage-0]\nMove Operator\n```\n\n#### 开始遍历\n\n将 OperatorTree 中的所有根节点保存在一个 toWalk 的数组中，循环取出数组中的元素（省略 QB1，未画出）\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/7088b447.png)\n\n开始遍历\n\n取出最后一个元素 TS[p] 放入栈 opStack{TS[p]} 中\n\n#### Rule #1 TS% 生成 MapReduceTask 对象，确定 MapWork\n\n发现栈中的元素符合下面规则 R1（这里用 python 代码简单表示）\n\n```\n\"\".join([t + \"%\" for t in opStack]) == \"TS%\"\n```\n\n生成一个`MapReduceTask[Stage-1]`对象，`MapReduceTask[Stage-1]`对象的`MapWork`属性保存 Operator 根节点的引用。由于 OperatorTree 之间之间的 Parent Child 关系，这个时候`MapReduceTask[Stage-1]`包含了以`TS[p]`为根的所有 Operator\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/38fce288.png)\n\nStage-1 生成 Map 阶段\n\n#### Rule #2 TS%.*RS% 确定 ReduceWork\n\n继续遍历 TS[p] 的子 Operator，将子 Operator 存入栈 opStack 中 当第一个 RS 进栈后，即栈 opStack = {TS[p], FIL[18], RS[4]} 时，就会满足下面的规则 R2\n\n```\n\"\".join([t + \"%\" for t in opStack]) == \"TS%.*RS%\"\n```\n\n这时候在`MapReduceTask[Stage-1]`对象的`ReduceWork`属性保存`JOIN[5]`的引用\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/f8a8803e.png)\n\nStage-1 生成 Reduce 阶段\n\n#### Rule #3 RS%.*RS% 生成新 MapReduceTask 对象，切分 MapReduceTask\n\n继续遍历 JOIN[5] 的子 Operator，将子 Operator 存入栈 opStack 中\n\n当第二个 RS 放入栈时，即当栈`opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6]}`时，就会满足下面的规则 R3\n\n```\n\"\".join([t + \"%\" for t in opStack]) == “RS%.*RS%” //循环遍历opStack的每一个后缀数组\n```\n\n这时候创建一个新的`MapReduceTask[Stage-2]`对象，将 OperatorTree 从`JOIN[5]`和`RS[6]`之间剪开，并为`JOIN[5]`生成一个子 Operator `FS[19]`，`RS[6]`生成一个`TS[20]`，`MapReduceTask[Stage-2]`对象的`MapWork`属性保存`TS[20]`的引用。\n\n新生成的`FS[19]`将中间数据落地，存储在 HDFS 临时文件中。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/cc1c7642.png)\n\nStage-2\n\n继续遍历 RS[6] 的子 Operator，将子 Operator 存入栈 opStack 中\n\n当`opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13]}`时，又会满足 R3 规则\n\n同理生成`MapReduceTask[Stage-3]`对象，并切开 Stage-2 和 Stage-3 的 OperatorTree\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6e876f9e.png)\n\nStage-3\n\n#### R4 FS% 连接 MapReduceTask 与 MoveTask\n\n最终将所有子 Operator 存入栈中之后，`opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13], GBY[14], SEL[15], FS[17]}` 满足规则 R4\n\n```\n\"\".join([t + \"%\" for t in opStack]) == “FS%”\n```\n\n这时候将`MoveTask`与`MapReduceTask[Stage-3]`连接起来，并生成一个`StatsTask`，修改表的元信息\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6d3b67c9.png)\n\nMoveTask\n\n#### 合并 Stage\n\n此时并没有结束，还有两个根节点没有遍历。\n\n将 opStack 栈清空，将 toWalk 的第二个元素加入栈。会发现`opStack = {TS[du]}`继续满足 R1 TS%，生成`MapReduceTask[Stage-5]`\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6799e54e.png)\n\nStage-5\n\n继续从`TS[du]`向下遍历，当`opStack={TS[du], RS[7]}`时，满足规则 R2 TS%.*RS%\n\n此时将`JOIN[8]`保存为`MapReduceTask[Stage-5]`的`ReduceWork`时，发现在一个 Map 对象保存的 Operator 与 MapReduceWork 对象关系的`Map<Operator, MapReduceWork>`对象中发现，`JOIN[8]`已经存在。此时将`MapReduceTask[Stage-2]`和`MapReduceTask[Stage-5]`合并为一个 MapReduceTask\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/164ea8fd.png)\n\n合并 Stage-2 和 Stage-5\n\n同理从最后一个根节点`TS[c]`开始遍历，也会对 MapReduceTask 进行合并\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/9259eee5.png)\n\n合并 Stage-1 和 Stage-6\n\n#### 切分 Map Reduce 阶段\n\n最后一个阶段，将 MapWork 和 ReduceWork 中的 OperatorTree 以 RS 为界限剪开\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/129684ea.png)\n\n切分 Map Reduce 阶段\n\n#### OperatorTree 生成 MapReduceTask 全貌\n\n最终共生成 3 个 MapReduceTask，如下图\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/13308564.png)\n\nOperatorTree 生成 MapReduceTask 全貌\n\n### Phase6 物理层优化器\n\n这里不详细介绍每个优化器的原理，单独介绍一下 MapJoin 的优化器\n\n| 名称                                 | 作用                             |\n| :----------------------------------- | :------------------------------- |\n| Vectorizer                           | HIVE-4160，将在0.13中发布        |\n| SortMergeJoinResolver                | 与bucket配合，类似于归并排序     |\n| SamplingOptimizer                    | 并行order by优化器，在0.12中发布 |\n| CommonJoinResolver + MapJoinResolver | MapJoin优化器                    |\n\n#### MapJoin 原理\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/a4cd80c9.png)\n\nmapjoin 原理\n\nMapJoin 简单说就是在 Map 阶段将小表读入内存，顺序扫描大表完成 Join。\n\n上图是 Hive MapJoin 的原理图，出自 Facebook 工程师 Liyin Tang 的一篇介绍 Join 优化的 slice，从图中可以看出 MapJoin 分为两个阶段：\n\n1.  通过 MapReduce Local Task，将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会对 HashTableFiles 进行压缩。\n  \n2.  MapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在 Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务。\n  \n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/678dfd68.png)\n\nconditionaltask\n\n如果 Join 的两张表一张表是临时表，就会生成一个 ConditionalTask，在运行期间判断是否使用 MapJoin\n\n#### CommonJoinResolver 优化器\n\nCommonJoinResolver 优化器就是将 CommonJoin 转化为 MapJoin，转化过程如下\n\n1.  深度优先遍历 Task Tree\n2.  找到 JoinOperator，判断左右表数据量大小\n3.  对与小表 + 大表 => MapJoinTask，对于小 / 大表 + 中间表 => ConditionalTask\n\n遍历上一个阶段生成的 MapReduce 任务，发现`MapReduceTask[Stage-2]` `JOIN[8]`中有一张表为临时表，先对 Stage-2 进行深度拷贝（由于需要保留原始执行计划为 Backup Plan，所以这里将执行计划拷贝了一份），生成一个 MapJoinOperator 替代 JoinOperator，然后生成一个 MapReduceLocalWork 读取小表生成 HashTableFiles 上传至 DistributedCache 中。\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/4e209569.png)\n\nmapjoin 变换\n\nMapReduceTask 经过变换后的执行计划如下图所示\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/136eb26d.png)\n\nmapjoin 变换\n\n#### MapJoinResolver 优化器\n\nMapJoinResolver 优化器遍历 Task Tree，将所有有 local work 的 MapReduceTask 拆成两个 Task\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/1278f2fa.png)\n\nMapJoinResolver\n\n最终 MapJoinResolver 处理完之后，执行计划如下图所示\n\n![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/2a6ea0e2.png)\n\nMapJoinResolver\n\n### Hive SQL 编译过程的设计\n\n从上述整个 SQL 编译的过程，可以看出编译过程的设计有几个优点值得学习和借鉴\n\n*   使用 Antlr 开源软件定义语法规则，大大简化了词法和语法的编译解析过程，仅仅需要维护一份语法文件即可。\n*   整体思路很清晰，分阶段的设计使整个编译过程代码容易维护，使得后续各种优化器方便的以可插拔的方式开关，譬如 Hive 0.13 最新的特性 Vectorization 和对 Tez 引擎的支持都是可插拔的。\n*   每个 Operator 只完成单一的功能，简化了整个 MapReduce 程序。\n\n### 社区发展方向\n\nHive 依然在迅速的发展中，为了提升 Hive 的性能，hortonworks 公司主导的 Stinger 计划提出了一系列对 Hive 的改进，比较重要的改进有：\n\n*   Vectorization - 使 Hive 从单行单行处理数据改为批量处理方式，大大提升了指令流水线和缓存的利用率\n*   Hive on Tez - 将 Hive 底层的 MapReduce 计算框架替换为 Tez 计算框架。Tez 不仅可以支持多 Reduce 阶段的任务 MRR，还可以一次性提交执行计划，因而能更好的分配资源。\n*   Cost Based Optimizer - 使 Hive 能够自动选择最优的 Join 顺序，提高查询速度\n*   Implement insert, update, and delete in Hive with full ACID support - 支持表按主键的增量更新\n\n我们也将跟进社区的发展，结合自身的业务需要，提升 Hive 型 ETL 流程的性能\n\n### 参考\n\nAntlr: [http://www.antlr.org/](http://www.antlr.org/) \n\nWiki Antlr 介绍: [http://en.wikipedia.org/wiki/ANTLR](http://en.wikipedia.org/wiki/ANTLR) \n\nHive Wiki: [https://cwiki.apache.org/confluence/display/Hive/Home](https://cwiki.apache.org/confluence/display/Hive/Home) \n\nHiveSQL 编译过程: [http://www.slideshare.net/recruitcojp/internal-hive](http://www.slideshare.net/recruitcojp/internal-hive) \n\nJoin Optimization in Hive: [Join Strategies in Hive from the 2011 Hadoop Summit (Liyin Tang, Namit Jain)](https://cwiki.apache.org/confluence/download/attachments/27362054/Hive+Summit+2011-join.pdf?version=1&modificationDate=1309986642000) Hive Design Docs: [https://cwiki.apache.org/confluence/display/Hive/DesignDocs](https://cwiki.apache.org/confluence/display/Hive/DesignDocs)\n","slug":"Hive/Hive SQL 的编译过程","published":1,"updated":"2019-07-12T11:46:42.031Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy01ec5h0004m587axe0b376","content":"<blockquote>\n<p>原文地址 <a href=\"https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html\" target=\"_blank\" rel=\"noopener\">https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html</a></p>\n</blockquote>\n<p>Hive 是基于 Hadoop 的一个数据仓库系统，在各大公司都有广泛的应用。美团数据仓库也是基于 Hive 搭建，每天执行近万次的 Hive ETL 计算流程，负责每天数百 GB 的数据存储和分析。Hive 的稳定性和性能对我们的数据分析非常关键。</p>\n<p>在几次升级 Hive 的过程中，我们遇到了一些大大小小的问题。通过向社区的咨询和自己的努力，在解决这些问题的同时我们对 Hive 将 SQL 编译为 MapReduce 的过程有了比较深入的理解。对这一过程的理解不仅帮助我们解决了一些 Hive 的 bug，也有利于我们优化 Hive SQL，提升我们对 Hive 的掌控力，同时有能力去定制一些需要的功能。</p>\n<h2 id=\"MapReduce-实现基本-SQL-操作的原理\"><a href=\"#MapReduce-实现基本-SQL-操作的原理\" class=\"headerlink\" title=\"MapReduce 实现基本 SQL 操作的原理\"></a>MapReduce 实现基本 SQL 操作的原理</h2><p>详细讲解 SQL 编译为 MapReduce 之前，我们先来看看 MapReduce 框架实现 SQL 基本操作的原理</p>\n<h3 id=\"Join-的实现原理\"><a href=\"#Join-的实现原理\" class=\"headerlink\" title=\"Join 的实现原理\"></a>Join 的实现原理</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select u.name, o.orderid from order o join user u on o.uid = u.uid;</span><br></pre></td></tr></table></figure>\n\n<p>在 map 的输出 value 中为不同表的数据打上 tag 标记，在 reduce 阶段根据 tag 判断数据来源。MapReduce 的过程如下（这里只是说明最基本的 Join 的实现，还有其他的实现方式）</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/73cd82b9.png\" alt></p>\n<p>MapReduce CommonJoin 的实现</p>\n<h3 id=\"Group-By-的实现原理\"><a href=\"#Group-By-的实现原理\" class=\"headerlink\" title=\"Group By 的实现原理\"></a>Group By 的实现原理</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select rank, isonline, count(*) from city group by rank, isonline;</span><br></pre></td></tr></table></figure>\n\n<p>将 GroupBy 的字段组合为 map 的输出 key 值，利用 MapReduce 的排序，在 reduce 阶段保存 LastKey 区分不同的 key。MapReduce 的过程如下（当然这里只是说明 Reduce 端的非 Hash 聚合过程）</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/bcb10088.png\" alt></p>\n<p>MapReduce Group By 的实现</p>\n<h3 id=\"Distinct-的实现原理\"><a href=\"#Distinct-的实现原理\" class=\"headerlink\" title=\"Distinct 的实现原理\"></a>Distinct 的实现原理</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select dealid, count(distinct uid) num from order group by dealid;</span><br></pre></td></tr></table></figure>\n\n<p>当只有一个 distinct 字段时，如果不考虑 Map 阶段的 Hash GroupBy，只需要将 GroupBy 字段和 Distinct 字段组合为 map 输出 key，利用 mapreduce 的排序，同时将 GroupBy 字段作为 reduce 的 key，在 reduce 阶段保存 LastKey 即可完成去重</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/d7816cf9.png\" alt></p>\n<p>MapReduce Distinct 的实现</p>\n<p>如果有多个 distinct 字段呢，如下面的 SQL</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select dealid, count(distinct uid), count(distinct date) from order group by dealid;</span><br></pre></td></tr></table></figure>\n\n<p>实现方式有两种：</p>\n<p>（1）如果仍然按照上面一个 distinct 字段的方法，即下图这种实现方式，无法跟据 uid 和 date 分别排序，也就无法通过 LastKey 去重，仍然需要在 reduce 阶段在内存中通过 Hash 去重</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/16d67006.png\" alt></p>\n<p>MapReduce Multi Distinct 的实现</p>\n<p>（2）第二种实现方式，可以对所有的 distinct 字段编号，每行数据生成 n 行数据，那么相同字段就会分别排序，这时只需要在 reduce 阶段记录 LastKey 即可去重。</p>\n<p>这种实现方式很好的利用了 MapReduce 的排序，节省了 reduce 阶段去重的内存消耗，但是缺点是增加了 shuffle 的数据量。</p>\n<p>需要注意的是，在生成 reduce value 时，除第一个 distinct 字段所在行需要保留 value 值，其余 distinct 数据行 value 字段均可为空。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/68fce9bb.png\" alt></p>\n<p>MapReduce Multi Distinct 的实现</p>\n<h2 id=\"SQL-转化为-MapReduce-的过程\"><a href=\"#SQL-转化为-MapReduce-的过程\" class=\"headerlink\" title=\"SQL 转化为 MapReduce 的过程\"></a>SQL 转化为 MapReduce 的过程</h2><p>了解了 MapReduce 实现 SQL 基本操作之后，我们来看看 Hive 是如何将 SQL 转化为 MapReduce 任务的，整个编译过程分为六个阶段：</p>\n<ol>\n<li>Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象语法树 AST Tree</li>\n<li>遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock</li>\n<li>遍历 QueryBlock，翻译为执行操作树 OperatorTree</li>\n<li>逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量</li>\n<li>遍历 OperatorTree，翻译为 MapReduce 任务</li>\n<li>物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划</li>\n</ol>\n<p>下面分别对这六个阶段进行介绍</p>\n<h3 id=\"Phase1-SQL-词法，语法解析\"><a href=\"#Phase1-SQL-词法，语法解析\" class=\"headerlink\" title=\"Phase1 SQL 词法，语法解析\"></a>Phase1 SQL 词法，语法解析</h3><h4 id=\"Antlr\"><a href=\"#Antlr\" class=\"headerlink\" title=\"Antlr\"></a>Antlr</h4><p>Hive 使用 Antlr 实现 SQL 的词法和语法解析。Antlr 是一种语言识别的工具，可以用来构造领域语言。 这里不详细介绍 Antlr，只需要了解使用 Antlr 构造特定的语言只需要编写一个语法文件，定义词法和语法替换规则即可，Antlr 完成了词法分析、语法分析、语义分析、中间代码生成的过程。</p>\n<p>Hive 中语法规则的定义文件在 0.10 版本以前是 Hive.g 一个文件，随着语法规则越来越复杂，由语法规则生成的 Java 解析类可能超过 Java 类文件的最大上限，0.11 版本将 Hive.g 拆成了 5 个文件，词法规则 HiveLexer.g 和语法规则的 4 个文件 SelectClauseParser.g，FromClauseParser.g，IdentifiersParser.g，HiveParser.g。</p>\n<h4 id=\"抽象语法树-AST-Tree\"><a href=\"#抽象语法树-AST-Tree\" class=\"headerlink\" title=\"抽象语法树 AST Tree\"></a>抽象语法树 AST Tree</h4><p>经过词法和语法解析后，如果需要对表达式做进一步的处理，使用 Antlr 的抽象语法树语法 Abstract Syntax Tree，在语法分析的同时将输入语句转换成抽象语法树，后续在遍历语法树时完成进一步的处理。</p>\n<p>下面的一段语法是 Hive SQL 中 SelectStatement 的语法规则，从中可以看出，SelectStatement 包含 select, from, where, groupby, having, orderby 等子句。 （在下面的语法规则中，箭头表示对于原语句的改写，改写后会加入一些特殊词标示特定语法，比如 TOK_QUERY 标示一个查询块）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">selectStatement</span><br><span class=\"line\">   :</span><br><span class=\"line\">   selectClause</span><br><span class=\"line\">   fromClause</span><br><span class=\"line\">   whereClause?</span><br><span class=\"line\">   groupByClause?</span><br><span class=\"line\">   havingClause?</span><br><span class=\"line\">   orderByClause?</span><br><span class=\"line\">   clusterByClause?</span><br><span class=\"line\">   distributeByClause?</span><br><span class=\"line\">   sortByClause?</span><br><span class=\"line\">   limitClause? -&gt; ^(TOK_QUERY fromClause ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE))</span><br><span class=\"line\">                     selectClause whereClause? groupByClause? havingClause? orderByClause? clusterByClause?</span><br><span class=\"line\">                     distributeByClause? sortByClause? limitClause?))</span><br><span class=\"line\">   ;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"样例-SQL\"><a href=\"#样例-SQL\" class=\"headerlink\" title=\"样例 SQL\"></a>样例 SQL</h4><p>为了详细说明 SQL 翻译为 MapReduce 的过程，这里以一条简单的 SQL 为例，SQL 中包含一个子查询，最终将数据写入到一张表中</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM</span><br><span class=\"line\">( </span><br><span class=\"line\">  <span class=\"keyword\">SELECT</span></span><br><span class=\"line\">    p.datekey datekey,</span><br><span class=\"line\">    p.userid userid,</span><br><span class=\"line\">    c.clienttype</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span></span><br><span class=\"line\">    detail.usersequence_client c</span><br><span class=\"line\">    <span class=\"keyword\">JOIN</span> fact.orderpayment p <span class=\"keyword\">ON</span> p.orderid = c.orderid</span><br><span class=\"line\">    <span class=\"keyword\">JOIN</span> default.user du <span class=\"keyword\">ON</span> du.userid = p.userid</span><br><span class=\"line\">  <span class=\"keyword\">WHERE</span> p.datekey = <span class=\"number\">20131118</span> </span><br><span class=\"line\">) base</span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> OVERWRITE <span class=\"keyword\">TABLE</span> <span class=\"string\">`test`</span>.<span class=\"string\">`customer_kpi`</span></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">  base.datekey,</span><br><span class=\"line\">  base.clienttype,</span><br><span class=\"line\">  <span class=\"keyword\">count</span>(<span class=\"keyword\">distinct</span> base.userid) buyer_count</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> base.datekey, base.clienttype</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"SQL-生成-AST-Tree\"><a href=\"#SQL-生成-AST-Tree\" class=\"headerlink\" title=\"SQL 生成 AST Tree\"></a>SQL 生成 AST Tree</h4><p>Antlr 对 Hive SQL 解析的代码如下，HiveLexerX，HiveParser 分别是 Antlr 对语法文件 Hive.g 编译后自动生成的词法解析和语法解析类，在这两个类中进行复杂的解析。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));    </span><br><span class=\"line\">TokenRewriteStream tokens = new TokenRewriteStream(lexer);</span><br><span class=\"line\">if (ctx != null) &#123;</span><br><span class=\"line\">  ctx.setTokenRewriteStream(tokens);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">HiveParser parser = new HiveParser(tokens);                                 </span><br><span class=\"line\">parser.setTreeAdaptor(adaptor);</span><br><span class=\"line\">HiveParser.statement_return r = null;</span><br><span class=\"line\">try &#123;</span><br><span class=\"line\">  r = parser.statement();                                                   </span><br><span class=\"line\">&#125; catch (RecognitionException e) &#123;</span><br><span class=\"line\">  e.printStackTrace();</span><br><span class=\"line\">  throw new ParseException(parser.errors);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>最终生成的 AST Tree 如下图右侧（使用 Antlr Works 生成，Antlr Works 是 Antlr 提供的编写语法文件的编辑器），图中只是展开了骨架的几个节点，没有完全展开。 子查询 1/2，分别对应右侧第 1/2 两个部分。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/53c6802d.png\" alt></p>\n<p>SQL 生成 AST Tree</p>\n<p>这里注意一下内层子查询也会生成一个 TOK_DESTINATION 节点。请看上面 SelectStatement 的语法规则，这个节点是在语法改写中特意增加了的一个节点。原因是 Hive 中所有查询的数据均会保存在 HDFS 临时的文件中，无论是中间的子查询还是查询最终的结果，Insert 语句最终会将数据写入表所在的 HDFS 目录下。</p>\n<p>详细来看，将内存子查询的 from 子句展开后，得到如下 AST Tree，每个表生成一个 TOK_TABREF 节点，Join 条件生成一个 “=” 节点。其他 SQL 部分类似，不一一详述。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/f7ebe4b8.png\" alt></p>\n<p>AST Tree</p>\n<h3 id=\"Phase2-SQL-基本组成单元-QueryBlock\"><a href=\"#Phase2-SQL-基本组成单元-QueryBlock\" class=\"headerlink\" title=\"Phase2 SQL 基本组成单元 QueryBlock\"></a>Phase2 SQL 基本组成单元 QueryBlock</h3><p>AST Tree 仍然非常复杂，不够结构化，不方便直接翻译为 MapReduce 程序，AST Tree 转化为 QueryBlock 就是将 SQL 进一部抽象和结构化。</p>\n<h4 id=\"QueryBlock\"><a href=\"#QueryBlock\" class=\"headerlink\" title=\"QueryBlock\"></a>QueryBlock</h4><p>QueryBlock 是一条 SQL 最基本的组成单元，包括三个部分：输入源，计算过程，输出。简单来讲一个 QueryBlock 就是一个子查询。</p>\n<p>下图为 Hive 中 QueryBlock 相关对象的类图，解释图中几个重要的属性</p>\n<ul>\n<li>QB#aliasToSubq（表示 QB 类的 aliasToSubq 属性）保存子查询的 QB 对象，aliasToSubq key 值是子查询的别名</li>\n<li>QB#qbp 即 QBParseInfo 保存一个基本 SQL 单元中的给个操作部分的 AST Tree 结构，QBParseInfo#nameToDest 这个 HashMap 保存查询单元的输出，key 的形式是 inclause-i（由于 Hive 支持 Multi Insert 语句，所以可能有多个输出），value 是对应的 ASTNode 节点，即 TOK_DESTINATION 节点。类 QBParseInfo 其余 HashMap 属性分别保存输出和各个操作的 ASTNode 节点的对应关系。</li>\n<li>QBParseInfo#JoinExpr 保存 TOK_JOIN 节点。QB#QBJoinTree 是对 Join 语法树的结构化。</li>\n<li>QB#qbm 保存每个输入表的元信息，比如表在 HDFS 上的路径，保存表数据的文件格式等。</li>\n<li>QBExpr 这个对象是为了表示 Union 操作。</li>\n</ul>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/fc74a4ea.png\" alt></p>\n<p>QueryBlock</p>\n<h4 id=\"AST-Tree-生成-QueryBlock\"><a href=\"#AST-Tree-生成-QueryBlock\" class=\"headerlink\" title=\"AST Tree 生成 QueryBlock\"></a>AST Tree 生成 QueryBlock</h4><p>AST Tree 生成 QueryBlock 的过程是一个递归的过程，先序遍历 AST Tree，遇到不同的 Token 节点，保存到相应的属性中，主要包含以下几个过程</p>\n<ul>\n<li>TOK_QUERY =&gt; 创建 QB 对象，循环递归子节点</li>\n<li>TOK_FROM =&gt; 将表名语法部分保存到 QB 对象的<code>aliasToTabs</code>等属性中</li>\n<li>TOK_INSERT =&gt; 循环递归子节点</li>\n<li>TOK_DESTINATION =&gt; 将输出目标的语法部分保存在 QBParseInfo 对象的 nameToDest 属性中</li>\n<li>TOK_SELECT =&gt; 分别将查询表达式的语法部分保存在<code>destToSelExpr</code>、<code>destToAggregationExprs</code>、<code>destToDistinctFuncExprs</code>三个属性中</li>\n<li>TOK_WHERE =&gt; 将 Where 部分的语法保存在 QBParseInfo 对象的 destToWhereExpr 属性中</li>\n</ul>\n<p>最终样例 SQL 生成两个 QB 对象，QB 对象的关系如下，QB1 是外层查询，QB2 是子查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">QB1</span><br><span class=\"line\"></span><br><span class=\"line\">  \\</span><br><span class=\"line\"></span><br><span class=\"line\">   QB2</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Phase3-逻辑操作符-Operator\"><a href=\"#Phase3-逻辑操作符-Operator\" class=\"headerlink\" title=\"Phase3 逻辑操作符 Operator\"></a>Phase3 逻辑操作符 Operator</h3><h4 id=\"Operator\"><a href=\"#Operator\" class=\"headerlink\" title=\"Operator\"></a>Operator</h4><p>Hive 最终生成的 MapReduce 任务，Map 阶段和 Reduce 阶段均由 OperatorTree 组成。逻辑操作符，就是在 Map 阶段或者 Reduce 阶段完成单一特定的操作。</p>\n<p>基本的操作符包括 TableScanOperator，SelectOperator，FilterOperator，JoinOperator，GroupByOperator，ReduceSinkOperator</p>\n<p>从名字就能猜出各个操作符完成的功能，TableScanOperator 从 MapReduce 框架的 Map 接口原始输入表的数据，控制扫描表的数据行数，标记是从原表中取数据。JoinOperator 完成 Join 操作。FilterOperator 完成过滤操作</p>\n<p>ReduceSinkOperator 将 Map 端的字段组合序列化为 Reduce Key/value, Partition Key，只可能出现在 Map 阶段，同时也标志着 Hive 生成的 MapReduce 程序中 Map 阶段的结束。</p>\n<p>Operator 在 Map Reduce 阶段之间的数据传递都是一个流式的过程。每一个 Operator 对一行数据完成操作后之后将数据传递给 childOperator 计算。</p>\n<p>Operator 类的主要属性和方法如下</p>\n<ul>\n<li>RowSchema 表示 Operator 的输出字段</li>\n<li>InputObjInspector outputObjInspector 解析输入和输出字段</li>\n<li>processOp 接收父 Operator 传递的数据，forward 将处理好的数据传递给子 Operator 处理</li>\n<li>Hive 每一行数据经过一个 Operator 处理之后，会对字段重新编号，colExprMap 记录每个表达式经过当前 Operator 处理前后的名称对应关系，在下一个阶段逻辑优化阶段用来回溯字段名</li>\n<li>由于 Hive 的 MapReduce 程序是一个动态的程序，即不确定一个 MapReduce Job 会进行什么运算，可能是 Join，也可能是 GroupBy，所以 Operator 将所有运行时需要的参数保存在 OperatorDesc 中，OperatorDesc 在提交任务前序列化到 HDFS 上，在 MapReduce 任务执行前从 HDFS 读取并反序列化。Map 阶段 OperatorTree 在 HDFS 上的位置在 Job.getConf(“hive.exec.plan”) + “/map.xml”</li>\n</ul>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/bb36a793.png\" alt></p>\n<p>QueryBlock</p>\n<h4 id=\"QueryBlock-生成-Operator-Tree\"><a href=\"#QueryBlock-生成-Operator-Tree\" class=\"headerlink\" title=\"QueryBlock 生成 Operator Tree\"></a>QueryBlock 生成 Operator Tree</h4><p>QueryBlock 生成 Operator Tree 就是遍历上一个过程中生成的 QB 和 QBParseInfo 对象的保存语法的属性，包含如下几个步骤：</p>\n<ul>\n<li>QB#aliasToSubq =&gt; 有子查询，递归调用</li>\n<li>QB#aliasToTabs =&gt; TableScanOperator</li>\n<li>QBParseInfo#joinExpr =&gt; QBJoinTree =&gt; ReduceSinkOperator + JoinOperator</li>\n<li>QBParseInfo#destToWhereExpr =&gt; FilterOperator</li>\n<li>QBParseInfo#destToGroupby =&gt; ReduceSinkOperator + GroupByOperator</li>\n<li>QBParseInfo#destToOrderby =&gt; ReduceSinkOperator + ExtractOperator</li>\n</ul>\n<p><strong><em>由于 Join/GroupBy/OrderBy 均需要在 Reduce 阶段完成，所以在生成相应操作的 Operator 之前都会先生成一个 ReduceSinkOperator，将字段组合并序列化为 Reduce Key/value, Partition Key</em></strong></p>\n<p>接下来详细分析样例 SQL 生成 OperatorTree 的过程</p>\n<p>先序遍历上一个阶段生成的 QB 对象</p>\n<ol>\n<li><p>首先根据子 QueryBlock <code>QB2#aliasToTabs {du=dim.user, c=detail.usersequence_client, p=fact.orderpayment}</code>生成 TableScanOperator</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TableScanOperator(“dim.user”) TS[0]</span><br><span class=\"line\">TableScanOperator(“detail.usersequence_client”) TS[1]       TableScanOperator(“fact.orderpayment”) TS[2]</span><br></pre></td></tr></table></figure>\n\n\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>先序遍历<code>QBParseInfo#joinExpr</code>生成<code>QBJoinTree</code>，类<code>QBJoinTree</code>也是一个树状结构，<code>QBJoinTree</code>保存左右表的 ASTNode 和这个查询的别名，最终生成的查询树如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   base</span><br><span class=\"line\">   /  \\</span><br><span class=\"line\">  p    du</span><br><span class=\"line\"> /      \\</span><br><span class=\"line\">c        p</span><br></pre></td></tr></table></figure>\n\n\n</li>\n</ol>\n<ol start=\"3\">\n<li>前序遍历<code>QBJoinTree</code>，先生成<code>detail.usersequence_client</code>和<code>fact.orderpayment</code>的 Join 操作树</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/a720d129.png\" alt></p>\n<p>Join to Operator</p>\n<p><strong><em>图中 TS=TableScanOperator RS=ReduceSinkOperator JOIN=JoinOperator</em></strong></p>\n<ol>\n<li>生成中间表与 dim.user 的 Join 操作树</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/d368c7d0.png\" alt></p>\n<p>Join to Operator</p>\n<ol>\n<li>根据 QB2 <code>QBParseInfo#destToWhereExpr</code> 生成<code>FilterOperator</code>。此时 QB2 遍历完成。</li>\n</ol>\n<p>下图中 SelectOperator 在某些场景下会根据一些条件判断是否需要解析字段。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/383ce9ae.png\" alt></p>\n<p>Where to Operator</p>\n<p><strong><em>图中 FIL= FilterOperator SEL= SelectOperator</em></strong></p>\n<ol>\n<li>根据 QB1 的 QBParseInfo#destToGroupby 生成 ReduceSinkOperator + GroupByOperator</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/fa192406.png\" alt></p>\n<p>GroupBy to Operator</p>\n<p><strong><em>图中 GBY= GroupByOperator</em></strong> <strong><em>GBY[12] 是 HASH 聚合，即在内存中通过 Hash 进行聚合运算</em></strong></p>\n<ol>\n<li>最终都解析完后，会生成一个 FileSinkOperator，将数据写入 HDFS</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/5cb009cc.png\" alt></p>\n<p>FileSinkOperator</p>\n<p><strong><em>图中 FS=FileSinkOperator</em></strong></p>\n<h3 id=\"Phase4-逻辑层优化器\"><a href=\"#Phase4-逻辑层优化器\" class=\"headerlink\" title=\"Phase4 逻辑层优化器\"></a>Phase4 逻辑层优化器</h3><p>大部分逻辑层优化器通过变换 OperatorTree，合并操作符，达到减少 MapReduce Job，减少 shuffle 数据量的目的。</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">名称</th>\n<th align=\"left\">作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">② SimpleFetchOptimizer</td>\n<td align=\"left\">优化没有GroupBy表达式的聚合查询</td>\n</tr>\n<tr>\n<td align=\"left\">② MapJoinProcessor</td>\n<td align=\"left\">MapJoin，需要SQL中提供hint，0.11版本已不用</td>\n</tr>\n<tr>\n<td align=\"left\">② BucketMapJoinOptimizer</td>\n<td align=\"left\">BucketMapJoin</td>\n</tr>\n<tr>\n<td align=\"left\">② GroupByOptimizer</td>\n<td align=\"left\">Map端聚合</td>\n</tr>\n<tr>\n<td align=\"left\">① ReduceSinkDeDuplication</td>\n<td align=\"left\">合并线性的OperatorTree中partition/sort key相同的reduce</td>\n</tr>\n<tr>\n<td align=\"left\">① PredicatePushDown</td>\n<td align=\"left\">谓词前置</td>\n</tr>\n<tr>\n<td align=\"left\">① CorrelationOptimizer</td>\n<td align=\"left\">利用查询中的相关性，合并有相关性的Job，HIVE-2206</td>\n</tr>\n<tr>\n<td align=\"left\">ColumnPruner</td>\n<td align=\"left\">字段剪枝</td>\n</tr>\n</tbody></table>\n<p>表格中①的优化器均是一个 Job 干尽可能多的事情 / 合并。②的都是减少 shuffle 数据量，甚至不做 Reduce。</p>\n<p>CorrelationOptimizer 优化器非常复杂，都能利用查询中的相关性，合并有相关性的 Job，参考 <a href=\"https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer\" target=\"_blank\" rel=\"noopener\">Hive Correlation Optimizer</a></p>\n<p>对于样例 SQL，有两个优化器对其进行优化。下面分别介绍这两个优化器的作用，并补充一个优化器 ReduceSinkDeDuplication 的作用</p>\n<h4 id=\"PredicatePushDown-优化器\"><a href=\"#PredicatePushDown-优化器\" class=\"headerlink\" title=\"PredicatePushDown 优化器\"></a>PredicatePushDown 优化器</h4><p>断言判断提前优化器将 OperatorTree 中的 FilterOperator 提前到 TableScanOperator 之后</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/8c45914c.png\" alt></p>\n<p>PredicatePushDown</p>\n<h4 id=\"NonBlockingOpDeDupProc-优化器\"><a href=\"#NonBlockingOpDeDupProc-优化器\" class=\"headerlink\" title=\"NonBlockingOpDeDupProc 优化器\"></a>NonBlockingOpDeDupProc 优化器</h4><p><code>NonBlockingOpDeDupProc</code>优化器合并 SEL-SEL 或者 FIL-FIL 为一个 Operator</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/943f0313.png\" alt></p>\n<p>NonBlockingOpDeDupProc</p>\n<h4 id=\"ReduceSinkDeDuplication-优化器\"><a href=\"#ReduceSinkDeDuplication-优化器\" class=\"headerlink\" title=\"ReduceSinkDeDuplication 优化器\"></a>ReduceSinkDeDuplication 优化器</h4><p>ReduceSinkDeDuplication 可以合并线性相连的两个 RS。实际上 CorrelationOptimizer 是 ReduceSinkDeDuplication 的超集，能合并线性和非线性的操作 RS，但是 Hive 先实现的 ReduceSinkDeDuplication</p>\n<p>譬如下面这条 SQL 语句</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from (<span class=\"keyword\">select</span> <span class=\"keyword\">key</span>, <span class=\"keyword\">value</span> <span class=\"keyword\">from</span> src <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> <span class=\"keyword\">key</span>, <span class=\"keyword\">value</span>) s <span class=\"keyword\">select</span> s.key <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> s.key;</span><br></pre></td></tr></table></figure>\n\n<p>经过前面几个阶段之后，会生成如下的 OperatorTree，两个 Tree 是相连的，这里没有画到一起</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/67f4d79f.png\" alt></p>\n<p>ReduceSinkDeDuplication</p>\n<p>这时候遍历 OperatorTree 后能发现前前后两个 RS 输出的 Key 值和 PartitionKey 如下</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\"></th>\n<th align=\"left\">Key</th>\n<th>PartitionKey</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">childRS</td>\n<td align=\"left\">key</td>\n<td>key</td>\n</tr>\n<tr>\n<td align=\"left\">parentRS</td>\n<td align=\"left\">key,value</td>\n<td>key,value</td>\n</tr>\n</tbody></table>\n<p>ReduceSinkDeDuplication 优化器检测到：1. pRS Key 完全包含 cRS Key，且排序顺序一致；2. pRS PartitionKey 完全包含 cRS PartitionKey。符合优化条件，会对执行计划进行优化。</p>\n<p>ReduceSinkDeDuplication 将 childRS 和 parentheRS 与 childRS 之间的 Operator 删掉，保留的 RS 的 Key 为 key,value 字段，PartitionKey 为 key 字段。合并后的 OperatorTree 如下：</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/9ef61667.png\" alt></p>\n<p>ReduceSinkDeDuplication</p>\n<h3 id=\"Phase5-OperatorTree-生成-MapReduce-Job-的过程\"><a href=\"#Phase5-OperatorTree-生成-MapReduce-Job-的过程\" class=\"headerlink\" title=\"Phase5 OperatorTree 生成 MapReduce Job 的过程\"></a>Phase5 OperatorTree 生成 MapReduce Job 的过程</h3><p>OperatorTree 转化为 MapReduce Job 的过程分为下面几个阶段</p>\n<ol>\n<li>对输出表生成 MoveTask</li>\n<li>从 OperatorTree 的其中一个根节点向下深度优先遍历</li>\n<li>ReduceSinkOperator 标示 Map/Reduce 的界限，多个 Job 间的界限</li>\n<li>遍历其他根节点，遇过碰到 JoinOperator 合并 MapReduceTask</li>\n<li>生成 StatTask 更新元数据</li>\n<li>剪断 Map 与 Reduce 间的 Operator 的关系</li>\n</ol>\n<h4 id=\"对输出表生成-MoveTask\"><a href=\"#对输出表生成-MoveTask\" class=\"headerlink\" title=\"对输出表生成 MoveTask\"></a>对输出表生成 MoveTask</h4><p>由上一步 OperatorTree 只生成了一个 FileSinkOperator，直接生成一个 MoveTask，完成将最终生成的 HDFS 临时文件移动到目标表目录下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MoveTask[Stage-0]</span><br><span class=\"line\">Move Operator</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"开始遍历\"><a href=\"#开始遍历\" class=\"headerlink\" title=\"开始遍历\"></a>开始遍历</h4><p>将 OperatorTree 中的所有根节点保存在一个 toWalk 的数组中，循环取出数组中的元素（省略 QB1，未画出）</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/7088b447.png\" alt></p>\n<p>开始遍历</p>\n<p>取出最后一个元素 TS[p] 放入栈 opStack{TS[p]} 中</p>\n<h4 id=\"Rule-1-TS-生成-MapReduceTask-对象，确定-MapWork\"><a href=\"#Rule-1-TS-生成-MapReduceTask-对象，确定-MapWork\" class=\"headerlink\" title=\"Rule #1 TS% 生成 MapReduceTask 对象，确定 MapWork\"></a>Rule #1 TS% 生成 MapReduceTask 对象，确定 MapWork</h4><p>发现栈中的元素符合下面规则 R1（这里用 python 代码简单表示）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;.join([t + &quot;%&quot; for t in opStack]) == &quot;TS%&quot;</span><br></pre></td></tr></table></figure>\n\n<p>生成一个<code>MapReduceTask[Stage-1]</code>对象，<code>MapReduceTask[Stage-1]</code>对象的<code>MapWork</code>属性保存 Operator 根节点的引用。由于 OperatorTree 之间之间的 Parent Child 关系，这个时候<code>MapReduceTask[Stage-1]</code>包含了以<code>TS[p]</code>为根的所有 Operator</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/38fce288.png\" alt></p>\n<p>Stage-1 生成 Map 阶段</p>\n<h4 id=\"Rule-2-TS-RS-确定-ReduceWork\"><a href=\"#Rule-2-TS-RS-确定-ReduceWork\" class=\"headerlink\" title=\"Rule #2 TS%.*RS% 确定 ReduceWork\"></a>Rule #2 TS%.*RS% 确定 ReduceWork</h4><p>继续遍历 TS[p] 的子 Operator，将子 Operator 存入栈 opStack 中 当第一个 RS 进栈后，即栈 opStack = {TS[p], FIL[18], RS[4]} 时，就会满足下面的规则 R2</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;.join([t + &quot;%&quot; for t in opStack]) == &quot;TS%.*RS%&quot;</span><br></pre></td></tr></table></figure>\n\n<p>这时候在<code>MapReduceTask[Stage-1]</code>对象的<code>ReduceWork</code>属性保存<code>JOIN[5]</code>的引用</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/f8a8803e.png\" alt></p>\n<p>Stage-1 生成 Reduce 阶段</p>\n<h4 id=\"Rule-3-RS-RS-生成新-MapReduceTask-对象，切分-MapReduceTask\"><a href=\"#Rule-3-RS-RS-生成新-MapReduceTask-对象，切分-MapReduceTask\" class=\"headerlink\" title=\"Rule #3 RS%.*RS% 生成新 MapReduceTask 对象，切分 MapReduceTask\"></a>Rule #3 RS%.*RS% 生成新 MapReduceTask 对象，切分 MapReduceTask</h4><p>继续遍历 JOIN[5] 的子 Operator，将子 Operator 存入栈 opStack 中</p>\n<p>当第二个 RS 放入栈时，即当栈<code>opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6]}</code>时，就会满足下面的规则 R3</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;.join([t + &quot;%&quot; for t in opStack]) == “RS%.*RS%” //循环遍历opStack的每一个后缀数组</span><br></pre></td></tr></table></figure>\n\n<p>这时候创建一个新的<code>MapReduceTask[Stage-2]</code>对象，将 OperatorTree 从<code>JOIN[5]</code>和<code>RS[6]</code>之间剪开，并为<code>JOIN[5]</code>生成一个子 Operator <code>FS[19]</code>，<code>RS[6]</code>生成一个<code>TS[20]</code>，<code>MapReduceTask[Stage-2]</code>对象的<code>MapWork</code>属性保存<code>TS[20]</code>的引用。</p>\n<p>新生成的<code>FS[19]</code>将中间数据落地，存储在 HDFS 临时文件中。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/cc1c7642.png\" alt></p>\n<p>Stage-2</p>\n<p>继续遍历 RS[6] 的子 Operator，将子 Operator 存入栈 opStack 中</p>\n<p>当<code>opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13]}</code>时，又会满足 R3 规则</p>\n<p>同理生成<code>MapReduceTask[Stage-3]</code>对象，并切开 Stage-2 和 Stage-3 的 OperatorTree</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6e876f9e.png\" alt></p>\n<p>Stage-3</p>\n<h4 id=\"R4-FS-连接-MapReduceTask-与-MoveTask\"><a href=\"#R4-FS-连接-MapReduceTask-与-MoveTask\" class=\"headerlink\" title=\"R4 FS% 连接 MapReduceTask 与 MoveTask\"></a>R4 FS% 连接 MapReduceTask 与 MoveTask</h4><p>最终将所有子 Operator 存入栈中之后，<code>opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13], GBY[14], SEL[15], FS[17]}</code> 满足规则 R4</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;.join([t + &quot;%&quot; for t in opStack]) == “FS%”</span><br></pre></td></tr></table></figure>\n\n<p>这时候将<code>MoveTask</code>与<code>MapReduceTask[Stage-3]</code>连接起来，并生成一个<code>StatsTask</code>，修改表的元信息</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6d3b67c9.png\" alt></p>\n<p>MoveTask</p>\n<h4 id=\"合并-Stage\"><a href=\"#合并-Stage\" class=\"headerlink\" title=\"合并 Stage\"></a>合并 Stage</h4><p>此时并没有结束，还有两个根节点没有遍历。</p>\n<p>将 opStack 栈清空，将 toWalk 的第二个元素加入栈。会发现<code>opStack = {TS[du]}</code>继续满足 R1 TS%，生成<code>MapReduceTask[Stage-5]</code></p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6799e54e.png\" alt></p>\n<p>Stage-5</p>\n<p>继续从<code>TS[du]</code>向下遍历，当<code>opStack={TS[du], RS[7]}</code>时，满足规则 R2 TS%.*RS%</p>\n<p>此时将<code>JOIN[8]</code>保存为<code>MapReduceTask[Stage-5]</code>的<code>ReduceWork</code>时，发现在一个 Map 对象保存的 Operator 与 MapReduceWork 对象关系的<code>Map&lt;Operator, MapReduceWork&gt;</code>对象中发现，<code>JOIN[8]</code>已经存在。此时将<code>MapReduceTask[Stage-2]</code>和<code>MapReduceTask[Stage-5]</code>合并为一个 MapReduceTask</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/164ea8fd.png\" alt></p>\n<p>合并 Stage-2 和 Stage-5</p>\n<p>同理从最后一个根节点<code>TS[c]</code>开始遍历，也会对 MapReduceTask 进行合并</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/9259eee5.png\" alt></p>\n<p>合并 Stage-1 和 Stage-6</p>\n<h4 id=\"切分-Map-Reduce-阶段\"><a href=\"#切分-Map-Reduce-阶段\" class=\"headerlink\" title=\"切分 Map Reduce 阶段\"></a>切分 Map Reduce 阶段</h4><p>最后一个阶段，将 MapWork 和 ReduceWork 中的 OperatorTree 以 RS 为界限剪开</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/129684ea.png\" alt></p>\n<p>切分 Map Reduce 阶段</p>\n<h4 id=\"OperatorTree-生成-MapReduceTask-全貌\"><a href=\"#OperatorTree-生成-MapReduceTask-全貌\" class=\"headerlink\" title=\"OperatorTree 生成 MapReduceTask 全貌\"></a>OperatorTree 生成 MapReduceTask 全貌</h4><p>最终共生成 3 个 MapReduceTask，如下图</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/13308564.png\" alt></p>\n<p>OperatorTree 生成 MapReduceTask 全貌</p>\n<h3 id=\"Phase6-物理层优化器\"><a href=\"#Phase6-物理层优化器\" class=\"headerlink\" title=\"Phase6 物理层优化器\"></a>Phase6 物理层优化器</h3><p>这里不详细介绍每个优化器的原理，单独介绍一下 MapJoin 的优化器</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">名称</th>\n<th align=\"left\">作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Vectorizer</td>\n<td align=\"left\">HIVE-4160，将在0.13中发布</td>\n</tr>\n<tr>\n<td align=\"left\">SortMergeJoinResolver</td>\n<td align=\"left\">与bucket配合，类似于归并排序</td>\n</tr>\n<tr>\n<td align=\"left\">SamplingOptimizer</td>\n<td align=\"left\">并行order by优化器，在0.12中发布</td>\n</tr>\n<tr>\n<td align=\"left\">CommonJoinResolver + MapJoinResolver</td>\n<td align=\"left\">MapJoin优化器</td>\n</tr>\n</tbody></table>\n<h4 id=\"MapJoin-原理\"><a href=\"#MapJoin-原理\" class=\"headerlink\" title=\"MapJoin 原理\"></a>MapJoin 原理</h4><p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/a4cd80c9.png\" alt></p>\n<p>mapjoin 原理</p>\n<p>MapJoin 简单说就是在 Map 阶段将小表读入内存，顺序扫描大表完成 Join。</p>\n<p>上图是 Hive MapJoin 的原理图，出自 Facebook 工程师 Liyin Tang 的一篇介绍 Join 优化的 slice，从图中可以看出 MapJoin 分为两个阶段：</p>\n<ol>\n<li><p>通过 MapReduce Local Task，将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会对 HashTableFiles 进行压缩。</p>\n</li>\n<li><p>MapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在 Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务。</p>\n</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/678dfd68.png\" alt></p>\n<p>conditionaltask</p>\n<p>如果 Join 的两张表一张表是临时表，就会生成一个 ConditionalTask，在运行期间判断是否使用 MapJoin</p>\n<h4 id=\"CommonJoinResolver-优化器\"><a href=\"#CommonJoinResolver-优化器\" class=\"headerlink\" title=\"CommonJoinResolver 优化器\"></a>CommonJoinResolver 优化器</h4><p>CommonJoinResolver 优化器就是将 CommonJoin 转化为 MapJoin，转化过程如下</p>\n<ol>\n<li>深度优先遍历 Task Tree</li>\n<li>找到 JoinOperator，判断左右表数据量大小</li>\n<li>对与小表 + 大表 =&gt; MapJoinTask，对于小 / 大表 + 中间表 =&gt; ConditionalTask</li>\n</ol>\n<p>遍历上一个阶段生成的 MapReduce 任务，发现<code>MapReduceTask[Stage-2]</code> <code>JOIN[8]</code>中有一张表为临时表，先对 Stage-2 进行深度拷贝（由于需要保留原始执行计划为 Backup Plan，所以这里将执行计划拷贝了一份），生成一个 MapJoinOperator 替代 JoinOperator，然后生成一个 MapReduceLocalWork 读取小表生成 HashTableFiles 上传至 DistributedCache 中。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/4e209569.png\" alt></p>\n<p>mapjoin 变换</p>\n<p>MapReduceTask 经过变换后的执行计划如下图所示</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/136eb26d.png\" alt></p>\n<p>mapjoin 变换</p>\n<h4 id=\"MapJoinResolver-优化器\"><a href=\"#MapJoinResolver-优化器\" class=\"headerlink\" title=\"MapJoinResolver 优化器\"></a>MapJoinResolver 优化器</h4><p>MapJoinResolver 优化器遍历 Task Tree，将所有有 local work 的 MapReduceTask 拆成两个 Task</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/1278f2fa.png\" alt></p>\n<p>MapJoinResolver</p>\n<p>最终 MapJoinResolver 处理完之后，执行计划如下图所示</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/2a6ea0e2.png\" alt></p>\n<p>MapJoinResolver</p>\n<h3 id=\"Hive-SQL-编译过程的设计\"><a href=\"#Hive-SQL-编译过程的设计\" class=\"headerlink\" title=\"Hive SQL 编译过程的设计\"></a>Hive SQL 编译过程的设计</h3><p>从上述整个 SQL 编译的过程，可以看出编译过程的设计有几个优点值得学习和借鉴</p>\n<ul>\n<li>使用 Antlr 开源软件定义语法规则，大大简化了词法和语法的编译解析过程，仅仅需要维护一份语法文件即可。</li>\n<li>整体思路很清晰，分阶段的设计使整个编译过程代码容易维护，使得后续各种优化器方便的以可插拔的方式开关，譬如 Hive 0.13 最新的特性 Vectorization 和对 Tez 引擎的支持都是可插拔的。</li>\n<li>每个 Operator 只完成单一的功能，简化了整个 MapReduce 程序。</li>\n</ul>\n<h3 id=\"社区发展方向\"><a href=\"#社区发展方向\" class=\"headerlink\" title=\"社区发展方向\"></a>社区发展方向</h3><p>Hive 依然在迅速的发展中，为了提升 Hive 的性能，hortonworks 公司主导的 Stinger 计划提出了一系列对 Hive 的改进，比较重要的改进有：</p>\n<ul>\n<li>Vectorization - 使 Hive 从单行单行处理数据改为批量处理方式，大大提升了指令流水线和缓存的利用率</li>\n<li>Hive on Tez - 将 Hive 底层的 MapReduce 计算框架替换为 Tez 计算框架。Tez 不仅可以支持多 Reduce 阶段的任务 MRR，还可以一次性提交执行计划，因而能更好的分配资源。</li>\n<li>Cost Based Optimizer - 使 Hive 能够自动选择最优的 Join 顺序，提高查询速度</li>\n<li>Implement insert, update, and delete in Hive with full ACID support - 支持表按主键的增量更新</li>\n</ul>\n<p>我们也将跟进社区的发展，结合自身的业务需要，提升 Hive 型 ETL 流程的性能</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>Antlr: <a href=\"http://www.antlr.org/\" target=\"_blank\" rel=\"noopener\">http://www.antlr.org/</a> </p>\n<p>Wiki Antlr 介绍: <a href=\"http://en.wikipedia.org/wiki/ANTLR\" target=\"_blank\" rel=\"noopener\">http://en.wikipedia.org/wiki/ANTLR</a> </p>\n<p>Hive Wiki: <a href=\"https://cwiki.apache.org/confluence/display/Hive/Home\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/Home</a> </p>\n<p>HiveSQL 编译过程: <a href=\"http://www.slideshare.net/recruitcojp/internal-hive\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/recruitcojp/internal-hive</a> </p>\n<p>Join Optimization in Hive: <a href=\"https://cwiki.apache.org/confluence/download/attachments/27362054/Hive+Summit+2011-join.pdf?version=1&modificationDate=1309986642000\" target=\"_blank\" rel=\"noopener\">Join Strategies in Hive from the 2011 Hadoop Summit (Liyin Tang, Namit Jain)</a> Hive Design Docs: <a href=\"https://cwiki.apache.org/confluence/display/Hive/DesignDocs\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/DesignDocs</a></p>\n","site":{"data":{}},"length":15783,"excerpt":"","more":"<blockquote>\n<p>原文地址 <a href=\"https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html\" target=\"_blank\" rel=\"noopener\">https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html</a></p>\n</blockquote>\n<p>Hive 是基于 Hadoop 的一个数据仓库系统，在各大公司都有广泛的应用。美团数据仓库也是基于 Hive 搭建，每天执行近万次的 Hive ETL 计算流程，负责每天数百 GB 的数据存储和分析。Hive 的稳定性和性能对我们的数据分析非常关键。</p>\n<p>在几次升级 Hive 的过程中，我们遇到了一些大大小小的问题。通过向社区的咨询和自己的努力，在解决这些问题的同时我们对 Hive 将 SQL 编译为 MapReduce 的过程有了比较深入的理解。对这一过程的理解不仅帮助我们解决了一些 Hive 的 bug，也有利于我们优化 Hive SQL，提升我们对 Hive 的掌控力，同时有能力去定制一些需要的功能。</p>\n<h2 id=\"MapReduce-实现基本-SQL-操作的原理\"><a href=\"#MapReduce-实现基本-SQL-操作的原理\" class=\"headerlink\" title=\"MapReduce 实现基本 SQL 操作的原理\"></a>MapReduce 实现基本 SQL 操作的原理</h2><p>详细讲解 SQL 编译为 MapReduce 之前，我们先来看看 MapReduce 框架实现 SQL 基本操作的原理</p>\n<h3 id=\"Join-的实现原理\"><a href=\"#Join-的实现原理\" class=\"headerlink\" title=\"Join 的实现原理\"></a>Join 的实现原理</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select u.name, o.orderid from order o join user u on o.uid = u.uid;</span><br></pre></td></tr></table></figure>\n\n<p>在 map 的输出 value 中为不同表的数据打上 tag 标记，在 reduce 阶段根据 tag 判断数据来源。MapReduce 的过程如下（这里只是说明最基本的 Join 的实现，还有其他的实现方式）</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/73cd82b9.png\" alt></p>\n<p>MapReduce CommonJoin 的实现</p>\n<h3 id=\"Group-By-的实现原理\"><a href=\"#Group-By-的实现原理\" class=\"headerlink\" title=\"Group By 的实现原理\"></a>Group By 的实现原理</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select rank, isonline, count(*) from city group by rank, isonline;</span><br></pre></td></tr></table></figure>\n\n<p>将 GroupBy 的字段组合为 map 的输出 key 值，利用 MapReduce 的排序，在 reduce 阶段保存 LastKey 区分不同的 key。MapReduce 的过程如下（当然这里只是说明 Reduce 端的非 Hash 聚合过程）</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/bcb10088.png\" alt></p>\n<p>MapReduce Group By 的实现</p>\n<h3 id=\"Distinct-的实现原理\"><a href=\"#Distinct-的实现原理\" class=\"headerlink\" title=\"Distinct 的实现原理\"></a>Distinct 的实现原理</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select dealid, count(distinct uid) num from order group by dealid;</span><br></pre></td></tr></table></figure>\n\n<p>当只有一个 distinct 字段时，如果不考虑 Map 阶段的 Hash GroupBy，只需要将 GroupBy 字段和 Distinct 字段组合为 map 输出 key，利用 mapreduce 的排序，同时将 GroupBy 字段作为 reduce 的 key，在 reduce 阶段保存 LastKey 即可完成去重</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/d7816cf9.png\" alt></p>\n<p>MapReduce Distinct 的实现</p>\n<p>如果有多个 distinct 字段呢，如下面的 SQL</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select dealid, count(distinct uid), count(distinct date) from order group by dealid;</span><br></pre></td></tr></table></figure>\n\n<p>实现方式有两种：</p>\n<p>（1）如果仍然按照上面一个 distinct 字段的方法，即下图这种实现方式，无法跟据 uid 和 date 分别排序，也就无法通过 LastKey 去重，仍然需要在 reduce 阶段在内存中通过 Hash 去重</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/16d67006.png\" alt></p>\n<p>MapReduce Multi Distinct 的实现</p>\n<p>（2）第二种实现方式，可以对所有的 distinct 字段编号，每行数据生成 n 行数据，那么相同字段就会分别排序，这时只需要在 reduce 阶段记录 LastKey 即可去重。</p>\n<p>这种实现方式很好的利用了 MapReduce 的排序，节省了 reduce 阶段去重的内存消耗，但是缺点是增加了 shuffle 的数据量。</p>\n<p>需要注意的是，在生成 reduce value 时，除第一个 distinct 字段所在行需要保留 value 值，其余 distinct 数据行 value 字段均可为空。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/68fce9bb.png\" alt></p>\n<p>MapReduce Multi Distinct 的实现</p>\n<h2 id=\"SQL-转化为-MapReduce-的过程\"><a href=\"#SQL-转化为-MapReduce-的过程\" class=\"headerlink\" title=\"SQL 转化为 MapReduce 的过程\"></a>SQL 转化为 MapReduce 的过程</h2><p>了解了 MapReduce 实现 SQL 基本操作之后，我们来看看 Hive 是如何将 SQL 转化为 MapReduce 任务的，整个编译过程分为六个阶段：</p>\n<ol>\n<li>Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象语法树 AST Tree</li>\n<li>遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock</li>\n<li>遍历 QueryBlock，翻译为执行操作树 OperatorTree</li>\n<li>逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量</li>\n<li>遍历 OperatorTree，翻译为 MapReduce 任务</li>\n<li>物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划</li>\n</ol>\n<p>下面分别对这六个阶段进行介绍</p>\n<h3 id=\"Phase1-SQL-词法，语法解析\"><a href=\"#Phase1-SQL-词法，语法解析\" class=\"headerlink\" title=\"Phase1 SQL 词法，语法解析\"></a>Phase1 SQL 词法，语法解析</h3><h4 id=\"Antlr\"><a href=\"#Antlr\" class=\"headerlink\" title=\"Antlr\"></a>Antlr</h4><p>Hive 使用 Antlr 实现 SQL 的词法和语法解析。Antlr 是一种语言识别的工具，可以用来构造领域语言。 这里不详细介绍 Antlr，只需要了解使用 Antlr 构造特定的语言只需要编写一个语法文件，定义词法和语法替换规则即可，Antlr 完成了词法分析、语法分析、语义分析、中间代码生成的过程。</p>\n<p>Hive 中语法规则的定义文件在 0.10 版本以前是 Hive.g 一个文件，随着语法规则越来越复杂，由语法规则生成的 Java 解析类可能超过 Java 类文件的最大上限，0.11 版本将 Hive.g 拆成了 5 个文件，词法规则 HiveLexer.g 和语法规则的 4 个文件 SelectClauseParser.g，FromClauseParser.g，IdentifiersParser.g，HiveParser.g。</p>\n<h4 id=\"抽象语法树-AST-Tree\"><a href=\"#抽象语法树-AST-Tree\" class=\"headerlink\" title=\"抽象语法树 AST Tree\"></a>抽象语法树 AST Tree</h4><p>经过词法和语法解析后，如果需要对表达式做进一步的处理，使用 Antlr 的抽象语法树语法 Abstract Syntax Tree，在语法分析的同时将输入语句转换成抽象语法树，后续在遍历语法树时完成进一步的处理。</p>\n<p>下面的一段语法是 Hive SQL 中 SelectStatement 的语法规则，从中可以看出，SelectStatement 包含 select, from, where, groupby, having, orderby 等子句。 （在下面的语法规则中，箭头表示对于原语句的改写，改写后会加入一些特殊词标示特定语法，比如 TOK_QUERY 标示一个查询块）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">selectStatement</span><br><span class=\"line\">   :</span><br><span class=\"line\">   selectClause</span><br><span class=\"line\">   fromClause</span><br><span class=\"line\">   whereClause?</span><br><span class=\"line\">   groupByClause?</span><br><span class=\"line\">   havingClause?</span><br><span class=\"line\">   orderByClause?</span><br><span class=\"line\">   clusterByClause?</span><br><span class=\"line\">   distributeByClause?</span><br><span class=\"line\">   sortByClause?</span><br><span class=\"line\">   limitClause? -&gt; ^(TOK_QUERY fromClause ^(TOK_INSERT ^(TOK_DESTINATION ^(TOK_DIR TOK_TMP_FILE))</span><br><span class=\"line\">                     selectClause whereClause? groupByClause? havingClause? orderByClause? clusterByClause?</span><br><span class=\"line\">                     distributeByClause? sortByClause? limitClause?))</span><br><span class=\"line\">   ;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"样例-SQL\"><a href=\"#样例-SQL\" class=\"headerlink\" title=\"样例 SQL\"></a>样例 SQL</h4><p>为了详细说明 SQL 翻译为 MapReduce 的过程，这里以一条简单的 SQL 为例，SQL 中包含一个子查询，最终将数据写入到一张表中</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM</span><br><span class=\"line\">( </span><br><span class=\"line\">  <span class=\"keyword\">SELECT</span></span><br><span class=\"line\">    p.datekey datekey,</span><br><span class=\"line\">    p.userid userid,</span><br><span class=\"line\">    c.clienttype</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span></span><br><span class=\"line\">    detail.usersequence_client c</span><br><span class=\"line\">    <span class=\"keyword\">JOIN</span> fact.orderpayment p <span class=\"keyword\">ON</span> p.orderid = c.orderid</span><br><span class=\"line\">    <span class=\"keyword\">JOIN</span> default.user du <span class=\"keyword\">ON</span> du.userid = p.userid</span><br><span class=\"line\">  <span class=\"keyword\">WHERE</span> p.datekey = <span class=\"number\">20131118</span> </span><br><span class=\"line\">) base</span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> OVERWRITE <span class=\"keyword\">TABLE</span> <span class=\"string\">`test`</span>.<span class=\"string\">`customer_kpi`</span></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">  base.datekey,</span><br><span class=\"line\">  base.clienttype,</span><br><span class=\"line\">  <span class=\"keyword\">count</span>(<span class=\"keyword\">distinct</span> base.userid) buyer_count</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> base.datekey, base.clienttype</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"SQL-生成-AST-Tree\"><a href=\"#SQL-生成-AST-Tree\" class=\"headerlink\" title=\"SQL 生成 AST Tree\"></a>SQL 生成 AST Tree</h4><p>Antlr 对 Hive SQL 解析的代码如下，HiveLexerX，HiveParser 分别是 Antlr 对语法文件 Hive.g 编译后自动生成的词法解析和语法解析类，在这两个类中进行复杂的解析。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));    </span><br><span class=\"line\">TokenRewriteStream tokens = new TokenRewriteStream(lexer);</span><br><span class=\"line\">if (ctx != null) &#123;</span><br><span class=\"line\">  ctx.setTokenRewriteStream(tokens);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">HiveParser parser = new HiveParser(tokens);                                 </span><br><span class=\"line\">parser.setTreeAdaptor(adaptor);</span><br><span class=\"line\">HiveParser.statement_return r = null;</span><br><span class=\"line\">try &#123;</span><br><span class=\"line\">  r = parser.statement();                                                   </span><br><span class=\"line\">&#125; catch (RecognitionException e) &#123;</span><br><span class=\"line\">  e.printStackTrace();</span><br><span class=\"line\">  throw new ParseException(parser.errors);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>最终生成的 AST Tree 如下图右侧（使用 Antlr Works 生成，Antlr Works 是 Antlr 提供的编写语法文件的编辑器），图中只是展开了骨架的几个节点，没有完全展开。 子查询 1/2，分别对应右侧第 1/2 两个部分。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/53c6802d.png\" alt></p>\n<p>SQL 生成 AST Tree</p>\n<p>这里注意一下内层子查询也会生成一个 TOK_DESTINATION 节点。请看上面 SelectStatement 的语法规则，这个节点是在语法改写中特意增加了的一个节点。原因是 Hive 中所有查询的数据均会保存在 HDFS 临时的文件中，无论是中间的子查询还是查询最终的结果，Insert 语句最终会将数据写入表所在的 HDFS 目录下。</p>\n<p>详细来看，将内存子查询的 from 子句展开后，得到如下 AST Tree，每个表生成一个 TOK_TABREF 节点，Join 条件生成一个 “=” 节点。其他 SQL 部分类似，不一一详述。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/f7ebe4b8.png\" alt></p>\n<p>AST Tree</p>\n<h3 id=\"Phase2-SQL-基本组成单元-QueryBlock\"><a href=\"#Phase2-SQL-基本组成单元-QueryBlock\" class=\"headerlink\" title=\"Phase2 SQL 基本组成单元 QueryBlock\"></a>Phase2 SQL 基本组成单元 QueryBlock</h3><p>AST Tree 仍然非常复杂，不够结构化，不方便直接翻译为 MapReduce 程序，AST Tree 转化为 QueryBlock 就是将 SQL 进一部抽象和结构化。</p>\n<h4 id=\"QueryBlock\"><a href=\"#QueryBlock\" class=\"headerlink\" title=\"QueryBlock\"></a>QueryBlock</h4><p>QueryBlock 是一条 SQL 最基本的组成单元，包括三个部分：输入源，计算过程，输出。简单来讲一个 QueryBlock 就是一个子查询。</p>\n<p>下图为 Hive 中 QueryBlock 相关对象的类图，解释图中几个重要的属性</p>\n<ul>\n<li>QB#aliasToSubq（表示 QB 类的 aliasToSubq 属性）保存子查询的 QB 对象，aliasToSubq key 值是子查询的别名</li>\n<li>QB#qbp 即 QBParseInfo 保存一个基本 SQL 单元中的给个操作部分的 AST Tree 结构，QBParseInfo#nameToDest 这个 HashMap 保存查询单元的输出，key 的形式是 inclause-i（由于 Hive 支持 Multi Insert 语句，所以可能有多个输出），value 是对应的 ASTNode 节点，即 TOK_DESTINATION 节点。类 QBParseInfo 其余 HashMap 属性分别保存输出和各个操作的 ASTNode 节点的对应关系。</li>\n<li>QBParseInfo#JoinExpr 保存 TOK_JOIN 节点。QB#QBJoinTree 是对 Join 语法树的结构化。</li>\n<li>QB#qbm 保存每个输入表的元信息，比如表在 HDFS 上的路径，保存表数据的文件格式等。</li>\n<li>QBExpr 这个对象是为了表示 Union 操作。</li>\n</ul>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/fc74a4ea.png\" alt></p>\n<p>QueryBlock</p>\n<h4 id=\"AST-Tree-生成-QueryBlock\"><a href=\"#AST-Tree-生成-QueryBlock\" class=\"headerlink\" title=\"AST Tree 生成 QueryBlock\"></a>AST Tree 生成 QueryBlock</h4><p>AST Tree 生成 QueryBlock 的过程是一个递归的过程，先序遍历 AST Tree，遇到不同的 Token 节点，保存到相应的属性中，主要包含以下几个过程</p>\n<ul>\n<li>TOK_QUERY =&gt; 创建 QB 对象，循环递归子节点</li>\n<li>TOK_FROM =&gt; 将表名语法部分保存到 QB 对象的<code>aliasToTabs</code>等属性中</li>\n<li>TOK_INSERT =&gt; 循环递归子节点</li>\n<li>TOK_DESTINATION =&gt; 将输出目标的语法部分保存在 QBParseInfo 对象的 nameToDest 属性中</li>\n<li>TOK_SELECT =&gt; 分别将查询表达式的语法部分保存在<code>destToSelExpr</code>、<code>destToAggregationExprs</code>、<code>destToDistinctFuncExprs</code>三个属性中</li>\n<li>TOK_WHERE =&gt; 将 Where 部分的语法保存在 QBParseInfo 对象的 destToWhereExpr 属性中</li>\n</ul>\n<p>最终样例 SQL 生成两个 QB 对象，QB 对象的关系如下，QB1 是外层查询，QB2 是子查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">QB1</span><br><span class=\"line\"></span><br><span class=\"line\">  \\</span><br><span class=\"line\"></span><br><span class=\"line\">   QB2</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Phase3-逻辑操作符-Operator\"><a href=\"#Phase3-逻辑操作符-Operator\" class=\"headerlink\" title=\"Phase3 逻辑操作符 Operator\"></a>Phase3 逻辑操作符 Operator</h3><h4 id=\"Operator\"><a href=\"#Operator\" class=\"headerlink\" title=\"Operator\"></a>Operator</h4><p>Hive 最终生成的 MapReduce 任务，Map 阶段和 Reduce 阶段均由 OperatorTree 组成。逻辑操作符，就是在 Map 阶段或者 Reduce 阶段完成单一特定的操作。</p>\n<p>基本的操作符包括 TableScanOperator，SelectOperator，FilterOperator，JoinOperator，GroupByOperator，ReduceSinkOperator</p>\n<p>从名字就能猜出各个操作符完成的功能，TableScanOperator 从 MapReduce 框架的 Map 接口原始输入表的数据，控制扫描表的数据行数，标记是从原表中取数据。JoinOperator 完成 Join 操作。FilterOperator 完成过滤操作</p>\n<p>ReduceSinkOperator 将 Map 端的字段组合序列化为 Reduce Key/value, Partition Key，只可能出现在 Map 阶段，同时也标志着 Hive 生成的 MapReduce 程序中 Map 阶段的结束。</p>\n<p>Operator 在 Map Reduce 阶段之间的数据传递都是一个流式的过程。每一个 Operator 对一行数据完成操作后之后将数据传递给 childOperator 计算。</p>\n<p>Operator 类的主要属性和方法如下</p>\n<ul>\n<li>RowSchema 表示 Operator 的输出字段</li>\n<li>InputObjInspector outputObjInspector 解析输入和输出字段</li>\n<li>processOp 接收父 Operator 传递的数据，forward 将处理好的数据传递给子 Operator 处理</li>\n<li>Hive 每一行数据经过一个 Operator 处理之后，会对字段重新编号，colExprMap 记录每个表达式经过当前 Operator 处理前后的名称对应关系，在下一个阶段逻辑优化阶段用来回溯字段名</li>\n<li>由于 Hive 的 MapReduce 程序是一个动态的程序，即不确定一个 MapReduce Job 会进行什么运算，可能是 Join，也可能是 GroupBy，所以 Operator 将所有运行时需要的参数保存在 OperatorDesc 中，OperatorDesc 在提交任务前序列化到 HDFS 上，在 MapReduce 任务执行前从 HDFS 读取并反序列化。Map 阶段 OperatorTree 在 HDFS 上的位置在 Job.getConf(“hive.exec.plan”) + “/map.xml”</li>\n</ul>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/bb36a793.png\" alt></p>\n<p>QueryBlock</p>\n<h4 id=\"QueryBlock-生成-Operator-Tree\"><a href=\"#QueryBlock-生成-Operator-Tree\" class=\"headerlink\" title=\"QueryBlock 生成 Operator Tree\"></a>QueryBlock 生成 Operator Tree</h4><p>QueryBlock 生成 Operator Tree 就是遍历上一个过程中生成的 QB 和 QBParseInfo 对象的保存语法的属性，包含如下几个步骤：</p>\n<ul>\n<li>QB#aliasToSubq =&gt; 有子查询，递归调用</li>\n<li>QB#aliasToTabs =&gt; TableScanOperator</li>\n<li>QBParseInfo#joinExpr =&gt; QBJoinTree =&gt; ReduceSinkOperator + JoinOperator</li>\n<li>QBParseInfo#destToWhereExpr =&gt; FilterOperator</li>\n<li>QBParseInfo#destToGroupby =&gt; ReduceSinkOperator + GroupByOperator</li>\n<li>QBParseInfo#destToOrderby =&gt; ReduceSinkOperator + ExtractOperator</li>\n</ul>\n<p><strong><em>由于 Join/GroupBy/OrderBy 均需要在 Reduce 阶段完成，所以在生成相应操作的 Operator 之前都会先生成一个 ReduceSinkOperator，将字段组合并序列化为 Reduce Key/value, Partition Key</em></strong></p>\n<p>接下来详细分析样例 SQL 生成 OperatorTree 的过程</p>\n<p>先序遍历上一个阶段生成的 QB 对象</p>\n<ol>\n<li><p>首先根据子 QueryBlock <code>QB2#aliasToTabs {du=dim.user, c=detail.usersequence_client, p=fact.orderpayment}</code>生成 TableScanOperator</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">TableScanOperator(“dim.user”) TS[0]</span><br><span class=\"line\">TableScanOperator(“detail.usersequence_client”) TS[1]       TableScanOperator(“fact.orderpayment”) TS[2]</span><br></pre></td></tr></table></figure>\n\n\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>先序遍历<code>QBParseInfo#joinExpr</code>生成<code>QBJoinTree</code>，类<code>QBJoinTree</code>也是一个树状结构，<code>QBJoinTree</code>保存左右表的 ASTNode 和这个查询的别名，最终生成的查询树如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   base</span><br><span class=\"line\">   /  \\</span><br><span class=\"line\">  p    du</span><br><span class=\"line\"> /      \\</span><br><span class=\"line\">c        p</span><br></pre></td></tr></table></figure>\n\n\n</li>\n</ol>\n<ol start=\"3\">\n<li>前序遍历<code>QBJoinTree</code>，先生成<code>detail.usersequence_client</code>和<code>fact.orderpayment</code>的 Join 操作树</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/a720d129.png\" alt></p>\n<p>Join to Operator</p>\n<p><strong><em>图中 TS=TableScanOperator RS=ReduceSinkOperator JOIN=JoinOperator</em></strong></p>\n<ol>\n<li>生成中间表与 dim.user 的 Join 操作树</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/d368c7d0.png\" alt></p>\n<p>Join to Operator</p>\n<ol>\n<li>根据 QB2 <code>QBParseInfo#destToWhereExpr</code> 生成<code>FilterOperator</code>。此时 QB2 遍历完成。</li>\n</ol>\n<p>下图中 SelectOperator 在某些场景下会根据一些条件判断是否需要解析字段。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/383ce9ae.png\" alt></p>\n<p>Where to Operator</p>\n<p><strong><em>图中 FIL= FilterOperator SEL= SelectOperator</em></strong></p>\n<ol>\n<li>根据 QB1 的 QBParseInfo#destToGroupby 生成 ReduceSinkOperator + GroupByOperator</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/fa192406.png\" alt></p>\n<p>GroupBy to Operator</p>\n<p><strong><em>图中 GBY= GroupByOperator</em></strong> <strong><em>GBY[12] 是 HASH 聚合，即在内存中通过 Hash 进行聚合运算</em></strong></p>\n<ol>\n<li>最终都解析完后，会生成一个 FileSinkOperator，将数据写入 HDFS</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/5cb009cc.png\" alt></p>\n<p>FileSinkOperator</p>\n<p><strong><em>图中 FS=FileSinkOperator</em></strong></p>\n<h3 id=\"Phase4-逻辑层优化器\"><a href=\"#Phase4-逻辑层优化器\" class=\"headerlink\" title=\"Phase4 逻辑层优化器\"></a>Phase4 逻辑层优化器</h3><p>大部分逻辑层优化器通过变换 OperatorTree，合并操作符，达到减少 MapReduce Job，减少 shuffle 数据量的目的。</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">名称</th>\n<th align=\"left\">作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">② SimpleFetchOptimizer</td>\n<td align=\"left\">优化没有GroupBy表达式的聚合查询</td>\n</tr>\n<tr>\n<td align=\"left\">② MapJoinProcessor</td>\n<td align=\"left\">MapJoin，需要SQL中提供hint，0.11版本已不用</td>\n</tr>\n<tr>\n<td align=\"left\">② BucketMapJoinOptimizer</td>\n<td align=\"left\">BucketMapJoin</td>\n</tr>\n<tr>\n<td align=\"left\">② GroupByOptimizer</td>\n<td align=\"left\">Map端聚合</td>\n</tr>\n<tr>\n<td align=\"left\">① ReduceSinkDeDuplication</td>\n<td align=\"left\">合并线性的OperatorTree中partition/sort key相同的reduce</td>\n</tr>\n<tr>\n<td align=\"left\">① PredicatePushDown</td>\n<td align=\"left\">谓词前置</td>\n</tr>\n<tr>\n<td align=\"left\">① CorrelationOptimizer</td>\n<td align=\"left\">利用查询中的相关性，合并有相关性的Job，HIVE-2206</td>\n</tr>\n<tr>\n<td align=\"left\">ColumnPruner</td>\n<td align=\"left\">字段剪枝</td>\n</tr>\n</tbody></table>\n<p>表格中①的优化器均是一个 Job 干尽可能多的事情 / 合并。②的都是减少 shuffle 数据量，甚至不做 Reduce。</p>\n<p>CorrelationOptimizer 优化器非常复杂，都能利用查询中的相关性，合并有相关性的 Job，参考 <a href=\"https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer\" target=\"_blank\" rel=\"noopener\">Hive Correlation Optimizer</a></p>\n<p>对于样例 SQL，有两个优化器对其进行优化。下面分别介绍这两个优化器的作用，并补充一个优化器 ReduceSinkDeDuplication 的作用</p>\n<h4 id=\"PredicatePushDown-优化器\"><a href=\"#PredicatePushDown-优化器\" class=\"headerlink\" title=\"PredicatePushDown 优化器\"></a>PredicatePushDown 优化器</h4><p>断言判断提前优化器将 OperatorTree 中的 FilterOperator 提前到 TableScanOperator 之后</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/8c45914c.png\" alt></p>\n<p>PredicatePushDown</p>\n<h4 id=\"NonBlockingOpDeDupProc-优化器\"><a href=\"#NonBlockingOpDeDupProc-优化器\" class=\"headerlink\" title=\"NonBlockingOpDeDupProc 优化器\"></a>NonBlockingOpDeDupProc 优化器</h4><p><code>NonBlockingOpDeDupProc</code>优化器合并 SEL-SEL 或者 FIL-FIL 为一个 Operator</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/943f0313.png\" alt></p>\n<p>NonBlockingOpDeDupProc</p>\n<h4 id=\"ReduceSinkDeDuplication-优化器\"><a href=\"#ReduceSinkDeDuplication-优化器\" class=\"headerlink\" title=\"ReduceSinkDeDuplication 优化器\"></a>ReduceSinkDeDuplication 优化器</h4><p>ReduceSinkDeDuplication 可以合并线性相连的两个 RS。实际上 CorrelationOptimizer 是 ReduceSinkDeDuplication 的超集，能合并线性和非线性的操作 RS，但是 Hive 先实现的 ReduceSinkDeDuplication</p>\n<p>譬如下面这条 SQL 语句</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from (<span class=\"keyword\">select</span> <span class=\"keyword\">key</span>, <span class=\"keyword\">value</span> <span class=\"keyword\">from</span> src <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> <span class=\"keyword\">key</span>, <span class=\"keyword\">value</span>) s <span class=\"keyword\">select</span> s.key <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> s.key;</span><br></pre></td></tr></table></figure>\n\n<p>经过前面几个阶段之后，会生成如下的 OperatorTree，两个 Tree 是相连的，这里没有画到一起</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/67f4d79f.png\" alt></p>\n<p>ReduceSinkDeDuplication</p>\n<p>这时候遍历 OperatorTree 后能发现前前后两个 RS 输出的 Key 值和 PartitionKey 如下</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\"></th>\n<th align=\"left\">Key</th>\n<th>PartitionKey</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">childRS</td>\n<td align=\"left\">key</td>\n<td>key</td>\n</tr>\n<tr>\n<td align=\"left\">parentRS</td>\n<td align=\"left\">key,value</td>\n<td>key,value</td>\n</tr>\n</tbody></table>\n<p>ReduceSinkDeDuplication 优化器检测到：1. pRS Key 完全包含 cRS Key，且排序顺序一致；2. pRS PartitionKey 完全包含 cRS PartitionKey。符合优化条件，会对执行计划进行优化。</p>\n<p>ReduceSinkDeDuplication 将 childRS 和 parentheRS 与 childRS 之间的 Operator 删掉，保留的 RS 的 Key 为 key,value 字段，PartitionKey 为 key 字段。合并后的 OperatorTree 如下：</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/9ef61667.png\" alt></p>\n<p>ReduceSinkDeDuplication</p>\n<h3 id=\"Phase5-OperatorTree-生成-MapReduce-Job-的过程\"><a href=\"#Phase5-OperatorTree-生成-MapReduce-Job-的过程\" class=\"headerlink\" title=\"Phase5 OperatorTree 生成 MapReduce Job 的过程\"></a>Phase5 OperatorTree 生成 MapReduce Job 的过程</h3><p>OperatorTree 转化为 MapReduce Job 的过程分为下面几个阶段</p>\n<ol>\n<li>对输出表生成 MoveTask</li>\n<li>从 OperatorTree 的其中一个根节点向下深度优先遍历</li>\n<li>ReduceSinkOperator 标示 Map/Reduce 的界限，多个 Job 间的界限</li>\n<li>遍历其他根节点，遇过碰到 JoinOperator 合并 MapReduceTask</li>\n<li>生成 StatTask 更新元数据</li>\n<li>剪断 Map 与 Reduce 间的 Operator 的关系</li>\n</ol>\n<h4 id=\"对输出表生成-MoveTask\"><a href=\"#对输出表生成-MoveTask\" class=\"headerlink\" title=\"对输出表生成 MoveTask\"></a>对输出表生成 MoveTask</h4><p>由上一步 OperatorTree 只生成了一个 FileSinkOperator，直接生成一个 MoveTask，完成将最终生成的 HDFS 临时文件移动到目标表目录下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MoveTask[Stage-0]</span><br><span class=\"line\">Move Operator</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"开始遍历\"><a href=\"#开始遍历\" class=\"headerlink\" title=\"开始遍历\"></a>开始遍历</h4><p>将 OperatorTree 中的所有根节点保存在一个 toWalk 的数组中，循环取出数组中的元素（省略 QB1，未画出）</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/7088b447.png\" alt></p>\n<p>开始遍历</p>\n<p>取出最后一个元素 TS[p] 放入栈 opStack{TS[p]} 中</p>\n<h4 id=\"Rule-1-TS-生成-MapReduceTask-对象，确定-MapWork\"><a href=\"#Rule-1-TS-生成-MapReduceTask-对象，确定-MapWork\" class=\"headerlink\" title=\"Rule #1 TS% 生成 MapReduceTask 对象，确定 MapWork\"></a>Rule #1 TS% 生成 MapReduceTask 对象，确定 MapWork</h4><p>发现栈中的元素符合下面规则 R1（这里用 python 代码简单表示）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;.join([t + &quot;%&quot; for t in opStack]) == &quot;TS%&quot;</span><br></pre></td></tr></table></figure>\n\n<p>生成一个<code>MapReduceTask[Stage-1]</code>对象，<code>MapReduceTask[Stage-1]</code>对象的<code>MapWork</code>属性保存 Operator 根节点的引用。由于 OperatorTree 之间之间的 Parent Child 关系，这个时候<code>MapReduceTask[Stage-1]</code>包含了以<code>TS[p]</code>为根的所有 Operator</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/38fce288.png\" alt></p>\n<p>Stage-1 生成 Map 阶段</p>\n<h4 id=\"Rule-2-TS-RS-确定-ReduceWork\"><a href=\"#Rule-2-TS-RS-确定-ReduceWork\" class=\"headerlink\" title=\"Rule #2 TS%.*RS% 确定 ReduceWork\"></a>Rule #2 TS%.*RS% 确定 ReduceWork</h4><p>继续遍历 TS[p] 的子 Operator，将子 Operator 存入栈 opStack 中 当第一个 RS 进栈后，即栈 opStack = {TS[p], FIL[18], RS[4]} 时，就会满足下面的规则 R2</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;.join([t + &quot;%&quot; for t in opStack]) == &quot;TS%.*RS%&quot;</span><br></pre></td></tr></table></figure>\n\n<p>这时候在<code>MapReduceTask[Stage-1]</code>对象的<code>ReduceWork</code>属性保存<code>JOIN[5]</code>的引用</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/f8a8803e.png\" alt></p>\n<p>Stage-1 生成 Reduce 阶段</p>\n<h4 id=\"Rule-3-RS-RS-生成新-MapReduceTask-对象，切分-MapReduceTask\"><a href=\"#Rule-3-RS-RS-生成新-MapReduceTask-对象，切分-MapReduceTask\" class=\"headerlink\" title=\"Rule #3 RS%.*RS% 生成新 MapReduceTask 对象，切分 MapReduceTask\"></a>Rule #3 RS%.*RS% 生成新 MapReduceTask 对象，切分 MapReduceTask</h4><p>继续遍历 JOIN[5] 的子 Operator，将子 Operator 存入栈 opStack 中</p>\n<p>当第二个 RS 放入栈时，即当栈<code>opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6]}</code>时，就会满足下面的规则 R3</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;.join([t + &quot;%&quot; for t in opStack]) == “RS%.*RS%” //循环遍历opStack的每一个后缀数组</span><br></pre></td></tr></table></figure>\n\n<p>这时候创建一个新的<code>MapReduceTask[Stage-2]</code>对象，将 OperatorTree 从<code>JOIN[5]</code>和<code>RS[6]</code>之间剪开，并为<code>JOIN[5]</code>生成一个子 Operator <code>FS[19]</code>，<code>RS[6]</code>生成一个<code>TS[20]</code>，<code>MapReduceTask[Stage-2]</code>对象的<code>MapWork</code>属性保存<code>TS[20]</code>的引用。</p>\n<p>新生成的<code>FS[19]</code>将中间数据落地，存储在 HDFS 临时文件中。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/cc1c7642.png\" alt></p>\n<p>Stage-2</p>\n<p>继续遍历 RS[6] 的子 Operator，将子 Operator 存入栈 opStack 中</p>\n<p>当<code>opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13]}</code>时，又会满足 R3 规则</p>\n<p>同理生成<code>MapReduceTask[Stage-3]</code>对象，并切开 Stage-2 和 Stage-3 的 OperatorTree</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6e876f9e.png\" alt></p>\n<p>Stage-3</p>\n<h4 id=\"R4-FS-连接-MapReduceTask-与-MoveTask\"><a href=\"#R4-FS-连接-MapReduceTask-与-MoveTask\" class=\"headerlink\" title=\"R4 FS% 连接 MapReduceTask 与 MoveTask\"></a>R4 FS% 连接 MapReduceTask 与 MoveTask</h4><p>最终将所有子 Operator 存入栈中之后，<code>opStack = {TS[p], FIL[18], RS[4], JOIN[5], RS[6], JOIN[8], SEL[10], GBY[12], RS[13], GBY[14], SEL[15], FS[17]}</code> 满足规则 R4</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;.join([t + &quot;%&quot; for t in opStack]) == “FS%”</span><br></pre></td></tr></table></figure>\n\n<p>这时候将<code>MoveTask</code>与<code>MapReduceTask[Stage-3]</code>连接起来，并生成一个<code>StatsTask</code>，修改表的元信息</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6d3b67c9.png\" alt></p>\n<p>MoveTask</p>\n<h4 id=\"合并-Stage\"><a href=\"#合并-Stage\" class=\"headerlink\" title=\"合并 Stage\"></a>合并 Stage</h4><p>此时并没有结束，还有两个根节点没有遍历。</p>\n<p>将 opStack 栈清空，将 toWalk 的第二个元素加入栈。会发现<code>opStack = {TS[du]}</code>继续满足 R1 TS%，生成<code>MapReduceTask[Stage-5]</code></p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/6799e54e.png\" alt></p>\n<p>Stage-5</p>\n<p>继续从<code>TS[du]</code>向下遍历，当<code>opStack={TS[du], RS[7]}</code>时，满足规则 R2 TS%.*RS%</p>\n<p>此时将<code>JOIN[8]</code>保存为<code>MapReduceTask[Stage-5]</code>的<code>ReduceWork</code>时，发现在一个 Map 对象保存的 Operator 与 MapReduceWork 对象关系的<code>Map&lt;Operator, MapReduceWork&gt;</code>对象中发现，<code>JOIN[8]</code>已经存在。此时将<code>MapReduceTask[Stage-2]</code>和<code>MapReduceTask[Stage-5]</code>合并为一个 MapReduceTask</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/164ea8fd.png\" alt></p>\n<p>合并 Stage-2 和 Stage-5</p>\n<p>同理从最后一个根节点<code>TS[c]</code>开始遍历，也会对 MapReduceTask 进行合并</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/9259eee5.png\" alt></p>\n<p>合并 Stage-1 和 Stage-6</p>\n<h4 id=\"切分-Map-Reduce-阶段\"><a href=\"#切分-Map-Reduce-阶段\" class=\"headerlink\" title=\"切分 Map Reduce 阶段\"></a>切分 Map Reduce 阶段</h4><p>最后一个阶段，将 MapWork 和 ReduceWork 中的 OperatorTree 以 RS 为界限剪开</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/129684ea.png\" alt></p>\n<p>切分 Map Reduce 阶段</p>\n<h4 id=\"OperatorTree-生成-MapReduceTask-全貌\"><a href=\"#OperatorTree-生成-MapReduceTask-全貌\" class=\"headerlink\" title=\"OperatorTree 生成 MapReduceTask 全貌\"></a>OperatorTree 生成 MapReduceTask 全貌</h4><p>最终共生成 3 个 MapReduceTask，如下图</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/13308564.png\" alt></p>\n<p>OperatorTree 生成 MapReduceTask 全貌</p>\n<h3 id=\"Phase6-物理层优化器\"><a href=\"#Phase6-物理层优化器\" class=\"headerlink\" title=\"Phase6 物理层优化器\"></a>Phase6 物理层优化器</h3><p>这里不详细介绍每个优化器的原理，单独介绍一下 MapJoin 的优化器</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">名称</th>\n<th align=\"left\">作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Vectorizer</td>\n<td align=\"left\">HIVE-4160，将在0.13中发布</td>\n</tr>\n<tr>\n<td align=\"left\">SortMergeJoinResolver</td>\n<td align=\"left\">与bucket配合，类似于归并排序</td>\n</tr>\n<tr>\n<td align=\"left\">SamplingOptimizer</td>\n<td align=\"left\">并行order by优化器，在0.12中发布</td>\n</tr>\n<tr>\n<td align=\"left\">CommonJoinResolver + MapJoinResolver</td>\n<td align=\"left\">MapJoin优化器</td>\n</tr>\n</tbody></table>\n<h4 id=\"MapJoin-原理\"><a href=\"#MapJoin-原理\" class=\"headerlink\" title=\"MapJoin 原理\"></a>MapJoin 原理</h4><p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/a4cd80c9.png\" alt></p>\n<p>mapjoin 原理</p>\n<p>MapJoin 简单说就是在 Map 阶段将小表读入内存，顺序扫描大表完成 Join。</p>\n<p>上图是 Hive MapJoin 的原理图，出自 Facebook 工程师 Liyin Tang 的一篇介绍 Join 优化的 slice，从图中可以看出 MapJoin 分为两个阶段：</p>\n<ol>\n<li><p>通过 MapReduce Local Task，将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会对 HashTableFiles 进行压缩。</p>\n</li>\n<li><p>MapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在 Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务。</p>\n</li>\n</ol>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/678dfd68.png\" alt></p>\n<p>conditionaltask</p>\n<p>如果 Join 的两张表一张表是临时表，就会生成一个 ConditionalTask，在运行期间判断是否使用 MapJoin</p>\n<h4 id=\"CommonJoinResolver-优化器\"><a href=\"#CommonJoinResolver-优化器\" class=\"headerlink\" title=\"CommonJoinResolver 优化器\"></a>CommonJoinResolver 优化器</h4><p>CommonJoinResolver 优化器就是将 CommonJoin 转化为 MapJoin，转化过程如下</p>\n<ol>\n<li>深度优先遍历 Task Tree</li>\n<li>找到 JoinOperator，判断左右表数据量大小</li>\n<li>对与小表 + 大表 =&gt; MapJoinTask，对于小 / 大表 + 中间表 =&gt; ConditionalTask</li>\n</ol>\n<p>遍历上一个阶段生成的 MapReduce 任务，发现<code>MapReduceTask[Stage-2]</code> <code>JOIN[8]</code>中有一张表为临时表，先对 Stage-2 进行深度拷贝（由于需要保留原始执行计划为 Backup Plan，所以这里将执行计划拷贝了一份），生成一个 MapJoinOperator 替代 JoinOperator，然后生成一个 MapReduceLocalWork 读取小表生成 HashTableFiles 上传至 DistributedCache 中。</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/4e209569.png\" alt></p>\n<p>mapjoin 变换</p>\n<p>MapReduceTask 经过变换后的执行计划如下图所示</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/136eb26d.png\" alt></p>\n<p>mapjoin 变换</p>\n<h4 id=\"MapJoinResolver-优化器\"><a href=\"#MapJoinResolver-优化器\" class=\"headerlink\" title=\"MapJoinResolver 优化器\"></a>MapJoinResolver 优化器</h4><p>MapJoinResolver 优化器遍历 Task Tree，将所有有 local work 的 MapReduceTask 拆成两个 Task</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/1278f2fa.png\" alt></p>\n<p>MapJoinResolver</p>\n<p>最终 MapJoinResolver 处理完之后，执行计划如下图所示</p>\n<p><img src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2014/2a6ea0e2.png\" alt></p>\n<p>MapJoinResolver</p>\n<h3 id=\"Hive-SQL-编译过程的设计\"><a href=\"#Hive-SQL-编译过程的设计\" class=\"headerlink\" title=\"Hive SQL 编译过程的设计\"></a>Hive SQL 编译过程的设计</h3><p>从上述整个 SQL 编译的过程，可以看出编译过程的设计有几个优点值得学习和借鉴</p>\n<ul>\n<li>使用 Antlr 开源软件定义语法规则，大大简化了词法和语法的编译解析过程，仅仅需要维护一份语法文件即可。</li>\n<li>整体思路很清晰，分阶段的设计使整个编译过程代码容易维护，使得后续各种优化器方便的以可插拔的方式开关，譬如 Hive 0.13 最新的特性 Vectorization 和对 Tez 引擎的支持都是可插拔的。</li>\n<li>每个 Operator 只完成单一的功能，简化了整个 MapReduce 程序。</li>\n</ul>\n<h3 id=\"社区发展方向\"><a href=\"#社区发展方向\" class=\"headerlink\" title=\"社区发展方向\"></a>社区发展方向</h3><p>Hive 依然在迅速的发展中，为了提升 Hive 的性能，hortonworks 公司主导的 Stinger 计划提出了一系列对 Hive 的改进，比较重要的改进有：</p>\n<ul>\n<li>Vectorization - 使 Hive 从单行单行处理数据改为批量处理方式，大大提升了指令流水线和缓存的利用率</li>\n<li>Hive on Tez - 将 Hive 底层的 MapReduce 计算框架替换为 Tez 计算框架。Tez 不仅可以支持多 Reduce 阶段的任务 MRR，还可以一次性提交执行计划，因而能更好的分配资源。</li>\n<li>Cost Based Optimizer - 使 Hive 能够自动选择最优的 Join 顺序，提高查询速度</li>\n<li>Implement insert, update, and delete in Hive with full ACID support - 支持表按主键的增量更新</li>\n</ul>\n<p>我们也将跟进社区的发展，结合自身的业务需要，提升 Hive 型 ETL 流程的性能</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>Antlr: <a href=\"http://www.antlr.org/\" target=\"_blank\" rel=\"noopener\">http://www.antlr.org/</a> </p>\n<p>Wiki Antlr 介绍: <a href=\"http://en.wikipedia.org/wiki/ANTLR\" target=\"_blank\" rel=\"noopener\">http://en.wikipedia.org/wiki/ANTLR</a> </p>\n<p>Hive Wiki: <a href=\"https://cwiki.apache.org/confluence/display/Hive/Home\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/Home</a> </p>\n<p>HiveSQL 编译过程: <a href=\"http://www.slideshare.net/recruitcojp/internal-hive\" target=\"_blank\" rel=\"noopener\">http://www.slideshare.net/recruitcojp/internal-hive</a> </p>\n<p>Join Optimization in Hive: <a href=\"https://cwiki.apache.org/confluence/download/attachments/27362054/Hive+Summit+2011-join.pdf?version=1&modificationDate=1309986642000\" target=\"_blank\" rel=\"noopener\">Join Strategies in Hive from the 2011 Hadoop Summit (Liyin Tang, Namit Jain)</a> Hive Design Docs: <a href=\"https://cwiki.apache.org/confluence/display/Hive/DesignDocs\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/DesignDocs</a></p>\n"},{"title":"Hive中Join的类型和用法","date":"2019-07-09T04:33:48.000Z","_content":"\n一图以示之：\n\n\n\n![join-all](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/join-all.jpg)\n\n| 类型     | 定义                                                         | 语法             | 等效            |\n| -------- | ------------------------------------------------------------ | ---------------- | --------------- |\n| 内连接   | 只连接匹配的行                                               | inner join       | join            |\n| 左外连接 | 包含左边表的全部行以及右边表中全部匹配的行                   | left outer join  | left join       |\n| 右外连接 | 包含右边表的全部行以及左边表中全部匹配的行                   | right outer join | right join      |\n| 全外连接 | 包含左、右两个表的全部行，不管在另一边表中是否存在与它们匹配的行 | full out join    | full join       |\n| 左半连接 | 以left semi join关键字前面的表为主表，返回主表的KEY也在副表中的记录 | left semi join   |                 |\n| 交叉连接 | 生成笛卡尔积—它不适用任何匹配或者选取条件，而是直接讲一个数据源中的每一行与另个数据源的每一行匹配 | cross join       | join 不加on条件 |\n\n### in/exists \n\n#### exists的执行原理：\n\n> 对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率；\n\n#### in的执行原理\n\n> 是把外表和内表做hash连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。\n\n#### 使用场景说明：\n```sql\nSELECT\n\tc.CustomerId,\n\tc.CompanyName\nFROM\n\tCustomers c\nWHERE\n\tEXISTS\n\t(\n\t\tSELECT OrderID FROM Orders o WHERE o.CustomerID = c.CustomerID\n\t)\n```\n\n> 分析：这里使用exists的原因是，订单表里面可能记录很大，而客户表是一个相当较小的表，这样查询的话 是一种优化方式。\n\n```sql\nSELECT * FROM Orders WHERE CustomerId in (id1,id2,id3);\n\nSELECT\n\t*\nFROM\n\tOrders\nWHERE\n\tCustomerID in \n\t(\n\t\tSELECT CustomerId FROM Customers WHERE customer_type = 1\n\t)\t\n```\n\n> 分析 ：这里我只查找客户编号是id1,id2,id3 的人的订单信息.  in就特别合适了。\n\n**注意：in后面子查询可以使用任何子查询（或常数），exists后面的只能是相关子查询（不然没意义）**\n\n\n\n### left semi join 与 inner join 相同点与区别\n\nhive 的 join 类型有好几种，其实都是把 MR 中的几种方式都封装实现了，其中 join on、left semi join 算是里边具有代表性，且使用频率较高的 join 方式。\n\n1.联系\n\n他们都是 hive join 方式的一种，join on 属于 common join（shuffle join/reduce join），而 left semi join 则属于 map join（broadcast join）的一种变体，从名字可以看出他们的实现原理有差异。\n\n2.区别\n\n（1）Semi Join，也叫半连接，是从分布式数据库中借鉴过来的方法。它的产生动机是：对于reduce side join，跨机器的数据传输量非常大，这成了join操作的一个瓶颈，如果能够在map端过滤掉不会参加join操作的数据，则可以大大节省网络IO，提升执行效率。\n实现方法很简单：选取一个小表，假设是File1，将其参与join的key抽取出来，保存到文件File3中，File3文件一般很小，可以放到内存中。在map阶段，使用DistributedCache将File3复制到各个TaskTracker上，然后将File2中不在File3中的key对应的记录过滤掉，剩下的reduce阶段的工作与reduce side join相同。\n由于 hive 中没有 in/exist 这样的子句（新版将支持），所以需要将这种类型的子句转成 left semi join。left semi join 是只传递表的 join key 给 map 阶段 , 如果 key 足够小还是执行 map join, 如果不是则还是 common join。\n\n（2）left semi join 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。\n\n（3）对待右表中重复key的处理方式差异：因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join on 则会一直遍历。\n\n最后的结果是这会造成性能，以及 join 结果上的差异。\n\n（4）left semi join 中最后 select 的结果只许出现左表，因为右表只有 join key 参与关联计算了，而 join on 默认是整个关系模型都参与计算了。\n\n> 注意：大多数情况下 JOIN ON 和 left semi on 是对等的，但是在上述情况下会出现重复记录，导致结果差异，所以大家在使用的时候最好能了解这两种方式的原理，避免掉“坑”。\n\n\n\n示例：\n\n```sql\nSELECT a.key, a.val FROM a WHERE a.key in (SELECT b.key FROM b);\n```\n\n可以被改写为：\n\n ```sql\nSELECT a.key, a.val FROM a LEFT SEMI JOIN b on (a.key = b.key)\n ```\n\n特点：\n\n1、left semi join 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。\n\n2、left semi join 是只传递表的 join key 给 map 阶段，因此left semi join 中最后 select 的结果只许出现左表。\n\n3、因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 会产生多条，也会导致 left semi join 的性能更高。 \n\n比如以下A表和B表进行 join 或 left semi join，然后 select 出所有字段，结果区别如下：\n\n![left_smi_join](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/left_smi_join.jpg)\n\n注意：蓝色叉的那一列实际是不存在left semi join中的，因为最后 select 的结果只许出现左表。\n\n \n\n---\n\n参考：\n\n[Hive 中的 LEFT SEMI JOIN 与 JOIN ON](https://www.cnblogs.com/wqbin/p/11023008.html)\n\n[Hive中Join的类型和用法](https://www.cnblogs.com/liupengpengg/p/7908274.html)\n\nhttps://zhuanlan.zhihu.com/p/25435517\n\n[hive 的 left semi join 讲解](https://blog.csdn.net/happyrocking/article/details/79885071)\n\n[MapJoin和ReduceJoin区别及优化](https://blog.csdn.net/qq_17776287/article/details/78567514)","source":"_posts/Hive/join.md","raw":"---\ntitle: Hive中Join的类型和用法\ndate: 2019-07-09 12:33:48\ntags: \n    - sql\ncategories: \n    - Hive\n---\n\n一图以示之：\n\n\n\n![join-all](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/join-all.jpg)\n\n| 类型     | 定义                                                         | 语法             | 等效            |\n| -------- | ------------------------------------------------------------ | ---------------- | --------------- |\n| 内连接   | 只连接匹配的行                                               | inner join       | join            |\n| 左外连接 | 包含左边表的全部行以及右边表中全部匹配的行                   | left outer join  | left join       |\n| 右外连接 | 包含右边表的全部行以及左边表中全部匹配的行                   | right outer join | right join      |\n| 全外连接 | 包含左、右两个表的全部行，不管在另一边表中是否存在与它们匹配的行 | full out join    | full join       |\n| 左半连接 | 以left semi join关键字前面的表为主表，返回主表的KEY也在副表中的记录 | left semi join   |                 |\n| 交叉连接 | 生成笛卡尔积—它不适用任何匹配或者选取条件，而是直接讲一个数据源中的每一行与另个数据源的每一行匹配 | cross join       | join 不加on条件 |\n\n### in/exists \n\n#### exists的执行原理：\n\n> 对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率；\n\n#### in的执行原理\n\n> 是把外表和内表做hash连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。\n\n#### 使用场景说明：\n```sql\nSELECT\n\tc.CustomerId,\n\tc.CompanyName\nFROM\n\tCustomers c\nWHERE\n\tEXISTS\n\t(\n\t\tSELECT OrderID FROM Orders o WHERE o.CustomerID = c.CustomerID\n\t)\n```\n\n> 分析：这里使用exists的原因是，订单表里面可能记录很大，而客户表是一个相当较小的表，这样查询的话 是一种优化方式。\n\n```sql\nSELECT * FROM Orders WHERE CustomerId in (id1,id2,id3);\n\nSELECT\n\t*\nFROM\n\tOrders\nWHERE\n\tCustomerID in \n\t(\n\t\tSELECT CustomerId FROM Customers WHERE customer_type = 1\n\t)\t\n```\n\n> 分析 ：这里我只查找客户编号是id1,id2,id3 的人的订单信息.  in就特别合适了。\n\n**注意：in后面子查询可以使用任何子查询（或常数），exists后面的只能是相关子查询（不然没意义）**\n\n\n\n### left semi join 与 inner join 相同点与区别\n\nhive 的 join 类型有好几种，其实都是把 MR 中的几种方式都封装实现了，其中 join on、left semi join 算是里边具有代表性，且使用频率较高的 join 方式。\n\n1.联系\n\n他们都是 hive join 方式的一种，join on 属于 common join（shuffle join/reduce join），而 left semi join 则属于 map join（broadcast join）的一种变体，从名字可以看出他们的实现原理有差异。\n\n2.区别\n\n（1）Semi Join，也叫半连接，是从分布式数据库中借鉴过来的方法。它的产生动机是：对于reduce side join，跨机器的数据传输量非常大，这成了join操作的一个瓶颈，如果能够在map端过滤掉不会参加join操作的数据，则可以大大节省网络IO，提升执行效率。\n实现方法很简单：选取一个小表，假设是File1，将其参与join的key抽取出来，保存到文件File3中，File3文件一般很小，可以放到内存中。在map阶段，使用DistributedCache将File3复制到各个TaskTracker上，然后将File2中不在File3中的key对应的记录过滤掉，剩下的reduce阶段的工作与reduce side join相同。\n由于 hive 中没有 in/exist 这样的子句（新版将支持），所以需要将这种类型的子句转成 left semi join。left semi join 是只传递表的 join key 给 map 阶段 , 如果 key 足够小还是执行 map join, 如果不是则还是 common join。\n\n（2）left semi join 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。\n\n（3）对待右表中重复key的处理方式差异：因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join on 则会一直遍历。\n\n最后的结果是这会造成性能，以及 join 结果上的差异。\n\n（4）left semi join 中最后 select 的结果只许出现左表，因为右表只有 join key 参与关联计算了，而 join on 默认是整个关系模型都参与计算了。\n\n> 注意：大多数情况下 JOIN ON 和 left semi on 是对等的，但是在上述情况下会出现重复记录，导致结果差异，所以大家在使用的时候最好能了解这两种方式的原理，避免掉“坑”。\n\n\n\n示例：\n\n```sql\nSELECT a.key, a.val FROM a WHERE a.key in (SELECT b.key FROM b);\n```\n\n可以被改写为：\n\n ```sql\nSELECT a.key, a.val FROM a LEFT SEMI JOIN b on (a.key = b.key)\n ```\n\n特点：\n\n1、left semi join 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。\n\n2、left semi join 是只传递表的 join key 给 map 阶段，因此left semi join 中最后 select 的结果只许出现左表。\n\n3、因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 会产生多条，也会导致 left semi join 的性能更高。 \n\n比如以下A表和B表进行 join 或 left semi join，然后 select 出所有字段，结果区别如下：\n\n![left_smi_join](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/left_smi_join.jpg)\n\n注意：蓝色叉的那一列实际是不存在left semi join中的，因为最后 select 的结果只许出现左表。\n\n \n\n---\n\n参考：\n\n[Hive 中的 LEFT SEMI JOIN 与 JOIN ON](https://www.cnblogs.com/wqbin/p/11023008.html)\n\n[Hive中Join的类型和用法](https://www.cnblogs.com/liupengpengg/p/7908274.html)\n\nhttps://zhuanlan.zhihu.com/p/25435517\n\n[hive 的 left semi join 讲解](https://blog.csdn.net/happyrocking/article/details/79885071)\n\n[MapJoin和ReduceJoin区别及优化](https://blog.csdn.net/qq_17776287/article/details/78567514)","slug":"Hive/join","published":1,"updated":"2019-07-09T07:49:09.822Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy01ec5l0007m587xk3ocp4b","content":"<p>一图以示之：</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/join-all.jpg\" alt=\"join-all\"></p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>定义</th>\n<th>语法</th>\n<th>等效</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>内连接</td>\n<td>只连接匹配的行</td>\n<td>inner join</td>\n<td>join</td>\n</tr>\n<tr>\n<td>左外连接</td>\n<td>包含左边表的全部行以及右边表中全部匹配的行</td>\n<td>left outer join</td>\n<td>left join</td>\n</tr>\n<tr>\n<td>右外连接</td>\n<td>包含右边表的全部行以及左边表中全部匹配的行</td>\n<td>right outer join</td>\n<td>right join</td>\n</tr>\n<tr>\n<td>全外连接</td>\n<td>包含左、右两个表的全部行，不管在另一边表中是否存在与它们匹配的行</td>\n<td>full out join</td>\n<td>full join</td>\n</tr>\n<tr>\n<td>左半连接</td>\n<td>以left semi join关键字前面的表为主表，返回主表的KEY也在副表中的记录</td>\n<td>left semi join</td>\n<td></td>\n</tr>\n<tr>\n<td>交叉连接</td>\n<td>生成笛卡尔积—它不适用任何匹配或者选取条件，而是直接讲一个数据源中的每一行与另个数据源的每一行匹配</td>\n<td>cross join</td>\n<td>join 不加on条件</td>\n</tr>\n</tbody></table>\n<h3 id=\"in-exists\"><a href=\"#in-exists\" class=\"headerlink\" title=\"in/exists\"></a>in/exists</h3><h4 id=\"exists的执行原理：\"><a href=\"#exists的执行原理：\" class=\"headerlink\" title=\"exists的执行原理：\"></a>exists的执行原理：</h4><blockquote>\n<p>对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率；</p>\n</blockquote>\n<h4 id=\"in的执行原理\"><a href=\"#in的执行原理\" class=\"headerlink\" title=\"in的执行原理\"></a>in的执行原理</h4><blockquote>\n<p>是把外表和内表做hash连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。</p>\n</blockquote>\n<h4 id=\"使用场景说明：\"><a href=\"#使用场景说明：\" class=\"headerlink\" title=\"使用场景说明：\"></a>使用场景说明：</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">\tc.CustomerId,</span><br><span class=\"line\">\tc.CompanyName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">\tCustomers c</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">\t<span class=\"keyword\">EXISTS</span></span><br><span class=\"line\">\t(</span><br><span class=\"line\">\t\t<span class=\"keyword\">SELECT</span> OrderID <span class=\"keyword\">FROM</span> Orders o <span class=\"keyword\">WHERE</span> o.CustomerID = c.CustomerID</span><br><span class=\"line\">\t)</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>分析：这里使用exists的原因是，订单表里面可能记录很大，而客户表是一个相当较小的表，这样查询的话 是一种优化方式。</p>\n</blockquote>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> * <span class=\"keyword\">FROM</span> Orders <span class=\"keyword\">WHERE</span> CustomerId <span class=\"keyword\">in</span> (id1,id2,id3);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">\t*</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">\tOrders</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">\tCustomerID <span class=\"keyword\">in</span> </span><br><span class=\"line\">\t(</span><br><span class=\"line\">\t\t<span class=\"keyword\">SELECT</span> CustomerId <span class=\"keyword\">FROM</span> Customers <span class=\"keyword\">WHERE</span> customer_type = <span class=\"number\">1</span></span><br><span class=\"line\">\t)</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>分析 ：这里我只查找客户编号是id1,id2,id3 的人的订单信息.  in就特别合适了。</p>\n</blockquote>\n<p><strong>注意：in后面子查询可以使用任何子查询（或常数），exists后面的只能是相关子查询（不然没意义）</strong></p>\n<h3 id=\"left-semi-join-与-inner-join-相同点与区别\"><a href=\"#left-semi-join-与-inner-join-相同点与区别\" class=\"headerlink\" title=\"left semi join 与 inner join 相同点与区别\"></a>left semi join 与 inner join 相同点与区别</h3><p>hive 的 join 类型有好几种，其实都是把 MR 中的几种方式都封装实现了，其中 join on、left semi join 算是里边具有代表性，且使用频率较高的 join 方式。</p>\n<p>1.联系</p>\n<p>他们都是 hive join 方式的一种，join on 属于 common join（shuffle join/reduce join），而 left semi join 则属于 map join（broadcast join）的一种变体，从名字可以看出他们的实现原理有差异。</p>\n<p>2.区别</p>\n<p>（1）Semi Join，也叫半连接，是从分布式数据库中借鉴过来的方法。它的产生动机是：对于reduce side join，跨机器的数据传输量非常大，这成了join操作的一个瓶颈，如果能够在map端过滤掉不会参加join操作的数据，则可以大大节省网络IO，提升执行效率。<br>实现方法很简单：选取一个小表，假设是File1，将其参与join的key抽取出来，保存到文件File3中，File3文件一般很小，可以放到内存中。在map阶段，使用DistributedCache将File3复制到各个TaskTracker上，然后将File2中不在File3中的key对应的记录过滤掉，剩下的reduce阶段的工作与reduce side join相同。<br>由于 hive 中没有 in/exist 这样的子句（新版将支持），所以需要将这种类型的子句转成 left semi join。left semi join 是只传递表的 join key 给 map 阶段 , 如果 key 足够小还是执行 map join, 如果不是则还是 common join。</p>\n<p>（2）left semi join 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p>\n<p>（3）对待右表中重复key的处理方式差异：因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join on 则会一直遍历。</p>\n<p>最后的结果是这会造成性能，以及 join 结果上的差异。</p>\n<p>（4）left semi join 中最后 select 的结果只许出现左表，因为右表只有 join key 参与关联计算了，而 join on 默认是整个关系模型都参与计算了。</p>\n<blockquote>\n<p>注意：大多数情况下 JOIN ON 和 left semi on 是对等的，但是在上述情况下会出现重复记录，导致结果差异，所以大家在使用的时候最好能了解这两种方式的原理，避免掉“坑”。</p>\n</blockquote>\n<p>示例：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> a.key, a.val <span class=\"keyword\">FROM</span> a <span class=\"keyword\">WHERE</span> a.key <span class=\"keyword\">in</span> (<span class=\"keyword\">SELECT</span> b.key <span class=\"keyword\">FROM</span> b);</span><br></pre></td></tr></table></figure>\n\n<p>可以被改写为：</p>\n <figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> a.key, a.val <span class=\"keyword\">FROM</span> a <span class=\"keyword\">LEFT</span> <span class=\"keyword\">SEMI</span> <span class=\"keyword\">JOIN</span> b <span class=\"keyword\">on</span> (a.key = b.key)</span><br></pre></td></tr></table></figure>\n\n<p>特点：</p>\n<p>1、left semi join 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p>\n<p>2、left semi join 是只传递表的 join key 给 map 阶段，因此left semi join 中最后 select 的结果只许出现左表。</p>\n<p>3、因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 会产生多条，也会导致 left semi join 的性能更高。 </p>\n<p>比如以下A表和B表进行 join 或 left semi join，然后 select 出所有字段，结果区别如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/left_smi_join.jpg\" alt=\"left_smi_join\"></p>\n<p>注意：蓝色叉的那一列实际是不存在left semi join中的，因为最后 select 的结果只许出现左表。</p>\n<hr>\n<p>参考：</p>\n<p><a href=\"https://www.cnblogs.com/wqbin/p/11023008.html\" target=\"_blank\" rel=\"noopener\">Hive 中的 LEFT SEMI JOIN 与 JOIN ON</a></p>\n<p><a href=\"https://www.cnblogs.com/liupengpengg/p/7908274.html\" target=\"_blank\" rel=\"noopener\">Hive中Join的类型和用法</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/25435517\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/25435517</a></p>\n<p><a href=\"https://blog.csdn.net/happyrocking/article/details/79885071\" target=\"_blank\" rel=\"noopener\">hive 的 left semi join 讲解</a></p>\n<p><a href=\"https://blog.csdn.net/qq_17776287/article/details/78567514\" target=\"_blank\" rel=\"noopener\">MapJoin和ReduceJoin区别及优化</a></p>\n","site":{"data":{}},"length":2900,"excerpt":"","more":"<p>一图以示之：</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/join-all.jpg\" alt=\"join-all\"></p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>定义</th>\n<th>语法</th>\n<th>等效</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>内连接</td>\n<td>只连接匹配的行</td>\n<td>inner join</td>\n<td>join</td>\n</tr>\n<tr>\n<td>左外连接</td>\n<td>包含左边表的全部行以及右边表中全部匹配的行</td>\n<td>left outer join</td>\n<td>left join</td>\n</tr>\n<tr>\n<td>右外连接</td>\n<td>包含右边表的全部行以及左边表中全部匹配的行</td>\n<td>right outer join</td>\n<td>right join</td>\n</tr>\n<tr>\n<td>全外连接</td>\n<td>包含左、右两个表的全部行，不管在另一边表中是否存在与它们匹配的行</td>\n<td>full out join</td>\n<td>full join</td>\n</tr>\n<tr>\n<td>左半连接</td>\n<td>以left semi join关键字前面的表为主表，返回主表的KEY也在副表中的记录</td>\n<td>left semi join</td>\n<td></td>\n</tr>\n<tr>\n<td>交叉连接</td>\n<td>生成笛卡尔积—它不适用任何匹配或者选取条件，而是直接讲一个数据源中的每一行与另个数据源的每一行匹配</td>\n<td>cross join</td>\n<td>join 不加on条件</td>\n</tr>\n</tbody></table>\n<h3 id=\"in-exists\"><a href=\"#in-exists\" class=\"headerlink\" title=\"in/exists\"></a>in/exists</h3><h4 id=\"exists的执行原理：\"><a href=\"#exists的执行原理：\" class=\"headerlink\" title=\"exists的执行原理：\"></a>exists的执行原理：</h4><blockquote>\n<p>对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率；</p>\n</blockquote>\n<h4 id=\"in的执行原理\"><a href=\"#in的执行原理\" class=\"headerlink\" title=\"in的执行原理\"></a>in的执行原理</h4><blockquote>\n<p>是把外表和内表做hash连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。</p>\n</blockquote>\n<h4 id=\"使用场景说明：\"><a href=\"#使用场景说明：\" class=\"headerlink\" title=\"使用场景说明：\"></a>使用场景说明：</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">\tc.CustomerId,</span><br><span class=\"line\">\tc.CompanyName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">\tCustomers c</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">\t<span class=\"keyword\">EXISTS</span></span><br><span class=\"line\">\t(</span><br><span class=\"line\">\t\t<span class=\"keyword\">SELECT</span> OrderID <span class=\"keyword\">FROM</span> Orders o <span class=\"keyword\">WHERE</span> o.CustomerID = c.CustomerID</span><br><span class=\"line\">\t)</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>分析：这里使用exists的原因是，订单表里面可能记录很大，而客户表是一个相当较小的表，这样查询的话 是一种优化方式。</p>\n</blockquote>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> * <span class=\"keyword\">FROM</span> Orders <span class=\"keyword\">WHERE</span> CustomerId <span class=\"keyword\">in</span> (id1,id2,id3);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">\t*</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">\tOrders</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">\tCustomerID <span class=\"keyword\">in</span> </span><br><span class=\"line\">\t(</span><br><span class=\"line\">\t\t<span class=\"keyword\">SELECT</span> CustomerId <span class=\"keyword\">FROM</span> Customers <span class=\"keyword\">WHERE</span> customer_type = <span class=\"number\">1</span></span><br><span class=\"line\">\t)</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>分析 ：这里我只查找客户编号是id1,id2,id3 的人的订单信息.  in就特别合适了。</p>\n</blockquote>\n<p><strong>注意：in后面子查询可以使用任何子查询（或常数），exists后面的只能是相关子查询（不然没意义）</strong></p>\n<h3 id=\"left-semi-join-与-inner-join-相同点与区别\"><a href=\"#left-semi-join-与-inner-join-相同点与区别\" class=\"headerlink\" title=\"left semi join 与 inner join 相同点与区别\"></a>left semi join 与 inner join 相同点与区别</h3><p>hive 的 join 类型有好几种，其实都是把 MR 中的几种方式都封装实现了，其中 join on、left semi join 算是里边具有代表性，且使用频率较高的 join 方式。</p>\n<p>1.联系</p>\n<p>他们都是 hive join 方式的一种，join on 属于 common join（shuffle join/reduce join），而 left semi join 则属于 map join（broadcast join）的一种变体，从名字可以看出他们的实现原理有差异。</p>\n<p>2.区别</p>\n<p>（1）Semi Join，也叫半连接，是从分布式数据库中借鉴过来的方法。它的产生动机是：对于reduce side join，跨机器的数据传输量非常大，这成了join操作的一个瓶颈，如果能够在map端过滤掉不会参加join操作的数据，则可以大大节省网络IO，提升执行效率。<br>实现方法很简单：选取一个小表，假设是File1，将其参与join的key抽取出来，保存到文件File3中，File3文件一般很小，可以放到内存中。在map阶段，使用DistributedCache将File3复制到各个TaskTracker上，然后将File2中不在File3中的key对应的记录过滤掉，剩下的reduce阶段的工作与reduce side join相同。<br>由于 hive 中没有 in/exist 这样的子句（新版将支持），所以需要将这种类型的子句转成 left semi join。left semi join 是只传递表的 join key 给 map 阶段 , 如果 key 足够小还是执行 map join, 如果不是则还是 common join。</p>\n<p>（2）left semi join 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p>\n<p>（3）对待右表中重复key的处理方式差异：因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join on 则会一直遍历。</p>\n<p>最后的结果是这会造成性能，以及 join 结果上的差异。</p>\n<p>（4）left semi join 中最后 select 的结果只许出现左表，因为右表只有 join key 参与关联计算了，而 join on 默认是整个关系模型都参与计算了。</p>\n<blockquote>\n<p>注意：大多数情况下 JOIN ON 和 left semi on 是对等的，但是在上述情况下会出现重复记录，导致结果差异，所以大家在使用的时候最好能了解这两种方式的原理，避免掉“坑”。</p>\n</blockquote>\n<p>示例：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> a.key, a.val <span class=\"keyword\">FROM</span> a <span class=\"keyword\">WHERE</span> a.key <span class=\"keyword\">in</span> (<span class=\"keyword\">SELECT</span> b.key <span class=\"keyword\">FROM</span> b);</span><br></pre></td></tr></table></figure>\n\n<p>可以被改写为：</p>\n <figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> a.key, a.val <span class=\"keyword\">FROM</span> a <span class=\"keyword\">LEFT</span> <span class=\"keyword\">SEMI</span> <span class=\"keyword\">JOIN</span> b <span class=\"keyword\">on</span> (a.key = b.key)</span><br></pre></td></tr></table></figure>\n\n<p>特点：</p>\n<p>1、left semi join 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p>\n<p>2、left semi join 是只传递表的 join key 给 map 阶段，因此left semi join 中最后 select 的结果只许出现左表。</p>\n<p>3、因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 会产生多条，也会导致 left semi join 的性能更高。 </p>\n<p>比如以下A表和B表进行 join 或 left semi join，然后 select 出所有字段，结果区别如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/left_smi_join.jpg\" alt=\"left_smi_join\"></p>\n<p>注意：蓝色叉的那一列实际是不存在left semi join中的，因为最后 select 的结果只许出现左表。</p>\n<hr>\n<p>参考：</p>\n<p><a href=\"https://www.cnblogs.com/wqbin/p/11023008.html\" target=\"_blank\" rel=\"noopener\">Hive 中的 LEFT SEMI JOIN 与 JOIN ON</a></p>\n<p><a href=\"https://www.cnblogs.com/liupengpengg/p/7908274.html\" target=\"_blank\" rel=\"noopener\">Hive中Join的类型和用法</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/25435517\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/25435517</a></p>\n<p><a href=\"https://blog.csdn.net/happyrocking/article/details/79885071\" target=\"_blank\" rel=\"noopener\">hive 的 left semi join 讲解</a></p>\n<p><a href=\"https://blog.csdn.net/qq_17776287/article/details/78567514\" target=\"_blank\" rel=\"noopener\">MapJoin和ReduceJoin区别及优化</a></p>\n"},{"title":"Hive的三种Join方式","date":"2019-07-09T04:33:48.000Z","_content":"\nHive中就是把Map，Reduce的Join拿过来，通过SQL来表示。\n参考链接：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins\n\n### Common/Shuffle/Reduce Join\n\n假设要进行join的数据分别来自File1和File2\n\ncommon join是一种最简单的join方式，其主要思想如下：\n\n在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签 （tag）,比如：tag=0表示来自文件File1，tag=2表示来自文件File2。即：map阶段的主要任务是对不同文件中的数据打标签。\n\n在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作。\n\n![](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/reduce_join.png)\n\n\n\n![shuffle](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/shuffle.jpg)\n\n### Map Join\n\n之所以存在reduce side join，是因为在map阶段不能获取所有需要的join字段，即：同一个key对应的字段可能位于不同map中。Reduce side join是非常低效的，因为shuffle阶段要进行大量的数据传输。\n\nMap side join是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多 份，让每个map task内存中存在一份（比如存放到hash table中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table中查找是否有相同的key的记录，如果有，则连接后输出即可。\n\n为了支持文件的复制，Hadoop提供了一个类DistributedCache，使用该类的方法如下：\n\n（1）用户使用静态方法DistributedCache.addCacheFile()指定要复制的文件，它的参数是文件的URI（如果是 HDFS上的文件，可以这样：hdfs://namenode:9000/home/XXX/file，其中9000是自己配置的NameNode端口 号）。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。（2）用户使用 DistributedCache.getLocalCacheFiles()方法获取文件目录，并使用标准的文件读写API读取相应的文件。\n\n![map_join](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/map_join.jpg)\n\n1） 大小表连接：\n\n如果一张表的数据很大，另外一张表很少(<1000行)，那么我们可以将数据量少的那张表放到内存里面，在map端做join。 Hive支持Map Join，用法如下\n\n```sql\nselect /*+ MAPJOIN(time_dim) */ count(1) from\nstore_sales join time_dim on (ss_sold_time_sk = t_time_sk)\n```\n\n\n\n2） 需要做不等值join操作（a.x < b.y 或者 a.x like b.y等）\n\n这种操作如果直接使用join的话语法不支持不等于操作，hive语法解析会直接抛出错误 如果把不等于写到where里会造成笛卡尔积，数据异常增大，速度会很慢。甚至会任务无法跑成功~ 根据mapjoin的计算原理，MapJoin会把小表全部读入内存中，在map阶段直接拿另外一个表的数据和内存中表数据做匹配。这种情况下即使笛卡尔积也不会对任务运行速度造成太大的效率影响。 而且hive的where条件本身就是在map阶段进行的操作，所以在where里写入不等值比对的话，也不会造成额外负担。\n\n```sql\nselect /*+ MAPJOIN(a) */\na.start_level, b.*\nfrom dim_level a\njoin (select * from test) b\nwhere b.xx>=a.start_level and b.xx<end_level;\n```\n\n 3） MAPJOIN 结合 UNIONALL\n原始sql：\n\n```sql\nselect a.*,coalesce(c.categoryid,’NA’) as app_category\nfrom (select * from t_aa_pvid_ctr_hour_js_mes1\n) a\nleft outer join\n(select * fromt_qd_cmfu_book_info_mes\n) c\non a.app_id=c.book_id;\n```\n\n速度很慢，老办法，先查下数据分布:\n\n```sql\nselect *\nfrom\n(selectapp_id,count(1) cnt\nfromt_aa_pvid_ctr_hour_js_mes1\ngroup by app_id) t\norder by cnt DESC\nlimit 50;\n```\n\n数据分布如下：\n\n```\nNA      617370129\n2       118293314\n1       40673814\nd       20151236\nb       1846306\ns       1124246\n5       675240\n8       642231\n6       611104\nt       596973\n4       579473\n3       489516\n7       475999\n9       373395\n107580  10508\n```\n\n\n\n我们可以看到除了NA是有问题的异常值，还有appid=1~9的数据也很多，而这些数据是可以关联到的，所以这里不能简单的随机函数了。而fromt_qd_cmfu_book_info_mes这张app库表，又有几百万数据，太大以致不能放入内存使用mapjoin。\n\n解决方：首先将appid=NA和1到9的数据存入一组，并使用mapjoin与维表（维表也限定appid=1~9，这样内存就放得下了）关联，而除此之外的数据存入另一组，使用普通的join，最后使用union all 放到一起。\n\n```sql\nselect a.*,coalesce(c.categoryid,’NA’) as app_category\nfrom --if app_id isnot number value or <=9,then not join\n(select * fromt_aa_pvid_ctr_hour_js_mes1\nwhere cast(app_id asint)>9\n) a\nleft outer join\n(select * fromt_qd_cmfu_book_info_mes\nwhere cast(book_id asint)>9) c\non a.app_id=c.book_id\nunion all\nselect /*+ MAPJOIN(c)*/\na.*,coalesce(c.categoryid,’NA’) as app_category\nfrom –if app_id<=9,use map join\n(select * fromt_aa_pvid_ctr_hour_js_mes1\nwhere coalesce(cast(app_id as int),-999)<=9) a\nleft outer join\n(select * fromt_qd_cmfu_book_info_mes\nwhere cast(book_id asint)<=9) c\n--if app_id is notnumber value,then not join\non a.app_id=c.book_id\n```\n\n- 设置：\n\n  当然也可以让hive自动识别，把join变成合适的Map Join如下所示 注：当设置为true的时候，hive会自动获取两张表的数据，判定哪个是小表，然后放在内存中\n\n```sql\nset hive.auto.convert.join=true;\nselect count(*) from store_sales join time_dim on (ss_sold_time_sk = t_time_sk)\n```\n\n### SMB(Sort-Merge-Buket) Join\n\n- 场景：\n\n  大表对小表应该使用MapJoin，但是如果是大表对大表，如果进行shuffle，那就要人命了啊，第一个慢不用说，第二个容易出异常，既然是两个表进行join，肯定有相同的字段吧。\n\n  tb_a - 5亿（按排序分成五份，每份1亿放在指定的数值范围内,类似于分区表） a_id 100001 ~ 110000 - bucket-01-a -1亿 110001 ~ 120000 120001 ~ 130000 130001 ~ 140000 140001 ~ 150000\n\n  tb_b - 5亿（同上，同一个桶只能和对应的桶内数据做join） b_id 100001 ~ 110000 - bucket-01-b -1亿 110001 ~ 120000 120001 ~ 130000 130001 ~ 140000 140001 ~ 150000\n\n  *注：实际生产环境中，一天的数据可能有50G（举例子可以把数据弄大点，比如说10亿分成1000个bucket）。*\n\n- 原理：\n\n  在运行SMB Join的时候会重新创建两张表，当然这是在后台默认做的，不需要用户主动去创建，如下所示：\n\n  ![SMB Join](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/smb_join.png)\n\n**设置（默认是false）：**\n\n```sql\nset hive.auto.convert.sortmerge.join=true\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n```\n\n- 总结：\n\n  其实在写程序的时候，我们就可以知道哪些是大表哪些是小表，注意调优。\n\n  \n\n  任务执行计划参见 ： Map join和Common join详解\n\n\n\n----\n\n参考：\n\n[Hive Join的实现原理](https://blog.csdn.net/u013668852/article/details/79768266)\n\n[Hive的三种Join方式](https://www.cnblogs.com/raymoc/p/5323824.html)\n\n[Map join和Common join详解](https://blog.csdn.net/weixin_39216383/article/details/79043299)\n\n[SQL join中级篇--hive中 mapreduce join方法分析](https://www.cnblogs.com/chengyeliang/p/4512207.html)","source":"_posts/Hive/join_task.md","raw":"---\ntitle: Hive的三种Join方式\ndate: 2019-07-09 12:33:48\ntags: \n    - sql\ncategories: \n    - Hive\n---\n\nHive中就是把Map，Reduce的Join拿过来，通过SQL来表示。\n参考链接：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins\n\n### Common/Shuffle/Reduce Join\n\n假设要进行join的数据分别来自File1和File2\n\ncommon join是一种最简单的join方式，其主要思想如下：\n\n在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签 （tag）,比如：tag=0表示来自文件File1，tag=2表示来自文件File2。即：map阶段的主要任务是对不同文件中的数据打标签。\n\n在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作。\n\n![](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/reduce_join.png)\n\n\n\n![shuffle](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/shuffle.jpg)\n\n### Map Join\n\n之所以存在reduce side join，是因为在map阶段不能获取所有需要的join字段，即：同一个key对应的字段可能位于不同map中。Reduce side join是非常低效的，因为shuffle阶段要进行大量的数据传输。\n\nMap side join是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多 份，让每个map task内存中存在一份（比如存放到hash table中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table中查找是否有相同的key的记录，如果有，则连接后输出即可。\n\n为了支持文件的复制，Hadoop提供了一个类DistributedCache，使用该类的方法如下：\n\n（1）用户使用静态方法DistributedCache.addCacheFile()指定要复制的文件，它的参数是文件的URI（如果是 HDFS上的文件，可以这样：hdfs://namenode:9000/home/XXX/file，其中9000是自己配置的NameNode端口 号）。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。（2）用户使用 DistributedCache.getLocalCacheFiles()方法获取文件目录，并使用标准的文件读写API读取相应的文件。\n\n![map_join](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/map_join.jpg)\n\n1） 大小表连接：\n\n如果一张表的数据很大，另外一张表很少(<1000行)，那么我们可以将数据量少的那张表放到内存里面，在map端做join。 Hive支持Map Join，用法如下\n\n```sql\nselect /*+ MAPJOIN(time_dim) */ count(1) from\nstore_sales join time_dim on (ss_sold_time_sk = t_time_sk)\n```\n\n\n\n2） 需要做不等值join操作（a.x < b.y 或者 a.x like b.y等）\n\n这种操作如果直接使用join的话语法不支持不等于操作，hive语法解析会直接抛出错误 如果把不等于写到where里会造成笛卡尔积，数据异常增大，速度会很慢。甚至会任务无法跑成功~ 根据mapjoin的计算原理，MapJoin会把小表全部读入内存中，在map阶段直接拿另外一个表的数据和内存中表数据做匹配。这种情况下即使笛卡尔积也不会对任务运行速度造成太大的效率影响。 而且hive的where条件本身就是在map阶段进行的操作，所以在where里写入不等值比对的话，也不会造成额外负担。\n\n```sql\nselect /*+ MAPJOIN(a) */\na.start_level, b.*\nfrom dim_level a\njoin (select * from test) b\nwhere b.xx>=a.start_level and b.xx<end_level;\n```\n\n 3） MAPJOIN 结合 UNIONALL\n原始sql：\n\n```sql\nselect a.*,coalesce(c.categoryid,’NA’) as app_category\nfrom (select * from t_aa_pvid_ctr_hour_js_mes1\n) a\nleft outer join\n(select * fromt_qd_cmfu_book_info_mes\n) c\non a.app_id=c.book_id;\n```\n\n速度很慢，老办法，先查下数据分布:\n\n```sql\nselect *\nfrom\n(selectapp_id,count(1) cnt\nfromt_aa_pvid_ctr_hour_js_mes1\ngroup by app_id) t\norder by cnt DESC\nlimit 50;\n```\n\n数据分布如下：\n\n```\nNA      617370129\n2       118293314\n1       40673814\nd       20151236\nb       1846306\ns       1124246\n5       675240\n8       642231\n6       611104\nt       596973\n4       579473\n3       489516\n7       475999\n9       373395\n107580  10508\n```\n\n\n\n我们可以看到除了NA是有问题的异常值，还有appid=1~9的数据也很多，而这些数据是可以关联到的，所以这里不能简单的随机函数了。而fromt_qd_cmfu_book_info_mes这张app库表，又有几百万数据，太大以致不能放入内存使用mapjoin。\n\n解决方：首先将appid=NA和1到9的数据存入一组，并使用mapjoin与维表（维表也限定appid=1~9，这样内存就放得下了）关联，而除此之外的数据存入另一组，使用普通的join，最后使用union all 放到一起。\n\n```sql\nselect a.*,coalesce(c.categoryid,’NA’) as app_category\nfrom --if app_id isnot number value or <=9,then not join\n(select * fromt_aa_pvid_ctr_hour_js_mes1\nwhere cast(app_id asint)>9\n) a\nleft outer join\n(select * fromt_qd_cmfu_book_info_mes\nwhere cast(book_id asint)>9) c\non a.app_id=c.book_id\nunion all\nselect /*+ MAPJOIN(c)*/\na.*,coalesce(c.categoryid,’NA’) as app_category\nfrom –if app_id<=9,use map join\n(select * fromt_aa_pvid_ctr_hour_js_mes1\nwhere coalesce(cast(app_id as int),-999)<=9) a\nleft outer join\n(select * fromt_qd_cmfu_book_info_mes\nwhere cast(book_id asint)<=9) c\n--if app_id is notnumber value,then not join\non a.app_id=c.book_id\n```\n\n- 设置：\n\n  当然也可以让hive自动识别，把join变成合适的Map Join如下所示 注：当设置为true的时候，hive会自动获取两张表的数据，判定哪个是小表，然后放在内存中\n\n```sql\nset hive.auto.convert.join=true;\nselect count(*) from store_sales join time_dim on (ss_sold_time_sk = t_time_sk)\n```\n\n### SMB(Sort-Merge-Buket) Join\n\n- 场景：\n\n  大表对小表应该使用MapJoin，但是如果是大表对大表，如果进行shuffle，那就要人命了啊，第一个慢不用说，第二个容易出异常，既然是两个表进行join，肯定有相同的字段吧。\n\n  tb_a - 5亿（按排序分成五份，每份1亿放在指定的数值范围内,类似于分区表） a_id 100001 ~ 110000 - bucket-01-a -1亿 110001 ~ 120000 120001 ~ 130000 130001 ~ 140000 140001 ~ 150000\n\n  tb_b - 5亿（同上，同一个桶只能和对应的桶内数据做join） b_id 100001 ~ 110000 - bucket-01-b -1亿 110001 ~ 120000 120001 ~ 130000 130001 ~ 140000 140001 ~ 150000\n\n  *注：实际生产环境中，一天的数据可能有50G（举例子可以把数据弄大点，比如说10亿分成1000个bucket）。*\n\n- 原理：\n\n  在运行SMB Join的时候会重新创建两张表，当然这是在后台默认做的，不需要用户主动去创建，如下所示：\n\n  ![SMB Join](https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/smb_join.png)\n\n**设置（默认是false）：**\n\n```sql\nset hive.auto.convert.sortmerge.join=true\nset hive.optimize.bucketmapjoin=true;\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n```\n\n- 总结：\n\n  其实在写程序的时候，我们就可以知道哪些是大表哪些是小表，注意调优。\n\n  \n\n  任务执行计划参见 ： Map join和Common join详解\n\n\n\n----\n\n参考：\n\n[Hive Join的实现原理](https://blog.csdn.net/u013668852/article/details/79768266)\n\n[Hive的三种Join方式](https://www.cnblogs.com/raymoc/p/5323824.html)\n\n[Map join和Common join详解](https://blog.csdn.net/weixin_39216383/article/details/79043299)\n\n[SQL join中级篇--hive中 mapreduce join方法分析](https://www.cnblogs.com/chengyeliang/p/4512207.html)","slug":"Hive/join_task","published":1,"updated":"2019-07-09T07:47:49.230Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy01ec5l0008m5878li7p6sf","content":"<p>Hive中就是把Map，Reduce的Join拿过来，通过SQL来表示。<br>参考链接：<a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins</a></p>\n<h3 id=\"Common-Shuffle-Reduce-Join\"><a href=\"#Common-Shuffle-Reduce-Join\" class=\"headerlink\" title=\"Common/Shuffle/Reduce Join\"></a>Common/Shuffle/Reduce Join</h3><p>假设要进行join的数据分别来自File1和File2</p>\n<p>common join是一种最简单的join方式，其主要思想如下：</p>\n<p>在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签 （tag）,比如：tag=0表示来自文件File1，tag=2表示来自文件File2。即：map阶段的主要任务是对不同文件中的数据打标签。</p>\n<p>在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作。</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/reduce_join.png\" alt></p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/shuffle.jpg\" alt=\"shuffle\"></p>\n<h3 id=\"Map-Join\"><a href=\"#Map-Join\" class=\"headerlink\" title=\"Map Join\"></a>Map Join</h3><p>之所以存在reduce side join，是因为在map阶段不能获取所有需要的join字段，即：同一个key对应的字段可能位于不同map中。Reduce side join是非常低效的，因为shuffle阶段要进行大量的数据传输。</p>\n<p>Map side join是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多 份，让每个map task内存中存在一份（比如存放到hash table中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table中查找是否有相同的key的记录，如果有，则连接后输出即可。</p>\n<p>为了支持文件的复制，Hadoop提供了一个类DistributedCache，使用该类的方法如下：</p>\n<p>（1）用户使用静态方法DistributedCache.addCacheFile()指定要复制的文件，它的参数是文件的URI（如果是 HDFS上的文件，可以这样：hdfs://namenode:9000/home/XXX/file，其中9000是自己配置的NameNode端口 号）。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。（2）用户使用 DistributedCache.getLocalCacheFiles()方法获取文件目录，并使用标准的文件读写API读取相应的文件。</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/map_join.jpg\" alt=\"map_join\"></p>\n<p>1） 大小表连接：</p>\n<p>如果一张表的数据很大，另外一张表很少(&lt;1000行)，那么我们可以将数据量少的那张表放到内存里面，在map端做join。 Hive支持Map Join，用法如下</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"comment\">/*+ MAPJOIN(time_dim) */</span> <span class=\"keyword\">count</span>(<span class=\"number\">1</span>) <span class=\"keyword\">from</span></span><br><span class=\"line\">store_sales <span class=\"keyword\">join</span> time_dim <span class=\"keyword\">on</span> (ss_sold_time_sk = t_time_sk)</span><br></pre></td></tr></table></figure>\n\n<p>2） 需要做不等值join操作（a.x &lt; b.y 或者 a.x like b.y等）</p>\n<p>这种操作如果直接使用join的话语法不支持不等于操作，hive语法解析会直接抛出错误 如果把不等于写到where里会造成笛卡尔积，数据异常增大，速度会很慢。甚至会任务无法跑成功~ 根据mapjoin的计算原理，MapJoin会把小表全部读入内存中，在map阶段直接拿另外一个表的数据和内存中表数据做匹配。这种情况下即使笛卡尔积也不会对任务运行速度造成太大的效率影响。 而且hive的where条件本身就是在map阶段进行的操作，所以在where里写入不等值比对的话，也不会造成额外负担。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"comment\">/*+ MAPJOIN(a) */</span></span><br><span class=\"line\">a.start_level, b.*</span><br><span class=\"line\"><span class=\"keyword\">from</span> dim_level a</span><br><span class=\"line\"><span class=\"keyword\">join</span> (<span class=\"keyword\">select</span> * <span class=\"keyword\">from</span> <span class=\"keyword\">test</span>) b</span><br><span class=\"line\"><span class=\"keyword\">where</span> b.xx&gt;=a.start_level <span class=\"keyword\">and</span> b.xx&lt;end_level;</span><br></pre></td></tr></table></figure>\n\n<p> 3） MAPJOIN 结合 UNIONALL<br>原始sql：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> a.*,<span class=\"keyword\">coalesce</span>(c.categoryid,’NA’) <span class=\"keyword\">as</span> app_category</span><br><span class=\"line\"><span class=\"keyword\">from</span> (<span class=\"keyword\">select</span> * <span class=\"keyword\">from</span> t_aa_pvid_ctr_hour_js_mes1</span><br><span class=\"line\">) a</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_qd_cmfu_book_info_mes</span><br><span class=\"line\">) c</span><br><span class=\"line\"><span class=\"keyword\">on</span> a.app_id=c.book_id;</span><br></pre></td></tr></table></figure>\n\n<p>速度很慢，老办法，先查下数据分布:</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> *</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">(selectapp_id,<span class=\"keyword\">count</span>(<span class=\"number\">1</span>) cnt</span><br><span class=\"line\">fromt_aa_pvid_ctr_hour_js_mes1</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> app_id) t</span><br><span class=\"line\"><span class=\"keyword\">order</span> <span class=\"keyword\">by</span> cnt <span class=\"keyword\">DESC</span></span><br><span class=\"line\"><span class=\"keyword\">limit</span> <span class=\"number\">50</span>;</span><br></pre></td></tr></table></figure>\n\n<p>数据分布如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">NA      617370129</span><br><span class=\"line\">2       118293314</span><br><span class=\"line\">1       40673814</span><br><span class=\"line\">d       20151236</span><br><span class=\"line\">b       1846306</span><br><span class=\"line\">s       1124246</span><br><span class=\"line\">5       675240</span><br><span class=\"line\">8       642231</span><br><span class=\"line\">6       611104</span><br><span class=\"line\">t       596973</span><br><span class=\"line\">4       579473</span><br><span class=\"line\">3       489516</span><br><span class=\"line\">7       475999</span><br><span class=\"line\">9       373395</span><br><span class=\"line\">107580  10508</span><br></pre></td></tr></table></figure>\n\n<p>我们可以看到除了NA是有问题的异常值，还有appid=1~9的数据也很多，而这些数据是可以关联到的，所以这里不能简单的随机函数了。而fromt_qd_cmfu_book_info_mes这张app库表，又有几百万数据，太大以致不能放入内存使用mapjoin。</p>\n<p>解决方：首先将appid=NA和1到9的数据存入一组，并使用mapjoin与维表（维表也限定appid=1~9，这样内存就放得下了）关联，而除此之外的数据存入另一组，使用普通的join，最后使用union all 放到一起。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> a.*,<span class=\"keyword\">coalesce</span>(c.categoryid,’NA’) <span class=\"keyword\">as</span> app_category</span><br><span class=\"line\"><span class=\"keyword\">from</span> <span class=\"comment\">--if app_id isnot number value or &lt;=9,then not join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_aa_pvid_ctr_hour_js_mes1</span><br><span class=\"line\"><span class=\"keyword\">where</span> <span class=\"keyword\">cast</span>(app_id asint)&gt;<span class=\"number\">9</span></span><br><span class=\"line\">) a</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_qd_cmfu_book_info_mes</span><br><span class=\"line\"><span class=\"keyword\">where</span> <span class=\"keyword\">cast</span>(book_id asint)&gt;<span class=\"number\">9</span>) c</span><br><span class=\"line\"><span class=\"keyword\">on</span> a.app_id=c.book_id</span><br><span class=\"line\"><span class=\"keyword\">union</span> <span class=\"keyword\">all</span></span><br><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"comment\">/*+ MAPJOIN(c)*/</span></span><br><span class=\"line\">a.*,<span class=\"keyword\">coalesce</span>(c.categoryid,’NA’) <span class=\"keyword\">as</span> app_category</span><br><span class=\"line\"><span class=\"keyword\">from</span> –<span class=\"keyword\">if</span> app_id&lt;=<span class=\"number\">9</span>,<span class=\"keyword\">use</span> <span class=\"keyword\">map</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_aa_pvid_ctr_hour_js_mes1</span><br><span class=\"line\"><span class=\"keyword\">where</span> <span class=\"keyword\">coalesce</span>(<span class=\"keyword\">cast</span>(app_id <span class=\"keyword\">as</span> <span class=\"built_in\">int</span>),<span class=\"number\">-999</span>)&lt;=<span class=\"number\">9</span>) a</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_qd_cmfu_book_info_mes</span><br><span class=\"line\"><span class=\"keyword\">where</span> <span class=\"keyword\">cast</span>(book_id asint)&lt;=<span class=\"number\">9</span>) c</span><br><span class=\"line\"><span class=\"comment\">--if app_id is notnumber value,then not join</span></span><br><span class=\"line\"><span class=\"keyword\">on</span> a.app_id=c.book_id</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>设置：</p>\n<p>当然也可以让hive自动识别，把join变成合适的Map Join如下所示 注：当设置为true的时候，hive会自动获取两张表的数据，判定哪个是小表，然后放在内存中</p>\n</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span> hive.auto.convert.join=<span class=\"literal\">true</span>;</span><br><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"keyword\">count</span>(*) <span class=\"keyword\">from</span> store_sales <span class=\"keyword\">join</span> time_dim <span class=\"keyword\">on</span> (ss_sold_time_sk = t_time_sk)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"SMB-Sort-Merge-Buket-Join\"><a href=\"#SMB-Sort-Merge-Buket-Join\" class=\"headerlink\" title=\"SMB(Sort-Merge-Buket) Join\"></a>SMB(Sort-Merge-Buket) Join</h3><ul>\n<li><p>场景：</p>\n<p>大表对小表应该使用MapJoin，但是如果是大表对大表，如果进行shuffle，那就要人命了啊，第一个慢不用说，第二个容易出异常，既然是两个表进行join，肯定有相同的字段吧。</p>\n<p>tb_a - 5亿（按排序分成五份，每份1亿放在指定的数值范围内,类似于分区表） a_id 100001 ~ 110000 - bucket-01-a -1亿 110001 ~ 120000 120001 ~ 130000 130001 ~ 140000 140001 ~ 150000</p>\n<p>tb_b - 5亿（同上，同一个桶只能和对应的桶内数据做join） b_id 100001 ~ 110000 - bucket-01-b -1亿 110001 ~ 120000 120001 ~ 130000 130001 ~ 140000 140001 ~ 150000</p>\n<p><em>注：实际生产环境中，一天的数据可能有50G（举例子可以把数据弄大点，比如说10亿分成1000个bucket）。</em></p>\n</li>\n<li><p>原理：</p>\n<p>在运行SMB Join的时候会重新创建两张表，当然这是在后台默认做的，不需要用户主动去创建，如下所示：</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/smb_join.png\" alt=\"SMB Join\"></p>\n</li>\n</ul>\n<p><strong>设置（默认是false）：</strong></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span> hive.auto.convert.sortmerge.join=<span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"keyword\">set</span> hive.optimize.bucketmapjoin=<span class=\"literal\">true</span>;</span><br><span class=\"line\"><span class=\"keyword\">set</span> hive.optimize.bucketmapjoin.sortedmerge=<span class=\"literal\">true</span>;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>总结：</p>\n<p>其实在写程序的时候，我们就可以知道哪些是大表哪些是小表，注意调优。</p>\n</li>\n</ul>\n<p>  任务执行计划参见 ： Map join和Common join详解</p>\n<hr>\n<p>参考：</p>\n<p><a href=\"https://blog.csdn.net/u013668852/article/details/79768266\" target=\"_blank\" rel=\"noopener\">Hive Join的实现原理</a></p>\n<p><a href=\"https://www.cnblogs.com/raymoc/p/5323824.html\" target=\"_blank\" rel=\"noopener\">Hive的三种Join方式</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_39216383/article/details/79043299\" target=\"_blank\" rel=\"noopener\">Map join和Common join详解</a></p>\n<p><a href=\"https://www.cnblogs.com/chengyeliang/p/4512207.html\" target=\"_blank\" rel=\"noopener\">SQL join中级篇–hive中 mapreduce join方法分析</a></p>\n","site":{"data":{}},"length":4221,"excerpt":"","more":"<p>Hive中就是把Map，Reduce的Join拿过来，通过SQL来表示。<br>参考链接：<a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins</a></p>\n<h3 id=\"Common-Shuffle-Reduce-Join\"><a href=\"#Common-Shuffle-Reduce-Join\" class=\"headerlink\" title=\"Common/Shuffle/Reduce Join\"></a>Common/Shuffle/Reduce Join</h3><p>假设要进行join的数据分别来自File1和File2</p>\n<p>common join是一种最简单的join方式，其主要思想如下：</p>\n<p>在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签 （tag）,比如：tag=0表示来自文件File1，tag=2表示来自文件File2。即：map阶段的主要任务是对不同文件中的数据打标签。</p>\n<p>在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作。</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/reduce_join.png\" alt></p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/shuffle.jpg\" alt=\"shuffle\"></p>\n<h3 id=\"Map-Join\"><a href=\"#Map-Join\" class=\"headerlink\" title=\"Map Join\"></a>Map Join</h3><p>之所以存在reduce side join，是因为在map阶段不能获取所有需要的join字段，即：同一个key对应的字段可能位于不同map中。Reduce side join是非常低效的，因为shuffle阶段要进行大量的数据传输。</p>\n<p>Map side join是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多 份，让每个map task内存中存在一份（比如存放到hash table中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table中查找是否有相同的key的记录，如果有，则连接后输出即可。</p>\n<p>为了支持文件的复制，Hadoop提供了一个类DistributedCache，使用该类的方法如下：</p>\n<p>（1）用户使用静态方法DistributedCache.addCacheFile()指定要复制的文件，它的参数是文件的URI（如果是 HDFS上的文件，可以这样：hdfs://namenode:9000/home/XXX/file，其中9000是自己配置的NameNode端口 号）。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。（2）用户使用 DistributedCache.getLocalCacheFiles()方法获取文件目录，并使用标准的文件读写API读取相应的文件。</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/map_join.jpg\" alt=\"map_join\"></p>\n<p>1） 大小表连接：</p>\n<p>如果一张表的数据很大，另外一张表很少(&lt;1000行)，那么我们可以将数据量少的那张表放到内存里面，在map端做join。 Hive支持Map Join，用法如下</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"comment\">/*+ MAPJOIN(time_dim) */</span> <span class=\"keyword\">count</span>(<span class=\"number\">1</span>) <span class=\"keyword\">from</span></span><br><span class=\"line\">store_sales <span class=\"keyword\">join</span> time_dim <span class=\"keyword\">on</span> (ss_sold_time_sk = t_time_sk)</span><br></pre></td></tr></table></figure>\n\n<p>2） 需要做不等值join操作（a.x &lt; b.y 或者 a.x like b.y等）</p>\n<p>这种操作如果直接使用join的话语法不支持不等于操作，hive语法解析会直接抛出错误 如果把不等于写到where里会造成笛卡尔积，数据异常增大，速度会很慢。甚至会任务无法跑成功~ 根据mapjoin的计算原理，MapJoin会把小表全部读入内存中，在map阶段直接拿另外一个表的数据和内存中表数据做匹配。这种情况下即使笛卡尔积也不会对任务运行速度造成太大的效率影响。 而且hive的where条件本身就是在map阶段进行的操作，所以在where里写入不等值比对的话，也不会造成额外负担。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"comment\">/*+ MAPJOIN(a) */</span></span><br><span class=\"line\">a.start_level, b.*</span><br><span class=\"line\"><span class=\"keyword\">from</span> dim_level a</span><br><span class=\"line\"><span class=\"keyword\">join</span> (<span class=\"keyword\">select</span> * <span class=\"keyword\">from</span> <span class=\"keyword\">test</span>) b</span><br><span class=\"line\"><span class=\"keyword\">where</span> b.xx&gt;=a.start_level <span class=\"keyword\">and</span> b.xx&lt;end_level;</span><br></pre></td></tr></table></figure>\n\n<p> 3） MAPJOIN 结合 UNIONALL<br>原始sql：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> a.*,<span class=\"keyword\">coalesce</span>(c.categoryid,’NA’) <span class=\"keyword\">as</span> app_category</span><br><span class=\"line\"><span class=\"keyword\">from</span> (<span class=\"keyword\">select</span> * <span class=\"keyword\">from</span> t_aa_pvid_ctr_hour_js_mes1</span><br><span class=\"line\">) a</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_qd_cmfu_book_info_mes</span><br><span class=\"line\">) c</span><br><span class=\"line\"><span class=\"keyword\">on</span> a.app_id=c.book_id;</span><br></pre></td></tr></table></figure>\n\n<p>速度很慢，老办法，先查下数据分布:</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> *</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">(selectapp_id,<span class=\"keyword\">count</span>(<span class=\"number\">1</span>) cnt</span><br><span class=\"line\">fromt_aa_pvid_ctr_hour_js_mes1</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> app_id) t</span><br><span class=\"line\"><span class=\"keyword\">order</span> <span class=\"keyword\">by</span> cnt <span class=\"keyword\">DESC</span></span><br><span class=\"line\"><span class=\"keyword\">limit</span> <span class=\"number\">50</span>;</span><br></pre></td></tr></table></figure>\n\n<p>数据分布如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">NA      617370129</span><br><span class=\"line\">2       118293314</span><br><span class=\"line\">1       40673814</span><br><span class=\"line\">d       20151236</span><br><span class=\"line\">b       1846306</span><br><span class=\"line\">s       1124246</span><br><span class=\"line\">5       675240</span><br><span class=\"line\">8       642231</span><br><span class=\"line\">6       611104</span><br><span class=\"line\">t       596973</span><br><span class=\"line\">4       579473</span><br><span class=\"line\">3       489516</span><br><span class=\"line\">7       475999</span><br><span class=\"line\">9       373395</span><br><span class=\"line\">107580  10508</span><br></pre></td></tr></table></figure>\n\n<p>我们可以看到除了NA是有问题的异常值，还有appid=1~9的数据也很多，而这些数据是可以关联到的，所以这里不能简单的随机函数了。而fromt_qd_cmfu_book_info_mes这张app库表，又有几百万数据，太大以致不能放入内存使用mapjoin。</p>\n<p>解决方：首先将appid=NA和1到9的数据存入一组，并使用mapjoin与维表（维表也限定appid=1~9，这样内存就放得下了）关联，而除此之外的数据存入另一组，使用普通的join，最后使用union all 放到一起。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> a.*,<span class=\"keyword\">coalesce</span>(c.categoryid,’NA’) <span class=\"keyword\">as</span> app_category</span><br><span class=\"line\"><span class=\"keyword\">from</span> <span class=\"comment\">--if app_id isnot number value or &lt;=9,then not join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_aa_pvid_ctr_hour_js_mes1</span><br><span class=\"line\"><span class=\"keyword\">where</span> <span class=\"keyword\">cast</span>(app_id asint)&gt;<span class=\"number\">9</span></span><br><span class=\"line\">) a</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_qd_cmfu_book_info_mes</span><br><span class=\"line\"><span class=\"keyword\">where</span> <span class=\"keyword\">cast</span>(book_id asint)&gt;<span class=\"number\">9</span>) c</span><br><span class=\"line\"><span class=\"keyword\">on</span> a.app_id=c.book_id</span><br><span class=\"line\"><span class=\"keyword\">union</span> <span class=\"keyword\">all</span></span><br><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"comment\">/*+ MAPJOIN(c)*/</span></span><br><span class=\"line\">a.*,<span class=\"keyword\">coalesce</span>(c.categoryid,’NA’) <span class=\"keyword\">as</span> app_category</span><br><span class=\"line\"><span class=\"keyword\">from</span> –<span class=\"keyword\">if</span> app_id&lt;=<span class=\"number\">9</span>,<span class=\"keyword\">use</span> <span class=\"keyword\">map</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_aa_pvid_ctr_hour_js_mes1</span><br><span class=\"line\"><span class=\"keyword\">where</span> <span class=\"keyword\">coalesce</span>(<span class=\"keyword\">cast</span>(app_id <span class=\"keyword\">as</span> <span class=\"built_in\">int</span>),<span class=\"number\">-999</span>)&lt;=<span class=\"number\">9</span>) a</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span> * fromt_qd_cmfu_book_info_mes</span><br><span class=\"line\"><span class=\"keyword\">where</span> <span class=\"keyword\">cast</span>(book_id asint)&lt;=<span class=\"number\">9</span>) c</span><br><span class=\"line\"><span class=\"comment\">--if app_id is notnumber value,then not join</span></span><br><span class=\"line\"><span class=\"keyword\">on</span> a.app_id=c.book_id</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>设置：</p>\n<p>当然也可以让hive自动识别，把join变成合适的Map Join如下所示 注：当设置为true的时候，hive会自动获取两张表的数据，判定哪个是小表，然后放在内存中</p>\n</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span> hive.auto.convert.join=<span class=\"literal\">true</span>;</span><br><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"keyword\">count</span>(*) <span class=\"keyword\">from</span> store_sales <span class=\"keyword\">join</span> time_dim <span class=\"keyword\">on</span> (ss_sold_time_sk = t_time_sk)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"SMB-Sort-Merge-Buket-Join\"><a href=\"#SMB-Sort-Merge-Buket-Join\" class=\"headerlink\" title=\"SMB(Sort-Merge-Buket) Join\"></a>SMB(Sort-Merge-Buket) Join</h3><ul>\n<li><p>场景：</p>\n<p>大表对小表应该使用MapJoin，但是如果是大表对大表，如果进行shuffle，那就要人命了啊，第一个慢不用说，第二个容易出异常，既然是两个表进行join，肯定有相同的字段吧。</p>\n<p>tb_a - 5亿（按排序分成五份，每份1亿放在指定的数值范围内,类似于分区表） a_id 100001 ~ 110000 - bucket-01-a -1亿 110001 ~ 120000 120001 ~ 130000 130001 ~ 140000 140001 ~ 150000</p>\n<p>tb_b - 5亿（同上，同一个桶只能和对应的桶内数据做join） b_id 100001 ~ 110000 - bucket-01-b -1亿 110001 ~ 120000 120001 ~ 130000 130001 ~ 140000 140001 ~ 150000</p>\n<p><em>注：实际生产环境中，一天的数据可能有50G（举例子可以把数据弄大点，比如说10亿分成1000个bucket）。</em></p>\n</li>\n<li><p>原理：</p>\n<p>在运行SMB Join的时候会重新创建两张表，当然这是在后台默认做的，不需要用户主动去创建，如下所示：</p>\n<p><img src=\"https://raw.githubusercontent.com/gofreehj/BigData/master/Hive/images/smb_join.png\" alt=\"SMB Join\"></p>\n</li>\n</ul>\n<p><strong>设置（默认是false）：</strong></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span> hive.auto.convert.sortmerge.join=<span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"keyword\">set</span> hive.optimize.bucketmapjoin=<span class=\"literal\">true</span>;</span><br><span class=\"line\"><span class=\"keyword\">set</span> hive.optimize.bucketmapjoin.sortedmerge=<span class=\"literal\">true</span>;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>总结：</p>\n<p>其实在写程序的时候，我们就可以知道哪些是大表哪些是小表，注意调优。</p>\n</li>\n</ul>\n<p>  任务执行计划参见 ： Map join和Common join详解</p>\n<hr>\n<p>参考：</p>\n<p><a href=\"https://blog.csdn.net/u013668852/article/details/79768266\" target=\"_blank\" rel=\"noopener\">Hive Join的实现原理</a></p>\n<p><a href=\"https://www.cnblogs.com/raymoc/p/5323824.html\" target=\"_blank\" rel=\"noopener\">Hive的三种Join方式</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_39216383/article/details/79043299\" target=\"_blank\" rel=\"noopener\">Map join和Common join详解</a></p>\n<p><a href=\"https://www.cnblogs.com/chengyeliang/p/4512207.html\" target=\"_blank\" rel=\"noopener\">SQL join中级篇–hive中 mapreduce join方法分析</a></p>\n"},{"title":"Hive高级函数：窗口函数、分析函数、增强group","date":"2019-07-07T04:33:48.000Z","_content":"\n# 窗口函数与分析函数\n\n## 应用场景:\n\n（1）用于分区排序 \n（2）动态Group By \n（3）Top N \n（4）累计计算 \n（5）层次查询\n\n## 窗口函数\n\n| 函数                | 功能                                                         |\n| ------------------- | ------------------------------------------------------------ |\n| FIRST_VALUE         | 取分组内排序后，截止到当前行，第一个值                       |\n| LAST_VALUE          | 取分组内排序后，截止到当前行，最后一个值                     |\n| LEAD(col,n,DEFAULT) | 用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL） |\n| LAG(col,n,DEFAULT)  | 与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL） |\n\n**OVER从句**\n\n1、使用标准的聚合函数`COUNT、SUM、MIN、MAX、AVG` \n2、使用`PARTITION BY`语句，使用一个或者多个原始数据类型的列 \n3、使用`PARTITION BY`与`ORDER BY`语句，使用一个或者多个数据类型的分区或者排序列 \n4、使用窗口规范，窗口规范支持以下格式：\n\n```sql\n(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)\n```\n\n```sql\n(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)\n```\n\n```sql\n(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING\n```\n\n当`ORDER BY`后面缺少窗口从句条件，窗口规范默认是 `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`.\n\n当`ORDER BY`和窗口从句都缺失, 窗口规范默认是 `ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`.\n\n`OVER`从句支持以下函数， 但是并不支持和窗口一起使用它们。 \nRanking函数: `Rank, NTile, DenseRank, CumeDist, PercentRank`. \n`Lead` 和 `Lag` 函数.\n\n## 分析函数\n\n|              |                                                              |\n| ------------ | ------------------------------------------------------------ |\n| ROW_NUMBER() | 从1开始，按照顺序，生成分组内记录的序列,比如，按照pv降序排列，生成分组内每天的pv名次,ROW_NUMBER()的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。 |\n| RANK()       | 生成数据项在分组中的排名，排名相等会在名次中留下空位         |\n| DENSE_RANK() | 生成数据项在分组中的排名，排名相等会在名次中不会留下空位     |\n| CUME_DIST    | 小于等于当前值的行数/分组内总行数。比如，统计小于等于当前薪水的人数，所占总人数的比例 |\n| PERCENT_RANK | 分组内当前行的RANK值-1/分组内总行数-1                        |\n| NTILE(n)     | 用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) |\n\n**Hive2.1.0及以后支持Distinct**\n\n在聚合函数（SUM, COUNT and AVG）中，支持distinct，但是在ORDER BY 或者 窗口限制不支持。\n\n```sql\nCOUNT(DISTINCT a) OVER (PARTITION BY c)1\n```\n\n**Hive 2.2.0中在使用ORDER BY和窗口限制时支持distinct**\n\n```sql\nCOUNT(DISTINCT a) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)1\n```\n\n**Hive2.1.0及以后支持在OVER从句中支持聚合函数**\n\n```sql\nSELECT rank() OVER (ORDER BY sum(b))\nFROM T\nGROUP BY a;123\n```\n\n## 用例\n\n### 测试数据集：\n\n| user_id | device_id | user_type | amount | sex   | sales | log_time   |\n| ------- | --------- | --------- | ------ | ----- | ----- | ---------- |\n| u_001   | d_001     | new       | 60     | man   | 9     | 2019-07-01 |\n| u_002   | d_001     | old       | 40     | women | 6     | 2019-07-03 |\n| u_003   | d_001     | new       | 80     | man   | 5     | 2019-07-04 |\n| u_004   | d_001     | new       | 50     | man   | 4     | 2019-07-05 |\n| u_005   | d_001     | new       | 30     | man   | 7     | 2019-07-07 |\n| u_006   | d_002     | old       | 70     | women | 10    | 2019-07-02 |\n| u_007   | d_002     | old       | 90     | man   | 2     | 2019-07-03 |\n| u_008   | d_002     | new       | 10     | women | 1     | 2019-07-04 |\n| u_009   | d_002     | new       | 20     | women | 3     | 2019-07-06 |\n| u_010   | d_002     | new       | 100    | women | 8     | 2019-07-17 |\n\n### 1.COUNT、SUM、MIN、MAX、AVG\n\n#### rows\n\n```sql\nselect \n    user_id,\n    user_type,\n    sales,\n    --默认为从起点到当前行\n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc) AS sales_1,\n    --从起点到当前行，结果与sales_1相同（若排序字段有重复值则回出现不同，不稳定排序）。\n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sales_2,\n    --当前行+往前3行\n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS sales_3,\n    --当前行+往前3行+往后1行\n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) AS sales_4,\n    --当前行+往后所有行  \n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS sales_5,\n    --分组内所有行\n    SUM(sales) OVER(PARTITION BY user_type) AS sales_6                          \nfrom \n    order_detail\norder by \n    user_type,\n    sales,\n    user_id\n```\n\n结果：\n\n| user_id | user_type | sales | sales_1 | sales_2 | sales_3 | sales_4 | sales_5 | sales_6 |\n| ------- | --------- | ----- | ------- | ------- | ------- | ------- | ------- | ------- |\n| u_008   | new       | 1     | 1       | 1       | 1       | 4       | 37      | 37      |\n| u_009   | new       | 3     | 4       | 4       | 4       | 8       | 36      | 37      |\n| u_004   | new       | 4     | 8       | 8       | 8       | 13      | 33      | 37      |\n| u_003   | new       | 5     | 13      | 13      | 13      | 20      | 29      | 37      |\n| u_005   | new       | 7     | 20      | 20      | 19      | 27      | 24      | 37      |\n| u_010   | new       | 8     | 28      | 28      | 24      | 33      | 17      | 37      |\n| u_001   | new       | 9     | 37      | 37      | 29      | 29      | 9       | 37      |\n| u_007   | old       | 2     | 2       | 2       | 2       | 8       | 18      | 18      |\n| u_002   | old       | 6     | 8       | 8       | 8       | 18      | 16      | 18      |\n| u_006   | old       | 10    | 18      | 18      | 18      | 18      | 10      | 18      |\n\n> 注意:\n> 结果和ORDER BY相关,默认为升序\n> 如果不指定ROWS BETWEEN,默认为从起点到当前行;\n> 如果不指定ORDER BY，则将分组内所有值累加;\n>\n> 关键是理解ROWS BETWEEN含义,也叫做WINDOW子句：\n> PRECEDING：往前\n> FOLLOWING：往后\n> CURRENT ROW：当前行\n> UNBOUNDED：无界限（起点或终点）\n> UNBOUNDED PRECEDING：表示从前面的起点 \n> UNBOUNDED FOLLOWING：表示到后面的终点\n> 其他COUNT、AVG，MIN，MAX，和SUM用法一样。\n>\n> max()函数无论有没有order by 都是计算整个分区的最大值\n>\n> 更多可参考Oracle 数据库的分析语法\n\n> 理解：执行逻辑为先partiton 内order by 然后sum/max/min\n\n```sql\n参考：https://dacoolbaby.iteye.com/blog/1960373\n在Hive里面，可以把这一部分独立抽出来做声明。如：\nselect\n\tuser_id,\n\tuser_type,\n\tsales,\n\tlog_time,\n\tsum(sales) over w1 as s,\n\tmin(sales) over w1 as mi,\n\tmax(sales) over w1 as ma,\n\tavg(sales) over w1 as ag\nfrom\n\tapp.order_detail \n\twindow w1 as(distribute by user_type sort by sales asc rows between 2 preceding and 2 following) ;\n\t\n其中的window w1 则是抽出声明的窗口部分。\n\n如果在一条Hive SQL涉及到多个窗口函数的引用呢？\nselect\n\tuser_id,\n\tuser_type,\n\tsales,\n\tlog_time,\n\tsum(sales) over w1 as s1,\n\tsum(sales) over w2 as s2\nfrom\n\tapp.order_detail \n\twindow w1 as(distribute by user_type sort by sales asc rows between 2 preceding and 2 following),\n\tw2 as(distribute by user_type sort by sales asc rows between unbounded preceding and current row) ;\n```\n\n#### range\n\n```sql\n参考：\nhttps://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive\nselect\n\tuser_id,\n\tuser_type,\n\tsales,\n\tlog_time,\n\tsum(sales) OVER( PARTITION BY user_type ORDER BY unix_timestamp(log_time, 'yyyy-MM-dd') ASC RANGE BETWEEN 86400 PRECEDING and CURRENT ROW) as count\nfrom\n\tapp.order_detail\n```\n\n结果：\n\n| user_id | user_type | sales | log_time   | count |\n| ------- | --------- | ----- | ---------- | ----- |\n| u_001   | new       | 9     | 2019-07-01 | 9     |\n| u_008   | new       | 1     | 2019-07-04 | 6     |\n| u_003   | new       | 5     | 2019-07-04 | 6     |\n| u_004   | new       | 4     | 2019-07-05 | 10    |\n| u_009   | new       | 3     | 2019-07-06 | 7     |\n| u_005   | new       | 7     | 2019-07-07 | 10    |\n| u_010   | new       | 8     | 2019-07-17 | 8     |\n| u_006   | old       | 10    | 2019-07-02 | 10    |\n| u_007   | old       | 2     | 2019-07-03 | 18    |\n| u_002   | old       | 6     | 2019-07-03 | 18    |\n\n> 理解：当前时间往上三天的累积数量 86400=3600*24 （一天）\n\n### 2.first_value与last_value\n\n```sql\nselect \n    user_id,\n    user_type,\n    ROW_NUMBER() OVER(PARTITION BY user_type ORDER BY sales) AS row_num,  \n    first_value(user_id) over (partition by user_type order by sales desc) as max_sales_user,\n    first_value(user_id) over (partition by user_type order by sales asc) as min_sales_user,\n    last_value(user_id) over (partition by user_type order by sales desc) as curr_last_min_user,\n    last_value(user_id) over (partition by user_type order by sales asc) as curr_last_max_user\nfrom \n    order_detail;\n```\n\n结果：\n\n| user_id | user_type | row_num | max_sales_user | min_sales_user | curr_last_min_user | curr_last_max_user |\n| ------- | --------- | ------- | -------------- | -------------- | ------------------ | ------------------ |\n| u_001   | new       | 7       | u_001          | u_008          | u_001              | u_001              |\n| u_010   | new       | 6       | u_001          | u_008          | u_010              | u_010              |\n| u_005   | new       | 5       | u_001          | u_008          | u_005              | u_005              |\n| u_003   | new       | 4       | u_001          | u_008          | u_003              | u_003              |\n| u_004   | new       | 3       | u_001          | u_008          | u_004              | u_004              |\n| u_009   | new       | 2       | u_001          | u_008          | u_009              | u_009              |\n| u_008   | new       | 1       | u_001          | u_008          | u_008              | u_008              |\n| u_006   | old       | 3       | u_006          | u_007          | u_006              | u_006              |\n| u_002   | old       | 2       | u_006          | u_007          | u_002              | u_002              |\n| u_007   | old       | 1       | u_006          | u_007          | u_007              | u_007              |\n\n### 3.lead与lag\n\n```sql\nselect \n    user_id,device_id,\n    lead(device_id) over (order by sales) as default_after_one_line,\n    lag(device_id) over (order by sales) as default_before_one_line,\n    lead(device_id,2) over (order by sales) as after_two_line,\n    lag(device_id,2,'abc') over (order by sales) as before_two_line\nfrom \n    order_detail;\n```\n\n### 4.RANK、ROW_NUMBER、DENSE_RANK\n\n```sql\nselect \n    user_id,user_type,sales,\n    RANK() over (partition by user_type order by sales desc) as rank,\n    ROW_NUMBER() over (partition by user_type order by sales desc) as row_number,\n    DENSE_RANK() over (partition by user_type order by sales desc) as desc_rank\nfrom\n    order_detail;  \n```\n\n| user_id | user_type | sales | log_time   | rank | desc_rank | row_number |\n| ------- | --------- | ----- | ---------- | ---- | --------- | ---------- |\n| u_010   | new       | 8     | 2019-07-17 | 1    | 1         | 1          |\n| u_005   | new       | 7     | 2019-07-07 | 2    | 2         | 2          |\n| u_009   | new       | 3     | 2019-07-06 | 3    | 3         | 3          |\n| u_004   | new       | 4     | 2019-07-05 | 4    | 4         | 4          |\n| u_008   | new       | 1     | 2019-07-04 | 5    | 5         | 5          |\n| u_003   | new       | 5     | 2019-07-04 | 5    | 5         | 6          |\n| u_001   | new       | 9     | 2019-07-01 | 7    | 6         | 7          |\n| u_007   | old       | 2     | 2019-07-03 | 1    | 1         | 1          |\n| u_002   | old       | 6     | 2019-07-03 | 1    | 1         | 2          |\n| u_006   | old       | 10    | 2019-07-02 | 3    | 2         | 3          |\n\n### 5.NTILE\n\n```sql\nselect \n    user_type,sales,\n    --分组内将数据分成2片\n    NTILE(2) OVER(PARTITION BY user_type ORDER BY sales) AS nt2,\n    --分组内将数据分成3片    \n    NTILE(3) OVER(PARTITION BY user_type ORDER BY sales) AS nt3,\n    --分组内将数据分成4片    \n    NTILE(4) OVER(PARTITION BY user_type ORDER BY sales) AS nt4,\n    --将所有数据分成4片\n    NTILE(4) OVER(ORDER BY sales) AS all_nt4\nfrom \n    order_detail\norder by \n    user_type,\n    sales\n```\n\n| user_type | sales | nt2  | nt3  | nt4  | all_nt4 |\n| --------- | ----- | ---- | ---- | ---- | ------- |\n| new       | 1     | 1    | 1    | 1    | 1       |\n| new       | 3     | 1    | 1    | 1    | 1       |\n| new       | 4     | 1    | 1    | 2    | 2       |\n| new       | 5     | 1    | 2    | 2    | 2       |\n| new       | 7     | 2    | 2    | 3    | 3       |\n| new       | 8     | 2    | 3    | 3    | 3       |\n| new       | 9     | 2    | 3    | 4    | 4       |\n| old       | 2     | 1    | 1    | 1    | 1       |\n| old       | 6     | 1    | 2    | 2    | 2       |\n| old       | 10    | 2    | 3    | 3    | 4       |\n\n题：求取sale前20%的用户ID\n\n```sql\nselect\n    user_id\nfrom\n(\n    select \n        user_id,\n        NTILE(5) OVER(ORDER BY sales desc) AS nt\n    from \n        order_detail\n)A\nwhere nt=1;\n```\n\n### 6.CUME_DIST、PERCENT_RANK \n\n```sql\nselect \nuser_id,user_type,sales,\n--没有partition,所有数据均为1组\nCUME_DIST() OVER(ORDER BY sales) AS cd1,\n--按照user_type进行分组\nCUME_DIST() OVER(PARTITION BY user_type ORDER BY sales) AS cd2 \nfrom \norder_detail;   \n```\n\n| user_id | user_type | sales | cd1  | cd2                 |\n| ------- | --------- | ----- | ---- | ------------------- |\n| u_008   | new       | 1     | 0.1  | 0.14285714285714285 |\n| u_009   | new       | 3     | 0.3  | 0.2857142857142857  |\n| u_004   | new       | 4     | 0.4  | 0.42857142857142855 |\n| u_003   | new       | 5     | 0.5  | 0.5714285714285714  |\n| u_005   | new       | 7     | 0.7  | 0.7142857142857143  |\n| u_010   | new       | 8     | 0.8  | 0.8571428571428571  |\n| u_001   | new       | 9     | 0.9  | 1.0                 |\n| u_007   | old       | 2     | 0.2  | 0.3333333333333333  |\n| u_002   | old       | 6     | 0.6  | 0.6666666666666666  |\n| u_006   | old       | 10    | 1.0  | 1.0                 |\n\n\n```sql\nselect \nuser_type,sales,\n--分组内总行数      \nSUM(1) OVER(PARTITION BY user_type) AS s, \n--RANK值  \nRANK() OVER(ORDER BY sales) AS r,    \nPERCENT_RANK() OVER(ORDER BY sales) AS pr,\n--分组内     \nPERCENT_RANK() OVER(PARTITION BY user_type ORDER BY sales) AS prg \nfrom \norder_detail;   \n```\n\n| user_type | sales | s    | r    | pr                 | prg                 |\n| --------- | ----- | ---- | ---- | ------------------ | ------------------- |\n| new       | 1     | 7    | 1    | 0.0                | 0.0                 |\n| new       | 3     | 7    | 3    | 0.2222222222222222 | 0.16666666666666666 |\n| new       | 4     | 7    | 4    | 0.3333333333333333 | 0.3333333333333333  |\n| new       | 5     | 7    | 5    | 0.4444444444444444 | 0.5                 |\n| new       | 7     | 7    | 7    | 0.6666666666666666 | 0.6666666666666666  |\n| new       | 8     | 7    | 8    | 0.7777777777777778 | 0.8333333333333334  |\n| new       | 9     | 7    | 9    | 0.8888888888888888 | 1.0                 |\n| old       | 2     | 3    | 2    | 0.1111111111111111 | 0.0                 |\n| old       | 6     | 3    | 6    | 0.5555555555555556 | 0.5                 |\n| old       | 10    | 3    | 10   | 1.0                | 1.0                 |\n\n## 增强的聚合 Cube和Grouping 和Rollup\n\n这几个分析函数通常用于OLAP中，不能累加，而且需要根据不同维度上钻和下钻的指标统计，比如，分小时、天、月的UV数。\n\n**GROUPING SETS** \n在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行UNION ALL, \n其中的GROUPING__ID，表示结果属于哪一个分组集合。\n\n```sql\nselect\n    user_type,\n    sales,\n    count(user_id) as pv,\n    GROUPING__ID \nfrom \n    order_detail\ngroup by \n    user_type,sales\nGROUPING SETS(user_type,sales) \nORDER BY \n    GROUPING__ID;\n\nselect\n    user_type,\n    sales,\n    count(user_id) as pv,\n    GROUPING__ID \nfrom \n    order_detail\ngroup by \n    user_type,sales\nGROUPING SETS(user_type,sales,(user_type,sales)) \nORDER BY \n    GROUPING__ID;\n```\n\n**CUBE** \n根据GROUP BY的维度的所有组合进行聚合。\n\n```sql\nselect\n    user_type,\n    sales,\n    count(user_id) as pv,\n    GROUPING__ID \nfrom \n    order_detail\ngroup by \n    user_type,sales\nWITH CUBE \nORDER BY \n    GROUPING__ID;\n```\n\n| user_type | log_time   | pv   | grouping__id |\n| --------- | ---------- | ---- | ------------ |\n| NULL      | NULL       | 10   | 0            |\n| NULL      | 2019-07-01 | 1    | 2            |\n| NULL      | 2019-07-02 | 1    | 2            |\n| NULL      | 2019-07-03 | 2    | 2            |\n| NULL      | 2019-07-04 | 2    | 2            |\n| NULL      | 2019-07-05 | 1    | 2            |\n| NULL      | 2019-07-06 | 1    | 2            |\n| NULL      | 2019-07-07 | 1    | 2            |\n| NULL      | 2019-07-17 | 1    | 2            |\n| new       | NULL       | 7    | 1            |\n| new       | 2019-07-01 | 1    | 3            |\n| new       | 2019-07-04 | 2    | 3            |\n| new       | 2019-07-05 | 1    | 3            |\n| new       | 2019-07-06 | 1    | 3            |\n| new       | 2019-07-07 | 1    | 3            |\n| new       | 2019-07-17 | 1    | 3            |\n| old       | NULL       | 3    | 1            |\n| old       | 2019-07-02 | 1    | 3            |\n| old       | 2019-07-03 | 2    | 3            |\n\n**ROLLUP** \n是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合。\n\n```sql\nselect\n    user_type,\n    sales,\n    count(user_id) as pv,\n    GROUPING__ID \nfrom \n    order_detail\ngroup by \n    user_type,sales\nWITH ROLLUP \nORDER BY \n    GROUPING__ID;\n```\n\n\n\n题：\n\n1. 设备总共使用天数和最早、最晚使用的用户和时间\n\n2. 设备连续使用最长的天数\n\n3. 前三天的销售额，后三天的销售额？\n\n4. 每5分钟统计前一小时的在线人数\n\n-----\n\n参考：\n\n1. **https://blog.csdn.net/scgaliguodong123_/article/details/60881166** \n2. **https://blog.csdn.net/scgaliguodong123_/article/details/60135385**\n3. **https://dacoolbaby.iteye.com/blog/1960373**\n4. **https://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive**\n5. https://blog.csdn.net/Abysscarry/article/details/81408265\n6. http://lxw1234.com/archives/2015/07/367.htm\n7. https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics","source":"_posts/Hive/over.md","raw":"---\ntitle: Hive高级函数：窗口函数、分析函数、增强group\ndate: 2019-07-07 12:33:48\ntags: \n    - sql\ncategories: \n    - Hive\n---\n\n# 窗口函数与分析函数\n\n## 应用场景:\n\n（1）用于分区排序 \n（2）动态Group By \n（3）Top N \n（4）累计计算 \n（5）层次查询\n\n## 窗口函数\n\n| 函数                | 功能                                                         |\n| ------------------- | ------------------------------------------------------------ |\n| FIRST_VALUE         | 取分组内排序后，截止到当前行，第一个值                       |\n| LAST_VALUE          | 取分组内排序后，截止到当前行，最后一个值                     |\n| LEAD(col,n,DEFAULT) | 用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL） |\n| LAG(col,n,DEFAULT)  | 与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL） |\n\n**OVER从句**\n\n1、使用标准的聚合函数`COUNT、SUM、MIN、MAX、AVG` \n2、使用`PARTITION BY`语句，使用一个或者多个原始数据类型的列 \n3、使用`PARTITION BY`与`ORDER BY`语句，使用一个或者多个数据类型的分区或者排序列 \n4、使用窗口规范，窗口规范支持以下格式：\n\n```sql\n(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)\n```\n\n```sql\n(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)\n```\n\n```sql\n(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING\n```\n\n当`ORDER BY`后面缺少窗口从句条件，窗口规范默认是 `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`.\n\n当`ORDER BY`和窗口从句都缺失, 窗口规范默认是 `ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`.\n\n`OVER`从句支持以下函数， 但是并不支持和窗口一起使用它们。 \nRanking函数: `Rank, NTile, DenseRank, CumeDist, PercentRank`. \n`Lead` 和 `Lag` 函数.\n\n## 分析函数\n\n|              |                                                              |\n| ------------ | ------------------------------------------------------------ |\n| ROW_NUMBER() | 从1开始，按照顺序，生成分组内记录的序列,比如，按照pv降序排列，生成分组内每天的pv名次,ROW_NUMBER()的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。 |\n| RANK()       | 生成数据项在分组中的排名，排名相等会在名次中留下空位         |\n| DENSE_RANK() | 生成数据项在分组中的排名，排名相等会在名次中不会留下空位     |\n| CUME_DIST    | 小于等于当前值的行数/分组内总行数。比如，统计小于等于当前薪水的人数，所占总人数的比例 |\n| PERCENT_RANK | 分组内当前行的RANK值-1/分组内总行数-1                        |\n| NTILE(n)     | 用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) |\n\n**Hive2.1.0及以后支持Distinct**\n\n在聚合函数（SUM, COUNT and AVG）中，支持distinct，但是在ORDER BY 或者 窗口限制不支持。\n\n```sql\nCOUNT(DISTINCT a) OVER (PARTITION BY c)1\n```\n\n**Hive 2.2.0中在使用ORDER BY和窗口限制时支持distinct**\n\n```sql\nCOUNT(DISTINCT a) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)1\n```\n\n**Hive2.1.0及以后支持在OVER从句中支持聚合函数**\n\n```sql\nSELECT rank() OVER (ORDER BY sum(b))\nFROM T\nGROUP BY a;123\n```\n\n## 用例\n\n### 测试数据集：\n\n| user_id | device_id | user_type | amount | sex   | sales | log_time   |\n| ------- | --------- | --------- | ------ | ----- | ----- | ---------- |\n| u_001   | d_001     | new       | 60     | man   | 9     | 2019-07-01 |\n| u_002   | d_001     | old       | 40     | women | 6     | 2019-07-03 |\n| u_003   | d_001     | new       | 80     | man   | 5     | 2019-07-04 |\n| u_004   | d_001     | new       | 50     | man   | 4     | 2019-07-05 |\n| u_005   | d_001     | new       | 30     | man   | 7     | 2019-07-07 |\n| u_006   | d_002     | old       | 70     | women | 10    | 2019-07-02 |\n| u_007   | d_002     | old       | 90     | man   | 2     | 2019-07-03 |\n| u_008   | d_002     | new       | 10     | women | 1     | 2019-07-04 |\n| u_009   | d_002     | new       | 20     | women | 3     | 2019-07-06 |\n| u_010   | d_002     | new       | 100    | women | 8     | 2019-07-17 |\n\n### 1.COUNT、SUM、MIN、MAX、AVG\n\n#### rows\n\n```sql\nselect \n    user_id,\n    user_type,\n    sales,\n    --默认为从起点到当前行\n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc) AS sales_1,\n    --从起点到当前行，结果与sales_1相同（若排序字段有重复值则回出现不同，不稳定排序）。\n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sales_2,\n    --当前行+往前3行\n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS sales_3,\n    --当前行+往前3行+往后1行\n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) AS sales_4,\n    --当前行+往后所有行  \n    sum(sales) OVER(PARTITION BY user_type ORDER BY sales asc ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS sales_5,\n    --分组内所有行\n    SUM(sales) OVER(PARTITION BY user_type) AS sales_6                          \nfrom \n    order_detail\norder by \n    user_type,\n    sales,\n    user_id\n```\n\n结果：\n\n| user_id | user_type | sales | sales_1 | sales_2 | sales_3 | sales_4 | sales_5 | sales_6 |\n| ------- | --------- | ----- | ------- | ------- | ------- | ------- | ------- | ------- |\n| u_008   | new       | 1     | 1       | 1       | 1       | 4       | 37      | 37      |\n| u_009   | new       | 3     | 4       | 4       | 4       | 8       | 36      | 37      |\n| u_004   | new       | 4     | 8       | 8       | 8       | 13      | 33      | 37      |\n| u_003   | new       | 5     | 13      | 13      | 13      | 20      | 29      | 37      |\n| u_005   | new       | 7     | 20      | 20      | 19      | 27      | 24      | 37      |\n| u_010   | new       | 8     | 28      | 28      | 24      | 33      | 17      | 37      |\n| u_001   | new       | 9     | 37      | 37      | 29      | 29      | 9       | 37      |\n| u_007   | old       | 2     | 2       | 2       | 2       | 8       | 18      | 18      |\n| u_002   | old       | 6     | 8       | 8       | 8       | 18      | 16      | 18      |\n| u_006   | old       | 10    | 18      | 18      | 18      | 18      | 10      | 18      |\n\n> 注意:\n> 结果和ORDER BY相关,默认为升序\n> 如果不指定ROWS BETWEEN,默认为从起点到当前行;\n> 如果不指定ORDER BY，则将分组内所有值累加;\n>\n> 关键是理解ROWS BETWEEN含义,也叫做WINDOW子句：\n> PRECEDING：往前\n> FOLLOWING：往后\n> CURRENT ROW：当前行\n> UNBOUNDED：无界限（起点或终点）\n> UNBOUNDED PRECEDING：表示从前面的起点 \n> UNBOUNDED FOLLOWING：表示到后面的终点\n> 其他COUNT、AVG，MIN，MAX，和SUM用法一样。\n>\n> max()函数无论有没有order by 都是计算整个分区的最大值\n>\n> 更多可参考Oracle 数据库的分析语法\n\n> 理解：执行逻辑为先partiton 内order by 然后sum/max/min\n\n```sql\n参考：https://dacoolbaby.iteye.com/blog/1960373\n在Hive里面，可以把这一部分独立抽出来做声明。如：\nselect\n\tuser_id,\n\tuser_type,\n\tsales,\n\tlog_time,\n\tsum(sales) over w1 as s,\n\tmin(sales) over w1 as mi,\n\tmax(sales) over w1 as ma,\n\tavg(sales) over w1 as ag\nfrom\n\tapp.order_detail \n\twindow w1 as(distribute by user_type sort by sales asc rows between 2 preceding and 2 following) ;\n\t\n其中的window w1 则是抽出声明的窗口部分。\n\n如果在一条Hive SQL涉及到多个窗口函数的引用呢？\nselect\n\tuser_id,\n\tuser_type,\n\tsales,\n\tlog_time,\n\tsum(sales) over w1 as s1,\n\tsum(sales) over w2 as s2\nfrom\n\tapp.order_detail \n\twindow w1 as(distribute by user_type sort by sales asc rows between 2 preceding and 2 following),\n\tw2 as(distribute by user_type sort by sales asc rows between unbounded preceding and current row) ;\n```\n\n#### range\n\n```sql\n参考：\nhttps://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive\nselect\n\tuser_id,\n\tuser_type,\n\tsales,\n\tlog_time,\n\tsum(sales) OVER( PARTITION BY user_type ORDER BY unix_timestamp(log_time, 'yyyy-MM-dd') ASC RANGE BETWEEN 86400 PRECEDING and CURRENT ROW) as count\nfrom\n\tapp.order_detail\n```\n\n结果：\n\n| user_id | user_type | sales | log_time   | count |\n| ------- | --------- | ----- | ---------- | ----- |\n| u_001   | new       | 9     | 2019-07-01 | 9     |\n| u_008   | new       | 1     | 2019-07-04 | 6     |\n| u_003   | new       | 5     | 2019-07-04 | 6     |\n| u_004   | new       | 4     | 2019-07-05 | 10    |\n| u_009   | new       | 3     | 2019-07-06 | 7     |\n| u_005   | new       | 7     | 2019-07-07 | 10    |\n| u_010   | new       | 8     | 2019-07-17 | 8     |\n| u_006   | old       | 10    | 2019-07-02 | 10    |\n| u_007   | old       | 2     | 2019-07-03 | 18    |\n| u_002   | old       | 6     | 2019-07-03 | 18    |\n\n> 理解：当前时间往上三天的累积数量 86400=3600*24 （一天）\n\n### 2.first_value与last_value\n\n```sql\nselect \n    user_id,\n    user_type,\n    ROW_NUMBER() OVER(PARTITION BY user_type ORDER BY sales) AS row_num,  \n    first_value(user_id) over (partition by user_type order by sales desc) as max_sales_user,\n    first_value(user_id) over (partition by user_type order by sales asc) as min_sales_user,\n    last_value(user_id) over (partition by user_type order by sales desc) as curr_last_min_user,\n    last_value(user_id) over (partition by user_type order by sales asc) as curr_last_max_user\nfrom \n    order_detail;\n```\n\n结果：\n\n| user_id | user_type | row_num | max_sales_user | min_sales_user | curr_last_min_user | curr_last_max_user |\n| ------- | --------- | ------- | -------------- | -------------- | ------------------ | ------------------ |\n| u_001   | new       | 7       | u_001          | u_008          | u_001              | u_001              |\n| u_010   | new       | 6       | u_001          | u_008          | u_010              | u_010              |\n| u_005   | new       | 5       | u_001          | u_008          | u_005              | u_005              |\n| u_003   | new       | 4       | u_001          | u_008          | u_003              | u_003              |\n| u_004   | new       | 3       | u_001          | u_008          | u_004              | u_004              |\n| u_009   | new       | 2       | u_001          | u_008          | u_009              | u_009              |\n| u_008   | new       | 1       | u_001          | u_008          | u_008              | u_008              |\n| u_006   | old       | 3       | u_006          | u_007          | u_006              | u_006              |\n| u_002   | old       | 2       | u_006          | u_007          | u_002              | u_002              |\n| u_007   | old       | 1       | u_006          | u_007          | u_007              | u_007              |\n\n### 3.lead与lag\n\n```sql\nselect \n    user_id,device_id,\n    lead(device_id) over (order by sales) as default_after_one_line,\n    lag(device_id) over (order by sales) as default_before_one_line,\n    lead(device_id,2) over (order by sales) as after_two_line,\n    lag(device_id,2,'abc') over (order by sales) as before_two_line\nfrom \n    order_detail;\n```\n\n### 4.RANK、ROW_NUMBER、DENSE_RANK\n\n```sql\nselect \n    user_id,user_type,sales,\n    RANK() over (partition by user_type order by sales desc) as rank,\n    ROW_NUMBER() over (partition by user_type order by sales desc) as row_number,\n    DENSE_RANK() over (partition by user_type order by sales desc) as desc_rank\nfrom\n    order_detail;  \n```\n\n| user_id | user_type | sales | log_time   | rank | desc_rank | row_number |\n| ------- | --------- | ----- | ---------- | ---- | --------- | ---------- |\n| u_010   | new       | 8     | 2019-07-17 | 1    | 1         | 1          |\n| u_005   | new       | 7     | 2019-07-07 | 2    | 2         | 2          |\n| u_009   | new       | 3     | 2019-07-06 | 3    | 3         | 3          |\n| u_004   | new       | 4     | 2019-07-05 | 4    | 4         | 4          |\n| u_008   | new       | 1     | 2019-07-04 | 5    | 5         | 5          |\n| u_003   | new       | 5     | 2019-07-04 | 5    | 5         | 6          |\n| u_001   | new       | 9     | 2019-07-01 | 7    | 6         | 7          |\n| u_007   | old       | 2     | 2019-07-03 | 1    | 1         | 1          |\n| u_002   | old       | 6     | 2019-07-03 | 1    | 1         | 2          |\n| u_006   | old       | 10    | 2019-07-02 | 3    | 2         | 3          |\n\n### 5.NTILE\n\n```sql\nselect \n    user_type,sales,\n    --分组内将数据分成2片\n    NTILE(2) OVER(PARTITION BY user_type ORDER BY sales) AS nt2,\n    --分组内将数据分成3片    \n    NTILE(3) OVER(PARTITION BY user_type ORDER BY sales) AS nt3,\n    --分组内将数据分成4片    \n    NTILE(4) OVER(PARTITION BY user_type ORDER BY sales) AS nt4,\n    --将所有数据分成4片\n    NTILE(4) OVER(ORDER BY sales) AS all_nt4\nfrom \n    order_detail\norder by \n    user_type,\n    sales\n```\n\n| user_type | sales | nt2  | nt3  | nt4  | all_nt4 |\n| --------- | ----- | ---- | ---- | ---- | ------- |\n| new       | 1     | 1    | 1    | 1    | 1       |\n| new       | 3     | 1    | 1    | 1    | 1       |\n| new       | 4     | 1    | 1    | 2    | 2       |\n| new       | 5     | 1    | 2    | 2    | 2       |\n| new       | 7     | 2    | 2    | 3    | 3       |\n| new       | 8     | 2    | 3    | 3    | 3       |\n| new       | 9     | 2    | 3    | 4    | 4       |\n| old       | 2     | 1    | 1    | 1    | 1       |\n| old       | 6     | 1    | 2    | 2    | 2       |\n| old       | 10    | 2    | 3    | 3    | 4       |\n\n题：求取sale前20%的用户ID\n\n```sql\nselect\n    user_id\nfrom\n(\n    select \n        user_id,\n        NTILE(5) OVER(ORDER BY sales desc) AS nt\n    from \n        order_detail\n)A\nwhere nt=1;\n```\n\n### 6.CUME_DIST、PERCENT_RANK \n\n```sql\nselect \nuser_id,user_type,sales,\n--没有partition,所有数据均为1组\nCUME_DIST() OVER(ORDER BY sales) AS cd1,\n--按照user_type进行分组\nCUME_DIST() OVER(PARTITION BY user_type ORDER BY sales) AS cd2 \nfrom \norder_detail;   \n```\n\n| user_id | user_type | sales | cd1  | cd2                 |\n| ------- | --------- | ----- | ---- | ------------------- |\n| u_008   | new       | 1     | 0.1  | 0.14285714285714285 |\n| u_009   | new       | 3     | 0.3  | 0.2857142857142857  |\n| u_004   | new       | 4     | 0.4  | 0.42857142857142855 |\n| u_003   | new       | 5     | 0.5  | 0.5714285714285714  |\n| u_005   | new       | 7     | 0.7  | 0.7142857142857143  |\n| u_010   | new       | 8     | 0.8  | 0.8571428571428571  |\n| u_001   | new       | 9     | 0.9  | 1.0                 |\n| u_007   | old       | 2     | 0.2  | 0.3333333333333333  |\n| u_002   | old       | 6     | 0.6  | 0.6666666666666666  |\n| u_006   | old       | 10    | 1.0  | 1.0                 |\n\n\n```sql\nselect \nuser_type,sales,\n--分组内总行数      \nSUM(1) OVER(PARTITION BY user_type) AS s, \n--RANK值  \nRANK() OVER(ORDER BY sales) AS r,    \nPERCENT_RANK() OVER(ORDER BY sales) AS pr,\n--分组内     \nPERCENT_RANK() OVER(PARTITION BY user_type ORDER BY sales) AS prg \nfrom \norder_detail;   \n```\n\n| user_type | sales | s    | r    | pr                 | prg                 |\n| --------- | ----- | ---- | ---- | ------------------ | ------------------- |\n| new       | 1     | 7    | 1    | 0.0                | 0.0                 |\n| new       | 3     | 7    | 3    | 0.2222222222222222 | 0.16666666666666666 |\n| new       | 4     | 7    | 4    | 0.3333333333333333 | 0.3333333333333333  |\n| new       | 5     | 7    | 5    | 0.4444444444444444 | 0.5                 |\n| new       | 7     | 7    | 7    | 0.6666666666666666 | 0.6666666666666666  |\n| new       | 8     | 7    | 8    | 0.7777777777777778 | 0.8333333333333334  |\n| new       | 9     | 7    | 9    | 0.8888888888888888 | 1.0                 |\n| old       | 2     | 3    | 2    | 0.1111111111111111 | 0.0                 |\n| old       | 6     | 3    | 6    | 0.5555555555555556 | 0.5                 |\n| old       | 10    | 3    | 10   | 1.0                | 1.0                 |\n\n## 增强的聚合 Cube和Grouping 和Rollup\n\n这几个分析函数通常用于OLAP中，不能累加，而且需要根据不同维度上钻和下钻的指标统计，比如，分小时、天、月的UV数。\n\n**GROUPING SETS** \n在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行UNION ALL, \n其中的GROUPING__ID，表示结果属于哪一个分组集合。\n\n```sql\nselect\n    user_type,\n    sales,\n    count(user_id) as pv,\n    GROUPING__ID \nfrom \n    order_detail\ngroup by \n    user_type,sales\nGROUPING SETS(user_type,sales) \nORDER BY \n    GROUPING__ID;\n\nselect\n    user_type,\n    sales,\n    count(user_id) as pv,\n    GROUPING__ID \nfrom \n    order_detail\ngroup by \n    user_type,sales\nGROUPING SETS(user_type,sales,(user_type,sales)) \nORDER BY \n    GROUPING__ID;\n```\n\n**CUBE** \n根据GROUP BY的维度的所有组合进行聚合。\n\n```sql\nselect\n    user_type,\n    sales,\n    count(user_id) as pv,\n    GROUPING__ID \nfrom \n    order_detail\ngroup by \n    user_type,sales\nWITH CUBE \nORDER BY \n    GROUPING__ID;\n```\n\n| user_type | log_time   | pv   | grouping__id |\n| --------- | ---------- | ---- | ------------ |\n| NULL      | NULL       | 10   | 0            |\n| NULL      | 2019-07-01 | 1    | 2            |\n| NULL      | 2019-07-02 | 1    | 2            |\n| NULL      | 2019-07-03 | 2    | 2            |\n| NULL      | 2019-07-04 | 2    | 2            |\n| NULL      | 2019-07-05 | 1    | 2            |\n| NULL      | 2019-07-06 | 1    | 2            |\n| NULL      | 2019-07-07 | 1    | 2            |\n| NULL      | 2019-07-17 | 1    | 2            |\n| new       | NULL       | 7    | 1            |\n| new       | 2019-07-01 | 1    | 3            |\n| new       | 2019-07-04 | 2    | 3            |\n| new       | 2019-07-05 | 1    | 3            |\n| new       | 2019-07-06 | 1    | 3            |\n| new       | 2019-07-07 | 1    | 3            |\n| new       | 2019-07-17 | 1    | 3            |\n| old       | NULL       | 3    | 1            |\n| old       | 2019-07-02 | 1    | 3            |\n| old       | 2019-07-03 | 2    | 3            |\n\n**ROLLUP** \n是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合。\n\n```sql\nselect\n    user_type,\n    sales,\n    count(user_id) as pv,\n    GROUPING__ID \nfrom \n    order_detail\ngroup by \n    user_type,sales\nWITH ROLLUP \nORDER BY \n    GROUPING__ID;\n```\n\n\n\n题：\n\n1. 设备总共使用天数和最早、最晚使用的用户和时间\n\n2. 设备连续使用最长的天数\n\n3. 前三天的销售额，后三天的销售额？\n\n4. 每5分钟统计前一小时的在线人数\n\n-----\n\n参考：\n\n1. **https://blog.csdn.net/scgaliguodong123_/article/details/60881166** \n2. **https://blog.csdn.net/scgaliguodong123_/article/details/60135385**\n3. **https://dacoolbaby.iteye.com/blog/1960373**\n4. **https://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive**\n5. https://blog.csdn.net/Abysscarry/article/details/81408265\n6. http://lxw1234.com/archives/2015/07/367.htm\n7. https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics","slug":"Hive/over","published":1,"updated":"2019-07-12T08:50:07.464Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjy01ec5m0009m587iihmd9p3","content":"<h1 id=\"窗口函数与分析函数\"><a href=\"#窗口函数与分析函数\" class=\"headerlink\" title=\"窗口函数与分析函数\"></a>窗口函数与分析函数</h1><h2 id=\"应用场景\"><a href=\"#应用场景\" class=\"headerlink\" title=\"应用场景:\"></a>应用场景:</h2><p>（1）用于分区排序<br>（2）动态Group By<br>（3）Top N<br>（4）累计计算<br>（5）层次查询</p>\n<h2 id=\"窗口函数\"><a href=\"#窗口函数\" class=\"headerlink\" title=\"窗口函数\"></a>窗口函数</h2><table>\n<thead>\n<tr>\n<th>函数</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>FIRST_VALUE</td>\n<td>取分组内排序后，截止到当前行，第一个值</td>\n</tr>\n<tr>\n<td>LAST_VALUE</td>\n<td>取分组内排序后，截止到当前行，最后一个值</td>\n</tr>\n<tr>\n<td>LEAD(col,n,DEFAULT)</td>\n<td>用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL）</td>\n</tr>\n<tr>\n<td>LAG(col,n,DEFAULT)</td>\n<td>与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）</td>\n</tr>\n</tbody></table>\n<p><strong>OVER从句</strong></p>\n<p>1、使用标准的聚合函数<code>COUNT、SUM、MIN、MAX、AVG</code><br>2、使用<code>PARTITION BY</code>语句，使用一个或者多个原始数据类型的列<br>3、使用<code>PARTITION BY</code>与<code>ORDER BY</code>语句，使用一个或者多个数据类型的分区或者排序列<br>4、使用窗口规范，窗口规范支持以下格式：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure>\n\n<p>当<code>ORDER BY</code>后面缺少窗口从句条件，窗口规范默认是 <code>RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code>.</p>\n<p>当<code>ORDER BY</code>和窗口从句都缺失, 窗口规范默认是 <code>ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING</code>.</p>\n<p><code>OVER</code>从句支持以下函数， 但是并不支持和窗口一起使用它们。<br>Ranking函数: <code>Rank, NTile, DenseRank, CumeDist, PercentRank</code>.<br><code>Lead</code> 和 <code>Lag</code> 函数.</p>\n<h2 id=\"分析函数\"><a href=\"#分析函数\" class=\"headerlink\" title=\"分析函数\"></a>分析函数</h2><table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ROW_NUMBER()</td>\n<td>从1开始，按照顺序，生成分组内记录的序列,比如，按照pv降序排列，生成分组内每天的pv名次,ROW_NUMBER()的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。</td>\n</tr>\n<tr>\n<td>RANK()</td>\n<td>生成数据项在分组中的排名，排名相等会在名次中留下空位</td>\n</tr>\n<tr>\n<td>DENSE_RANK()</td>\n<td>生成数据项在分组中的排名，排名相等会在名次中不会留下空位</td>\n</tr>\n<tr>\n<td>CUME_DIST</td>\n<td>小于等于当前值的行数/分组内总行数。比如，统计小于等于当前薪水的人数，所占总人数的比例</td>\n</tr>\n<tr>\n<td>PERCENT_RANK</td>\n<td>分组内当前行的RANK值-1/分组内总行数-1</td>\n</tr>\n<tr>\n<td>NTILE(n)</td>\n<td>用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)</td>\n</tr>\n</tbody></table>\n<p><strong>Hive2.1.0及以后支持Distinct</strong></p>\n<p>在聚合函数（SUM, COUNT and AVG）中，支持distinct，但是在ORDER BY 或者 窗口限制不支持。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">COUNT(DISTINCT a) OVER (PARTITION BY c)1</span><br></pre></td></tr></table></figure>\n\n<p><strong>Hive 2.2.0中在使用ORDER BY和窗口限制时支持distinct</strong></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">COUNT(DISTINCT a) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)1</span><br></pre></td></tr></table></figure>\n\n<p><strong>Hive2.1.0及以后支持在OVER从句中支持聚合函数</strong></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"keyword\">rank</span>() <span class=\"keyword\">OVER</span> (<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">sum</span>(b))</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> T</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> a;123</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"用例\"><a href=\"#用例\" class=\"headerlink\" title=\"用例\"></a>用例</h2><h3 id=\"测试数据集：\"><a href=\"#测试数据集：\" class=\"headerlink\" title=\"测试数据集：\"></a>测试数据集：</h3><table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>device_id</th>\n<th>user_type</th>\n<th>amount</th>\n<th>sex</th>\n<th>sales</th>\n<th>log_time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_001</td>\n<td>d_001</td>\n<td>new</td>\n<td>60</td>\n<td>man</td>\n<td>9</td>\n<td>2019-07-01</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>d_001</td>\n<td>old</td>\n<td>40</td>\n<td>women</td>\n<td>6</td>\n<td>2019-07-03</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>d_001</td>\n<td>new</td>\n<td>80</td>\n<td>man</td>\n<td>5</td>\n<td>2019-07-04</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>d_001</td>\n<td>new</td>\n<td>50</td>\n<td>man</td>\n<td>4</td>\n<td>2019-07-05</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>d_001</td>\n<td>new</td>\n<td>30</td>\n<td>man</td>\n<td>7</td>\n<td>2019-07-07</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>d_002</td>\n<td>old</td>\n<td>70</td>\n<td>women</td>\n<td>10</td>\n<td>2019-07-02</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>d_002</td>\n<td>old</td>\n<td>90</td>\n<td>man</td>\n<td>2</td>\n<td>2019-07-03</td>\n</tr>\n<tr>\n<td>u_008</td>\n<td>d_002</td>\n<td>new</td>\n<td>10</td>\n<td>women</td>\n<td>1</td>\n<td>2019-07-04</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>d_002</td>\n<td>new</td>\n<td>20</td>\n<td>women</td>\n<td>3</td>\n<td>2019-07-06</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>d_002</td>\n<td>new</td>\n<td>100</td>\n<td>women</td>\n<td>8</td>\n<td>2019-07-17</td>\n</tr>\n</tbody></table>\n<h3 id=\"1-COUNT、SUM、MIN、MAX、AVG\"><a href=\"#1-COUNT、SUM、MIN、MAX、AVG\" class=\"headerlink\" title=\"1.COUNT、SUM、MIN、MAX、AVG\"></a>1.COUNT、SUM、MIN、MAX、AVG</h3><h4 id=\"rows\"><a href=\"#rows\" class=\"headerlink\" title=\"rows\"></a>rows</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_id,</span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"comment\">--默认为从起点到当前行</span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span>) <span class=\"keyword\">AS</span> sales_1,</span><br><span class=\"line\">    <span class=\"comment\">--从起点到当前行，结果与sales_1相同（若排序字段有重复值则回出现不同，不稳定排序）。</span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">ROWS</span> <span class=\"keyword\">BETWEEN</span> <span class=\"keyword\">UNBOUNDED</span> <span class=\"keyword\">PRECEDING</span> <span class=\"keyword\">AND</span> <span class=\"keyword\">CURRENT</span> <span class=\"keyword\">ROW</span>) <span class=\"keyword\">AS</span> sales_2,</span><br><span class=\"line\">    <span class=\"comment\">--当前行+往前3行</span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">ROWS</span> <span class=\"keyword\">BETWEEN</span> <span class=\"number\">3</span> <span class=\"keyword\">PRECEDING</span> <span class=\"keyword\">AND</span> <span class=\"keyword\">CURRENT</span> <span class=\"keyword\">ROW</span>) <span class=\"keyword\">AS</span> sales_3,</span><br><span class=\"line\">    <span class=\"comment\">--当前行+往前3行+往后1行</span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">ROWS</span> <span class=\"keyword\">BETWEEN</span> <span class=\"number\">3</span> <span class=\"keyword\">PRECEDING</span> <span class=\"keyword\">AND</span> <span class=\"number\">1</span> <span class=\"keyword\">FOLLOWING</span>) <span class=\"keyword\">AS</span> sales_4,</span><br><span class=\"line\">    <span class=\"comment\">--当前行+往后所有行  </span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">ROWS</span> <span class=\"keyword\">BETWEEN</span> <span class=\"keyword\">CURRENT</span> <span class=\"keyword\">ROW</span> <span class=\"keyword\">AND</span> <span class=\"keyword\">UNBOUNDED</span> <span class=\"keyword\">FOLLOWING</span>) <span class=\"keyword\">AS</span> sales_5,</span><br><span class=\"line\">    <span class=\"comment\">--分组内所有行</span></span><br><span class=\"line\">    <span class=\"keyword\">SUM</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type) <span class=\"keyword\">AS</span> sales_6                          </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">order</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    user_id</span><br></pre></td></tr></table></figure>\n\n<p>结果：</p>\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>sales</th>\n<th>sales_1</th>\n<th>sales_2</th>\n<th>sales_3</th>\n<th>sales_4</th>\n<th>sales_5</th>\n<th>sales_6</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>4</td>\n<td>37</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>3</td>\n<td>4</td>\n<td>4</td>\n<td>4</td>\n<td>8</td>\n<td>36</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>4</td>\n<td>8</td>\n<td>8</td>\n<td>8</td>\n<td>13</td>\n<td>33</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>5</td>\n<td>13</td>\n<td>13</td>\n<td>13</td>\n<td>20</td>\n<td>29</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>7</td>\n<td>20</td>\n<td>20</td>\n<td>19</td>\n<td>27</td>\n<td>24</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>new</td>\n<td>8</td>\n<td>28</td>\n<td>28</td>\n<td>24</td>\n<td>33</td>\n<td>17</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_001</td>\n<td>new</td>\n<td>9</td>\n<td>37</td>\n<td>37</td>\n<td>29</td>\n<td>29</td>\n<td>9</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n<td>8</td>\n<td>18</td>\n<td>18</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>6</td>\n<td>8</td>\n<td>8</td>\n<td>8</td>\n<td>18</td>\n<td>16</td>\n<td>18</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>10</td>\n<td>18</td>\n<td>18</td>\n<td>18</td>\n<td>18</td>\n<td>10</td>\n<td>18</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>注意:<br>结果和ORDER BY相关,默认为升序<br>如果不指定ROWS BETWEEN,默认为从起点到当前行;<br>如果不指定ORDER BY，则将分组内所有值累加;</p>\n<p>关键是理解ROWS BETWEEN含义,也叫做WINDOW子句：<br>PRECEDING：往前<br>FOLLOWING：往后<br>CURRENT ROW：当前行<br>UNBOUNDED：无界限（起点或终点）<br>UNBOUNDED PRECEDING：表示从前面的起点<br>UNBOUNDED FOLLOWING：表示到后面的终点<br>其他COUNT、AVG，MIN，MAX，和SUM用法一样。</p>\n<p>max()函数无论有没有order by 都是计算整个分区的最大值</p>\n<p>更多可参考Oracle 数据库的分析语法</p>\n</blockquote>\n<blockquote>\n<p>理解：执行逻辑为先partiton 内order by 然后sum/max/min</p>\n</blockquote>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">参考：https://dacoolbaby.iteye.com/blog/1960373</span><br><span class=\"line\">在Hive里面，可以把这一部分独立抽出来做声明。如：</span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">\tuser_id,</span><br><span class=\"line\">\tuser_type,</span><br><span class=\"line\">\tsales,</span><br><span class=\"line\">\tlog_time,</span><br><span class=\"line\">\t<span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> s,</span><br><span class=\"line\">\t<span class=\"keyword\">min</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> mi,</span><br><span class=\"line\">\t<span class=\"keyword\">max</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> ma,</span><br><span class=\"line\">\t<span class=\"keyword\">avg</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> ag</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">\tapp.order_detail </span><br><span class=\"line\">\t<span class=\"keyword\">window</span> w1 <span class=\"keyword\">as</span>(<span class=\"keyword\">distribute</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">sort</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">rows</span> <span class=\"keyword\">between</span> <span class=\"number\">2</span> <span class=\"keyword\">preceding</span> <span class=\"keyword\">and</span> <span class=\"number\">2</span> <span class=\"keyword\">following</span>) ;</span><br><span class=\"line\">\t</span><br><span class=\"line\">其中的window w1 则是抽出声明的窗口部分。</span><br><span class=\"line\"></span><br><span class=\"line\">如果在一条Hive SQL涉及到多个窗口函数的引用呢？</span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">\tuser_id,</span><br><span class=\"line\">\tuser_type,</span><br><span class=\"line\">\tsales,</span><br><span class=\"line\">\tlog_time,</span><br><span class=\"line\">\t<span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> s1,</span><br><span class=\"line\">\t<span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">over</span> w2 <span class=\"keyword\">as</span> s2</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">\tapp.order_detail </span><br><span class=\"line\">\t<span class=\"keyword\">window</span> w1 <span class=\"keyword\">as</span>(<span class=\"keyword\">distribute</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">sort</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">rows</span> <span class=\"keyword\">between</span> <span class=\"number\">2</span> <span class=\"keyword\">preceding</span> <span class=\"keyword\">and</span> <span class=\"number\">2</span> <span class=\"keyword\">following</span>),</span><br><span class=\"line\">\tw2 <span class=\"keyword\">as</span>(<span class=\"keyword\">distribute</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">sort</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">rows</span> <span class=\"keyword\">between</span> <span class=\"keyword\">unbounded</span> <span class=\"keyword\">preceding</span> <span class=\"keyword\">and</span> <span class=\"keyword\">current</span> <span class=\"keyword\">row</span>) ;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"range\"><a href=\"#range\" class=\"headerlink\" title=\"range\"></a>range</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">参考：</span><br><span class=\"line\">https://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive</span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">\tuser_id,</span><br><span class=\"line\">\tuser_type,</span><br><span class=\"line\">\tsales,</span><br><span class=\"line\">\tlog_time,</span><br><span class=\"line\">\t<span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>( <span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">unix_timestamp</span>(log_time, <span class=\"string\">'yyyy-MM-dd'</span>) <span class=\"keyword\">ASC</span> <span class=\"keyword\">RANGE</span> <span class=\"keyword\">BETWEEN</span> <span class=\"number\">86400</span> <span class=\"keyword\">PRECEDING</span> <span class=\"keyword\">and</span> <span class=\"keyword\">CURRENT</span> <span class=\"keyword\">ROW</span>) <span class=\"keyword\">as</span> <span class=\"keyword\">count</span></span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">\tapp.order_detail</span><br></pre></td></tr></table></figure>\n\n<p>结果：</p>\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>sales</th>\n<th>log_time</th>\n<th>count</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_001</td>\n<td>new</td>\n<td>9</td>\n<td>2019-07-01</td>\n<td>9</td>\n</tr>\n<tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>2019-07-04</td>\n<td>6</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>5</td>\n<td>2019-07-04</td>\n<td>6</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>4</td>\n<td>2019-07-05</td>\n<td>10</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>3</td>\n<td>2019-07-06</td>\n<td>7</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>7</td>\n<td>2019-07-07</td>\n<td>10</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>new</td>\n<td>8</td>\n<td>2019-07-17</td>\n<td>8</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>10</td>\n<td>2019-07-02</td>\n<td>10</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>2</td>\n<td>2019-07-03</td>\n<td>18</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>6</td>\n<td>2019-07-03</td>\n<td>18</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>理解：当前时间往上三天的累积数量 86400=3600*24 （一天）</p>\n</blockquote>\n<h3 id=\"2-first-value与last-value\"><a href=\"#2-first-value与last-value\" class=\"headerlink\" title=\"2.first_value与last_value\"></a>2.first_value与last_value</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_id,</span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    ROW_NUMBER() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> row_num,  </span><br><span class=\"line\">    <span class=\"keyword\">first_value</span>(user_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> max_sales_user,</span><br><span class=\"line\">    <span class=\"keyword\">first_value</span>(user_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span>) <span class=\"keyword\">as</span> min_sales_user,</span><br><span class=\"line\">    <span class=\"keyword\">last_value</span>(user_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> curr_last_min_user,</span><br><span class=\"line\">    <span class=\"keyword\">last_value</span>(user_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span>) <span class=\"keyword\">as</span> curr_last_max_user</span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail;</span><br></pre></td></tr></table></figure>\n\n<p>结果：</p>\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>row_num</th>\n<th>max_sales_user</th>\n<th>min_sales_user</th>\n<th>curr_last_min_user</th>\n<th>curr_last_max_user</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_001</td>\n<td>new</td>\n<td>7</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_001</td>\n<td>u_001</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>new</td>\n<td>6</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_010</td>\n<td>u_010</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>5</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_005</td>\n<td>u_005</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>4</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_003</td>\n<td>u_003</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>3</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_004</td>\n<td>u_004</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>2</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_009</td>\n<td>u_009</td>\n</tr>\n<tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_008</td>\n<td>u_008</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>3</td>\n<td>u_006</td>\n<td>u_007</td>\n<td>u_006</td>\n<td>u_006</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>2</td>\n<td>u_006</td>\n<td>u_007</td>\n<td>u_002</td>\n<td>u_002</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>1</td>\n<td>u_006</td>\n<td>u_007</td>\n<td>u_007</td>\n<td>u_007</td>\n</tr>\n</tbody></table>\n<h3 id=\"3-lead与lag\"><a href=\"#3-lead与lag\" class=\"headerlink\" title=\"3.lead与lag\"></a>3.lead与lag</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_id,device_id,</span><br><span class=\"line\">    <span class=\"keyword\">lead</span>(device_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales) <span class=\"keyword\">as</span> default_after_one_line,</span><br><span class=\"line\">    lag(device_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales) <span class=\"keyword\">as</span> default_before_one_line,</span><br><span class=\"line\">    <span class=\"keyword\">lead</span>(device_id,<span class=\"number\">2</span>) <span class=\"keyword\">over</span> (<span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales) <span class=\"keyword\">as</span> after_two_line,</span><br><span class=\"line\">    lag(device_id,<span class=\"number\">2</span>,<span class=\"string\">'abc'</span>) <span class=\"keyword\">over</span> (<span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales) <span class=\"keyword\">as</span> before_two_line</span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"4-RANK、ROW-NUMBER、DENSE-RANK\"><a href=\"#4-RANK、ROW-NUMBER、DENSE-RANK\" class=\"headerlink\" title=\"4.RANK、ROW_NUMBER、DENSE_RANK\"></a>4.RANK、ROW_NUMBER、DENSE_RANK</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_id,user_type,sales,</span><br><span class=\"line\">    <span class=\"keyword\">RANK</span>() <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> <span class=\"keyword\">rank</span>,</span><br><span class=\"line\">    ROW_NUMBER() <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> row_number,</span><br><span class=\"line\">    <span class=\"keyword\">DENSE_RANK</span>() <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> desc_rank</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">    order_detail;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>sales</th>\n<th>log_time</th>\n<th>rank</th>\n<th>desc_rank</th>\n<th>row_number</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_010</td>\n<td>new</td>\n<td>8</td>\n<td>2019-07-17</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>7</td>\n<td>2019-07-07</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>3</td>\n<td>2019-07-06</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>4</td>\n<td>2019-07-05</td>\n<td>4</td>\n<td>4</td>\n<td>4</td>\n</tr>\n<tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>2019-07-04</td>\n<td>5</td>\n<td>5</td>\n<td>5</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>5</td>\n<td>2019-07-04</td>\n<td>5</td>\n<td>5</td>\n<td>6</td>\n</tr>\n<tr>\n<td>u_001</td>\n<td>new</td>\n<td>9</td>\n<td>2019-07-01</td>\n<td>7</td>\n<td>6</td>\n<td>7</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>2</td>\n<td>2019-07-03</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>6</td>\n<td>2019-07-03</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>10</td>\n<td>2019-07-02</td>\n<td>3</td>\n<td>2</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<h3 id=\"5-NTILE\"><a href=\"#5-NTILE\" class=\"headerlink\" title=\"5.NTILE\"></a>5.NTILE</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_type,sales,</span><br><span class=\"line\">    <span class=\"comment\">--分组内将数据分成2片</span></span><br><span class=\"line\">    NTILE(<span class=\"number\">2</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> nt2,</span><br><span class=\"line\">    <span class=\"comment\">--分组内将数据分成3片    </span></span><br><span class=\"line\">    NTILE(<span class=\"number\">3</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> nt3,</span><br><span class=\"line\">    <span class=\"comment\">--分组内将数据分成4片    </span></span><br><span class=\"line\">    NTILE(<span class=\"number\">4</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> nt4,</span><br><span class=\"line\">    <span class=\"comment\">--将所有数据分成4片</span></span><br><span class=\"line\">    NTILE(<span class=\"number\">4</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> all_nt4</span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">order</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_type</th>\n<th>sales</th>\n<th>nt2</th>\n<th>nt3</th>\n<th>nt4</th>\n<th>all_nt4</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>new</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>new</td>\n<td>3</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>new</td>\n<td>4</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>new</td>\n<td>5</td>\n<td>1</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>new</td>\n<td>7</td>\n<td>2</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>8</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>9</td>\n<td>2</td>\n<td>3</td>\n<td>4</td>\n<td>4</td>\n</tr>\n<tr>\n<td>old</td>\n<td>2</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>old</td>\n<td>6</td>\n<td>1</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>old</td>\n<td>10</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n<td>4</td>\n</tr>\n</tbody></table>\n<p>题：求取sale前20%的用户ID</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_id</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">(</span><br><span class=\"line\">    <span class=\"keyword\">select</span> </span><br><span class=\"line\">        user_id,</span><br><span class=\"line\">        NTILE(<span class=\"number\">5</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">AS</span> nt</span><br><span class=\"line\">    <span class=\"keyword\">from</span> </span><br><span class=\"line\">        order_detail</span><br><span class=\"line\">)A</span><br><span class=\"line\"><span class=\"keyword\">where</span> nt=<span class=\"number\">1</span>;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"6-CUME-DIST、PERCENT-RANK\"><a href=\"#6-CUME-DIST、PERCENT-RANK\" class=\"headerlink\" title=\"6.CUME_DIST、PERCENT_RANK\"></a>6.CUME_DIST、PERCENT_RANK</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">user_id,user_type,sales,</span><br><span class=\"line\"><span class=\"comment\">--没有partition,所有数据均为1组</span></span><br><span class=\"line\"><span class=\"keyword\">CUME_DIST</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> cd1,</span><br><span class=\"line\"><span class=\"comment\">--按照user_type进行分组</span></span><br><span class=\"line\"><span class=\"keyword\">CUME_DIST</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> cd2 </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">order_detail;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>sales</th>\n<th>cd1</th>\n<th>cd2</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>0.1</td>\n<td>0.14285714285714285</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>3</td>\n<td>0.3</td>\n<td>0.2857142857142857</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>4</td>\n<td>0.4</td>\n<td>0.42857142857142855</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>5</td>\n<td>0.5</td>\n<td>0.5714285714285714</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>7</td>\n<td>0.7</td>\n<td>0.7142857142857143</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>new</td>\n<td>8</td>\n<td>0.8</td>\n<td>0.8571428571428571</td>\n</tr>\n<tr>\n<td>u_001</td>\n<td>new</td>\n<td>9</td>\n<td>0.9</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>2</td>\n<td>0.2</td>\n<td>0.3333333333333333</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>6</td>\n<td>0.6</td>\n<td>0.6666666666666666</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>10</td>\n<td>1.0</td>\n<td>1.0</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">user_type,sales,</span><br><span class=\"line\"><span class=\"comment\">--分组内总行数      </span></span><br><span class=\"line\"><span class=\"keyword\">SUM</span>(<span class=\"number\">1</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type) <span class=\"keyword\">AS</span> s, </span><br><span class=\"line\"><span class=\"comment\">--RANK值  </span></span><br><span class=\"line\"><span class=\"keyword\">RANK</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> r,    </span><br><span class=\"line\"><span class=\"keyword\">PERCENT_RANK</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> pr,</span><br><span class=\"line\"><span class=\"comment\">--分组内     </span></span><br><span class=\"line\"><span class=\"keyword\">PERCENT_RANK</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> prg </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">order_detail;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_type</th>\n<th>sales</th>\n<th>s</th>\n<th>r</th>\n<th>pr</th>\n<th>prg</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>new</td>\n<td>1</td>\n<td>7</td>\n<td>1</td>\n<td>0.0</td>\n<td>0.0</td>\n</tr>\n<tr>\n<td>new</td>\n<td>3</td>\n<td>7</td>\n<td>3</td>\n<td>0.2222222222222222</td>\n<td>0.16666666666666666</td>\n</tr>\n<tr>\n<td>new</td>\n<td>4</td>\n<td>7</td>\n<td>4</td>\n<td>0.3333333333333333</td>\n<td>0.3333333333333333</td>\n</tr>\n<tr>\n<td>new</td>\n<td>5</td>\n<td>7</td>\n<td>5</td>\n<td>0.4444444444444444</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>new</td>\n<td>7</td>\n<td>7</td>\n<td>7</td>\n<td>0.6666666666666666</td>\n<td>0.6666666666666666</td>\n</tr>\n<tr>\n<td>new</td>\n<td>8</td>\n<td>7</td>\n<td>8</td>\n<td>0.7777777777777778</td>\n<td>0.8333333333333334</td>\n</tr>\n<tr>\n<td>new</td>\n<td>9</td>\n<td>7</td>\n<td>9</td>\n<td>0.8888888888888888</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>old</td>\n<td>2</td>\n<td>3</td>\n<td>2</td>\n<td>0.1111111111111111</td>\n<td>0.0</td>\n</tr>\n<tr>\n<td>old</td>\n<td>6</td>\n<td>3</td>\n<td>6</td>\n<td>0.5555555555555556</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>old</td>\n<td>10</td>\n<td>3</td>\n<td>10</td>\n<td>1.0</td>\n<td>1.0</td>\n</tr>\n</tbody></table>\n<h2 id=\"增强的聚合-Cube和Grouping-和Rollup\"><a href=\"#增强的聚合-Cube和Grouping-和Rollup\" class=\"headerlink\" title=\"增强的聚合 Cube和Grouping 和Rollup\"></a>增强的聚合 Cube和Grouping 和Rollup</h2><p>这几个分析函数通常用于OLAP中，不能累加，而且需要根据不同维度上钻和下钻的指标统计，比如，分小时、天、月的UV数。</p>\n<p><strong>GROUPING SETS</strong><br>在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行UNION ALL,<br>其中的GROUPING__ID，表示结果属于哪一个分组集合。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"keyword\">count</span>(user_id) <span class=\"keyword\">as</span> pv,</span><br><span class=\"line\">    GROUPING__ID </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,sales</span><br><span class=\"line\"><span class=\"keyword\">GROUPING</span> <span class=\"keyword\">SETS</span>(user_type,sales) </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> </span><br><span class=\"line\">    GROUPING__ID;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"keyword\">count</span>(user_id) <span class=\"keyword\">as</span> pv,</span><br><span class=\"line\">    GROUPING__ID </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,sales</span><br><span class=\"line\"><span class=\"keyword\">GROUPING</span> <span class=\"keyword\">SETS</span>(user_type,sales,(user_type,sales)) </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> </span><br><span class=\"line\">    GROUPING__ID;</span><br></pre></td></tr></table></figure>\n\n<p><strong>CUBE</strong><br>根据GROUP BY的维度的所有组合进行聚合。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"keyword\">count</span>(user_id) <span class=\"keyword\">as</span> pv,</span><br><span class=\"line\">    GROUPING__ID </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,sales</span><br><span class=\"line\"><span class=\"keyword\">WITH</span> <span class=\"keyword\">CUBE</span> </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> </span><br><span class=\"line\">    GROUPING__ID;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_type</th>\n<th>log_time</th>\n<th>pv</th>\n<th>grouping__id</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NULL</td>\n<td>NULL</td>\n<td>10</td>\n<td>0</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-01</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-02</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-03</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-04</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-05</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-06</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-07</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-17</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>new</td>\n<td>NULL</td>\n<td>7</td>\n<td>1</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-01</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-04</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-05</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-06</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-07</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-17</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>old</td>\n<td>NULL</td>\n<td>3</td>\n<td>1</td>\n</tr>\n<tr>\n<td>old</td>\n<td>2019-07-02</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>old</td>\n<td>2019-07-03</td>\n<td>2</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<p><strong>ROLLUP</strong><br>是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"keyword\">count</span>(user_id) <span class=\"keyword\">as</span> pv,</span><br><span class=\"line\">    GROUPING__ID </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,sales</span><br><span class=\"line\"><span class=\"keyword\">WITH</span> <span class=\"keyword\">ROLLUP</span> </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> </span><br><span class=\"line\">    GROUPING__ID;</span><br></pre></td></tr></table></figure>\n\n<p>题：</p>\n<ol>\n<li><p>设备总共使用天数和最早、最晚使用的用户和时间</p>\n</li>\n<li><p>设备连续使用最长的天数</p>\n</li>\n<li><p>前三天的销售额，后三天的销售额？</p>\n</li>\n<li><p>每5分钟统计前一小时的在线人数</p>\n</li>\n</ol>\n<hr>\n<p>参考：</p>\n<ol>\n<li><strong><a href=\"https://blog.csdn.net/scgaliguodong123_/article/details/60881166\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/scgaliguodong123_/article/details/60881166</a></strong> </li>\n<li><strong><a href=\"https://blog.csdn.net/scgaliguodong123_/article/details/60135385\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/scgaliguodong123_/article/details/60135385</a></strong></li>\n<li><strong><a href=\"https://dacoolbaby.iteye.com/blog/1960373\" target=\"_blank\" rel=\"noopener\">https://dacoolbaby.iteye.com/blog/1960373</a></strong></li>\n<li><strong><a href=\"https://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive</a></strong></li>\n<li><a href=\"https://blog.csdn.net/Abysscarry/article/details/81408265\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/Abysscarry/article/details/81408265</a></li>\n<li><a href=\"http://lxw1234.com/archives/2015/07/367.htm\" target=\"_blank\" rel=\"noopener\">http://lxw1234.com/archives/2015/07/367.htm</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a></li>\n</ol>\n","site":{"data":{}},"length":11342,"excerpt":"","more":"<h1 id=\"窗口函数与分析函数\"><a href=\"#窗口函数与分析函数\" class=\"headerlink\" title=\"窗口函数与分析函数\"></a>窗口函数与分析函数</h1><h2 id=\"应用场景\"><a href=\"#应用场景\" class=\"headerlink\" title=\"应用场景:\"></a>应用场景:</h2><p>（1）用于分区排序<br>（2）动态Group By<br>（3）Top N<br>（4）累计计算<br>（5）层次查询</p>\n<h2 id=\"窗口函数\"><a href=\"#窗口函数\" class=\"headerlink\" title=\"窗口函数\"></a>窗口函数</h2><table>\n<thead>\n<tr>\n<th>函数</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>FIRST_VALUE</td>\n<td>取分组内排序后，截止到当前行，第一个值</td>\n</tr>\n<tr>\n<td>LAST_VALUE</td>\n<td>取分组内排序后，截止到当前行，最后一个值</td>\n</tr>\n<tr>\n<td>LEAD(col,n,DEFAULT)</td>\n<td>用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL）</td>\n</tr>\n<tr>\n<td>LAG(col,n,DEFAULT)</td>\n<td>与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）</td>\n</tr>\n</tbody></table>\n<p><strong>OVER从句</strong></p>\n<p>1、使用标准的聚合函数<code>COUNT、SUM、MIN、MAX、AVG</code><br>2、使用<code>PARTITION BY</code>语句，使用一个或者多个原始数据类型的列<br>3、使用<code>PARTITION BY</code>与<code>ORDER BY</code>语句，使用一个或者多个数据类型的分区或者排序列<br>4、使用窗口规范，窗口规范支持以下格式：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure>\n\n<p>当<code>ORDER BY</code>后面缺少窗口从句条件，窗口规范默认是 <code>RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code>.</p>\n<p>当<code>ORDER BY</code>和窗口从句都缺失, 窗口规范默认是 <code>ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING</code>.</p>\n<p><code>OVER</code>从句支持以下函数， 但是并不支持和窗口一起使用它们。<br>Ranking函数: <code>Rank, NTile, DenseRank, CumeDist, PercentRank</code>.<br><code>Lead</code> 和 <code>Lag</code> 函数.</p>\n<h2 id=\"分析函数\"><a href=\"#分析函数\" class=\"headerlink\" title=\"分析函数\"></a>分析函数</h2><table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ROW_NUMBER()</td>\n<td>从1开始，按照顺序，生成分组内记录的序列,比如，按照pv降序排列，生成分组内每天的pv名次,ROW_NUMBER()的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。</td>\n</tr>\n<tr>\n<td>RANK()</td>\n<td>生成数据项在分组中的排名，排名相等会在名次中留下空位</td>\n</tr>\n<tr>\n<td>DENSE_RANK()</td>\n<td>生成数据项在分组中的排名，排名相等会在名次中不会留下空位</td>\n</tr>\n<tr>\n<td>CUME_DIST</td>\n<td>小于等于当前值的行数/分组内总行数。比如，统计小于等于当前薪水的人数，所占总人数的比例</td>\n</tr>\n<tr>\n<td>PERCENT_RANK</td>\n<td>分组内当前行的RANK值-1/分组内总行数-1</td>\n</tr>\n<tr>\n<td>NTILE(n)</td>\n<td>用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)</td>\n</tr>\n</tbody></table>\n<p><strong>Hive2.1.0及以后支持Distinct</strong></p>\n<p>在聚合函数（SUM, COUNT and AVG）中，支持distinct，但是在ORDER BY 或者 窗口限制不支持。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">COUNT(DISTINCT a) OVER (PARTITION BY c)1</span><br></pre></td></tr></table></figure>\n\n<p><strong>Hive 2.2.0中在使用ORDER BY和窗口限制时支持distinct</strong></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">COUNT(DISTINCT a) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)1</span><br></pre></td></tr></table></figure>\n\n<p><strong>Hive2.1.0及以后支持在OVER从句中支持聚合函数</strong></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"keyword\">rank</span>() <span class=\"keyword\">OVER</span> (<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">sum</span>(b))</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> T</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> a;123</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"用例\"><a href=\"#用例\" class=\"headerlink\" title=\"用例\"></a>用例</h2><h3 id=\"测试数据集：\"><a href=\"#测试数据集：\" class=\"headerlink\" title=\"测试数据集：\"></a>测试数据集：</h3><table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>device_id</th>\n<th>user_type</th>\n<th>amount</th>\n<th>sex</th>\n<th>sales</th>\n<th>log_time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_001</td>\n<td>d_001</td>\n<td>new</td>\n<td>60</td>\n<td>man</td>\n<td>9</td>\n<td>2019-07-01</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>d_001</td>\n<td>old</td>\n<td>40</td>\n<td>women</td>\n<td>6</td>\n<td>2019-07-03</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>d_001</td>\n<td>new</td>\n<td>80</td>\n<td>man</td>\n<td>5</td>\n<td>2019-07-04</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>d_001</td>\n<td>new</td>\n<td>50</td>\n<td>man</td>\n<td>4</td>\n<td>2019-07-05</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>d_001</td>\n<td>new</td>\n<td>30</td>\n<td>man</td>\n<td>7</td>\n<td>2019-07-07</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>d_002</td>\n<td>old</td>\n<td>70</td>\n<td>women</td>\n<td>10</td>\n<td>2019-07-02</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>d_002</td>\n<td>old</td>\n<td>90</td>\n<td>man</td>\n<td>2</td>\n<td>2019-07-03</td>\n</tr>\n<tr>\n<td>u_008</td>\n<td>d_002</td>\n<td>new</td>\n<td>10</td>\n<td>women</td>\n<td>1</td>\n<td>2019-07-04</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>d_002</td>\n<td>new</td>\n<td>20</td>\n<td>women</td>\n<td>3</td>\n<td>2019-07-06</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>d_002</td>\n<td>new</td>\n<td>100</td>\n<td>women</td>\n<td>8</td>\n<td>2019-07-17</td>\n</tr>\n</tbody></table>\n<h3 id=\"1-COUNT、SUM、MIN、MAX、AVG\"><a href=\"#1-COUNT、SUM、MIN、MAX、AVG\" class=\"headerlink\" title=\"1.COUNT、SUM、MIN、MAX、AVG\"></a>1.COUNT、SUM、MIN、MAX、AVG</h3><h4 id=\"rows\"><a href=\"#rows\" class=\"headerlink\" title=\"rows\"></a>rows</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_id,</span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"comment\">--默认为从起点到当前行</span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span>) <span class=\"keyword\">AS</span> sales_1,</span><br><span class=\"line\">    <span class=\"comment\">--从起点到当前行，结果与sales_1相同（若排序字段有重复值则回出现不同，不稳定排序）。</span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">ROWS</span> <span class=\"keyword\">BETWEEN</span> <span class=\"keyword\">UNBOUNDED</span> <span class=\"keyword\">PRECEDING</span> <span class=\"keyword\">AND</span> <span class=\"keyword\">CURRENT</span> <span class=\"keyword\">ROW</span>) <span class=\"keyword\">AS</span> sales_2,</span><br><span class=\"line\">    <span class=\"comment\">--当前行+往前3行</span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">ROWS</span> <span class=\"keyword\">BETWEEN</span> <span class=\"number\">3</span> <span class=\"keyword\">PRECEDING</span> <span class=\"keyword\">AND</span> <span class=\"keyword\">CURRENT</span> <span class=\"keyword\">ROW</span>) <span class=\"keyword\">AS</span> sales_3,</span><br><span class=\"line\">    <span class=\"comment\">--当前行+往前3行+往后1行</span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">ROWS</span> <span class=\"keyword\">BETWEEN</span> <span class=\"number\">3</span> <span class=\"keyword\">PRECEDING</span> <span class=\"keyword\">AND</span> <span class=\"number\">1</span> <span class=\"keyword\">FOLLOWING</span>) <span class=\"keyword\">AS</span> sales_4,</span><br><span class=\"line\">    <span class=\"comment\">--当前行+往后所有行  </span></span><br><span class=\"line\">    <span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">ROWS</span> <span class=\"keyword\">BETWEEN</span> <span class=\"keyword\">CURRENT</span> <span class=\"keyword\">ROW</span> <span class=\"keyword\">AND</span> <span class=\"keyword\">UNBOUNDED</span> <span class=\"keyword\">FOLLOWING</span>) <span class=\"keyword\">AS</span> sales_5,</span><br><span class=\"line\">    <span class=\"comment\">--分组内所有行</span></span><br><span class=\"line\">    <span class=\"keyword\">SUM</span>(sales) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type) <span class=\"keyword\">AS</span> sales_6                          </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">order</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    user_id</span><br></pre></td></tr></table></figure>\n\n<p>结果：</p>\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>sales</th>\n<th>sales_1</th>\n<th>sales_2</th>\n<th>sales_3</th>\n<th>sales_4</th>\n<th>sales_5</th>\n<th>sales_6</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>4</td>\n<td>37</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>3</td>\n<td>4</td>\n<td>4</td>\n<td>4</td>\n<td>8</td>\n<td>36</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>4</td>\n<td>8</td>\n<td>8</td>\n<td>8</td>\n<td>13</td>\n<td>33</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>5</td>\n<td>13</td>\n<td>13</td>\n<td>13</td>\n<td>20</td>\n<td>29</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>7</td>\n<td>20</td>\n<td>20</td>\n<td>19</td>\n<td>27</td>\n<td>24</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>new</td>\n<td>8</td>\n<td>28</td>\n<td>28</td>\n<td>24</td>\n<td>33</td>\n<td>17</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_001</td>\n<td>new</td>\n<td>9</td>\n<td>37</td>\n<td>37</td>\n<td>29</td>\n<td>29</td>\n<td>9</td>\n<td>37</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n<td>8</td>\n<td>18</td>\n<td>18</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>6</td>\n<td>8</td>\n<td>8</td>\n<td>8</td>\n<td>18</td>\n<td>16</td>\n<td>18</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>10</td>\n<td>18</td>\n<td>18</td>\n<td>18</td>\n<td>18</td>\n<td>10</td>\n<td>18</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>注意:<br>结果和ORDER BY相关,默认为升序<br>如果不指定ROWS BETWEEN,默认为从起点到当前行;<br>如果不指定ORDER BY，则将分组内所有值累加;</p>\n<p>关键是理解ROWS BETWEEN含义,也叫做WINDOW子句：<br>PRECEDING：往前<br>FOLLOWING：往后<br>CURRENT ROW：当前行<br>UNBOUNDED：无界限（起点或终点）<br>UNBOUNDED PRECEDING：表示从前面的起点<br>UNBOUNDED FOLLOWING：表示到后面的终点<br>其他COUNT、AVG，MIN，MAX，和SUM用法一样。</p>\n<p>max()函数无论有没有order by 都是计算整个分区的最大值</p>\n<p>更多可参考Oracle 数据库的分析语法</p>\n</blockquote>\n<blockquote>\n<p>理解：执行逻辑为先partiton 内order by 然后sum/max/min</p>\n</blockquote>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">参考：https://dacoolbaby.iteye.com/blog/1960373</span><br><span class=\"line\">在Hive里面，可以把这一部分独立抽出来做声明。如：</span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">\tuser_id,</span><br><span class=\"line\">\tuser_type,</span><br><span class=\"line\">\tsales,</span><br><span class=\"line\">\tlog_time,</span><br><span class=\"line\">\t<span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> s,</span><br><span class=\"line\">\t<span class=\"keyword\">min</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> mi,</span><br><span class=\"line\">\t<span class=\"keyword\">max</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> ma,</span><br><span class=\"line\">\t<span class=\"keyword\">avg</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> ag</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">\tapp.order_detail </span><br><span class=\"line\">\t<span class=\"keyword\">window</span> w1 <span class=\"keyword\">as</span>(<span class=\"keyword\">distribute</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">sort</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">rows</span> <span class=\"keyword\">between</span> <span class=\"number\">2</span> <span class=\"keyword\">preceding</span> <span class=\"keyword\">and</span> <span class=\"number\">2</span> <span class=\"keyword\">following</span>) ;</span><br><span class=\"line\">\t</span><br><span class=\"line\">其中的window w1 则是抽出声明的窗口部分。</span><br><span class=\"line\"></span><br><span class=\"line\">如果在一条Hive SQL涉及到多个窗口函数的引用呢？</span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">\tuser_id,</span><br><span class=\"line\">\tuser_type,</span><br><span class=\"line\">\tsales,</span><br><span class=\"line\">\tlog_time,</span><br><span class=\"line\">\t<span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">over</span> w1 <span class=\"keyword\">as</span> s1,</span><br><span class=\"line\">\t<span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">over</span> w2 <span class=\"keyword\">as</span> s2</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">\tapp.order_detail </span><br><span class=\"line\">\t<span class=\"keyword\">window</span> w1 <span class=\"keyword\">as</span>(<span class=\"keyword\">distribute</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">sort</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">rows</span> <span class=\"keyword\">between</span> <span class=\"number\">2</span> <span class=\"keyword\">preceding</span> <span class=\"keyword\">and</span> <span class=\"number\">2</span> <span class=\"keyword\">following</span>),</span><br><span class=\"line\">\tw2 <span class=\"keyword\">as</span>(<span class=\"keyword\">distribute</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">sort</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span> <span class=\"keyword\">rows</span> <span class=\"keyword\">between</span> <span class=\"keyword\">unbounded</span> <span class=\"keyword\">preceding</span> <span class=\"keyword\">and</span> <span class=\"keyword\">current</span> <span class=\"keyword\">row</span>) ;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"range\"><a href=\"#range\" class=\"headerlink\" title=\"range\"></a>range</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">参考：</span><br><span class=\"line\">https://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive</span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">\tuser_id,</span><br><span class=\"line\">\tuser_type,</span><br><span class=\"line\">\tsales,</span><br><span class=\"line\">\tlog_time,</span><br><span class=\"line\">\t<span class=\"keyword\">sum</span>(sales) <span class=\"keyword\">OVER</span>( <span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"keyword\">unix_timestamp</span>(log_time, <span class=\"string\">'yyyy-MM-dd'</span>) <span class=\"keyword\">ASC</span> <span class=\"keyword\">RANGE</span> <span class=\"keyword\">BETWEEN</span> <span class=\"number\">86400</span> <span class=\"keyword\">PRECEDING</span> <span class=\"keyword\">and</span> <span class=\"keyword\">CURRENT</span> <span class=\"keyword\">ROW</span>) <span class=\"keyword\">as</span> <span class=\"keyword\">count</span></span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">\tapp.order_detail</span><br></pre></td></tr></table></figure>\n\n<p>结果：</p>\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>sales</th>\n<th>log_time</th>\n<th>count</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_001</td>\n<td>new</td>\n<td>9</td>\n<td>2019-07-01</td>\n<td>9</td>\n</tr>\n<tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>2019-07-04</td>\n<td>6</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>5</td>\n<td>2019-07-04</td>\n<td>6</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>4</td>\n<td>2019-07-05</td>\n<td>10</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>3</td>\n<td>2019-07-06</td>\n<td>7</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>7</td>\n<td>2019-07-07</td>\n<td>10</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>new</td>\n<td>8</td>\n<td>2019-07-17</td>\n<td>8</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>10</td>\n<td>2019-07-02</td>\n<td>10</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>2</td>\n<td>2019-07-03</td>\n<td>18</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>6</td>\n<td>2019-07-03</td>\n<td>18</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>理解：当前时间往上三天的累积数量 86400=3600*24 （一天）</p>\n</blockquote>\n<h3 id=\"2-first-value与last-value\"><a href=\"#2-first-value与last-value\" class=\"headerlink\" title=\"2.first_value与last_value\"></a>2.first_value与last_value</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_id,</span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    ROW_NUMBER() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> row_num,  </span><br><span class=\"line\">    <span class=\"keyword\">first_value</span>(user_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> max_sales_user,</span><br><span class=\"line\">    <span class=\"keyword\">first_value</span>(user_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span>) <span class=\"keyword\">as</span> min_sales_user,</span><br><span class=\"line\">    <span class=\"keyword\">last_value</span>(user_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> curr_last_min_user,</span><br><span class=\"line\">    <span class=\"keyword\">last_value</span>(user_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">asc</span>) <span class=\"keyword\">as</span> curr_last_max_user</span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail;</span><br></pre></td></tr></table></figure>\n\n<p>结果：</p>\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>row_num</th>\n<th>max_sales_user</th>\n<th>min_sales_user</th>\n<th>curr_last_min_user</th>\n<th>curr_last_max_user</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_001</td>\n<td>new</td>\n<td>7</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_001</td>\n<td>u_001</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>new</td>\n<td>6</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_010</td>\n<td>u_010</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>5</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_005</td>\n<td>u_005</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>4</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_003</td>\n<td>u_003</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>3</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_004</td>\n<td>u_004</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>2</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_009</td>\n<td>u_009</td>\n</tr>\n<tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>u_001</td>\n<td>u_008</td>\n<td>u_008</td>\n<td>u_008</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>3</td>\n<td>u_006</td>\n<td>u_007</td>\n<td>u_006</td>\n<td>u_006</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>2</td>\n<td>u_006</td>\n<td>u_007</td>\n<td>u_002</td>\n<td>u_002</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>1</td>\n<td>u_006</td>\n<td>u_007</td>\n<td>u_007</td>\n<td>u_007</td>\n</tr>\n</tbody></table>\n<h3 id=\"3-lead与lag\"><a href=\"#3-lead与lag\" class=\"headerlink\" title=\"3.lead与lag\"></a>3.lead与lag</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_id,device_id,</span><br><span class=\"line\">    <span class=\"keyword\">lead</span>(device_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales) <span class=\"keyword\">as</span> default_after_one_line,</span><br><span class=\"line\">    lag(device_id) <span class=\"keyword\">over</span> (<span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales) <span class=\"keyword\">as</span> default_before_one_line,</span><br><span class=\"line\">    <span class=\"keyword\">lead</span>(device_id,<span class=\"number\">2</span>) <span class=\"keyword\">over</span> (<span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales) <span class=\"keyword\">as</span> after_two_line,</span><br><span class=\"line\">    lag(device_id,<span class=\"number\">2</span>,<span class=\"string\">'abc'</span>) <span class=\"keyword\">over</span> (<span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales) <span class=\"keyword\">as</span> before_two_line</span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"4-RANK、ROW-NUMBER、DENSE-RANK\"><a href=\"#4-RANK、ROW-NUMBER、DENSE-RANK\" class=\"headerlink\" title=\"4.RANK、ROW_NUMBER、DENSE_RANK\"></a>4.RANK、ROW_NUMBER、DENSE_RANK</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_id,user_type,sales,</span><br><span class=\"line\">    <span class=\"keyword\">RANK</span>() <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> <span class=\"keyword\">rank</span>,</span><br><span class=\"line\">    ROW_NUMBER() <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> row_number,</span><br><span class=\"line\">    <span class=\"keyword\">DENSE_RANK</span>() <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_type <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">as</span> desc_rank</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">    order_detail;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>sales</th>\n<th>log_time</th>\n<th>rank</th>\n<th>desc_rank</th>\n<th>row_number</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_010</td>\n<td>new</td>\n<td>8</td>\n<td>2019-07-17</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>7</td>\n<td>2019-07-07</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>3</td>\n<td>2019-07-06</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>4</td>\n<td>2019-07-05</td>\n<td>4</td>\n<td>4</td>\n<td>4</td>\n</tr>\n<tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>2019-07-04</td>\n<td>5</td>\n<td>5</td>\n<td>5</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>5</td>\n<td>2019-07-04</td>\n<td>5</td>\n<td>5</td>\n<td>6</td>\n</tr>\n<tr>\n<td>u_001</td>\n<td>new</td>\n<td>9</td>\n<td>2019-07-01</td>\n<td>7</td>\n<td>6</td>\n<td>7</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>2</td>\n<td>2019-07-03</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>6</td>\n<td>2019-07-03</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>10</td>\n<td>2019-07-02</td>\n<td>3</td>\n<td>2</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<h3 id=\"5-NTILE\"><a href=\"#5-NTILE\" class=\"headerlink\" title=\"5.NTILE\"></a>5.NTILE</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">    user_type,sales,</span><br><span class=\"line\">    <span class=\"comment\">--分组内将数据分成2片</span></span><br><span class=\"line\">    NTILE(<span class=\"number\">2</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> nt2,</span><br><span class=\"line\">    <span class=\"comment\">--分组内将数据分成3片    </span></span><br><span class=\"line\">    NTILE(<span class=\"number\">3</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> nt3,</span><br><span class=\"line\">    <span class=\"comment\">--分组内将数据分成4片    </span></span><br><span class=\"line\">    NTILE(<span class=\"number\">4</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> nt4,</span><br><span class=\"line\">    <span class=\"comment\">--将所有数据分成4片</span></span><br><span class=\"line\">    NTILE(<span class=\"number\">4</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> all_nt4</span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">order</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_type</th>\n<th>sales</th>\n<th>nt2</th>\n<th>nt3</th>\n<th>nt4</th>\n<th>all_nt4</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>new</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>new</td>\n<td>3</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>new</td>\n<td>4</td>\n<td>1</td>\n<td>1</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>new</td>\n<td>5</td>\n<td>1</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>new</td>\n<td>7</td>\n<td>2</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>8</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>9</td>\n<td>2</td>\n<td>3</td>\n<td>4</td>\n<td>4</td>\n</tr>\n<tr>\n<td>old</td>\n<td>2</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>old</td>\n<td>6</td>\n<td>1</td>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>old</td>\n<td>10</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n<td>4</td>\n</tr>\n</tbody></table>\n<p>题：求取sale前20%的用户ID</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_id</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">(</span><br><span class=\"line\">    <span class=\"keyword\">select</span> </span><br><span class=\"line\">        user_id,</span><br><span class=\"line\">        NTILE(<span class=\"number\">5</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales <span class=\"keyword\">desc</span>) <span class=\"keyword\">AS</span> nt</span><br><span class=\"line\">    <span class=\"keyword\">from</span> </span><br><span class=\"line\">        order_detail</span><br><span class=\"line\">)A</span><br><span class=\"line\"><span class=\"keyword\">where</span> nt=<span class=\"number\">1</span>;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"6-CUME-DIST、PERCENT-RANK\"><a href=\"#6-CUME-DIST、PERCENT-RANK\" class=\"headerlink\" title=\"6.CUME_DIST、PERCENT_RANK\"></a>6.CUME_DIST、PERCENT_RANK</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">user_id,user_type,sales,</span><br><span class=\"line\"><span class=\"comment\">--没有partition,所有数据均为1组</span></span><br><span class=\"line\"><span class=\"keyword\">CUME_DIST</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> cd1,</span><br><span class=\"line\"><span class=\"comment\">--按照user_type进行分组</span></span><br><span class=\"line\"><span class=\"keyword\">CUME_DIST</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> cd2 </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">order_detail;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_id</th>\n<th>user_type</th>\n<th>sales</th>\n<th>cd1</th>\n<th>cd2</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>u_008</td>\n<td>new</td>\n<td>1</td>\n<td>0.1</td>\n<td>0.14285714285714285</td>\n</tr>\n<tr>\n<td>u_009</td>\n<td>new</td>\n<td>3</td>\n<td>0.3</td>\n<td>0.2857142857142857</td>\n</tr>\n<tr>\n<td>u_004</td>\n<td>new</td>\n<td>4</td>\n<td>0.4</td>\n<td>0.42857142857142855</td>\n</tr>\n<tr>\n<td>u_003</td>\n<td>new</td>\n<td>5</td>\n<td>0.5</td>\n<td>0.5714285714285714</td>\n</tr>\n<tr>\n<td>u_005</td>\n<td>new</td>\n<td>7</td>\n<td>0.7</td>\n<td>0.7142857142857143</td>\n</tr>\n<tr>\n<td>u_010</td>\n<td>new</td>\n<td>8</td>\n<td>0.8</td>\n<td>0.8571428571428571</td>\n</tr>\n<tr>\n<td>u_001</td>\n<td>new</td>\n<td>9</td>\n<td>0.9</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>u_007</td>\n<td>old</td>\n<td>2</td>\n<td>0.2</td>\n<td>0.3333333333333333</td>\n</tr>\n<tr>\n<td>u_002</td>\n<td>old</td>\n<td>6</td>\n<td>0.6</td>\n<td>0.6666666666666666</td>\n</tr>\n<tr>\n<td>u_006</td>\n<td>old</td>\n<td>10</td>\n<td>1.0</td>\n<td>1.0</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> </span><br><span class=\"line\">user_type,sales,</span><br><span class=\"line\"><span class=\"comment\">--分组内总行数      </span></span><br><span class=\"line\"><span class=\"keyword\">SUM</span>(<span class=\"number\">1</span>) <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type) <span class=\"keyword\">AS</span> s, </span><br><span class=\"line\"><span class=\"comment\">--RANK值  </span></span><br><span class=\"line\"><span class=\"keyword\">RANK</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> r,    </span><br><span class=\"line\"><span class=\"keyword\">PERCENT_RANK</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> pr,</span><br><span class=\"line\"><span class=\"comment\">--分组内     </span></span><br><span class=\"line\"><span class=\"keyword\">PERCENT_RANK</span>() <span class=\"keyword\">OVER</span>(<span class=\"keyword\">PARTITION</span> <span class=\"keyword\">BY</span> user_type <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> sales) <span class=\"keyword\">AS</span> prg </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">order_detail;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_type</th>\n<th>sales</th>\n<th>s</th>\n<th>r</th>\n<th>pr</th>\n<th>prg</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>new</td>\n<td>1</td>\n<td>7</td>\n<td>1</td>\n<td>0.0</td>\n<td>0.0</td>\n</tr>\n<tr>\n<td>new</td>\n<td>3</td>\n<td>7</td>\n<td>3</td>\n<td>0.2222222222222222</td>\n<td>0.16666666666666666</td>\n</tr>\n<tr>\n<td>new</td>\n<td>4</td>\n<td>7</td>\n<td>4</td>\n<td>0.3333333333333333</td>\n<td>0.3333333333333333</td>\n</tr>\n<tr>\n<td>new</td>\n<td>5</td>\n<td>7</td>\n<td>5</td>\n<td>0.4444444444444444</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>new</td>\n<td>7</td>\n<td>7</td>\n<td>7</td>\n<td>0.6666666666666666</td>\n<td>0.6666666666666666</td>\n</tr>\n<tr>\n<td>new</td>\n<td>8</td>\n<td>7</td>\n<td>8</td>\n<td>0.7777777777777778</td>\n<td>0.8333333333333334</td>\n</tr>\n<tr>\n<td>new</td>\n<td>9</td>\n<td>7</td>\n<td>9</td>\n<td>0.8888888888888888</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>old</td>\n<td>2</td>\n<td>3</td>\n<td>2</td>\n<td>0.1111111111111111</td>\n<td>0.0</td>\n</tr>\n<tr>\n<td>old</td>\n<td>6</td>\n<td>3</td>\n<td>6</td>\n<td>0.5555555555555556</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>old</td>\n<td>10</td>\n<td>3</td>\n<td>10</td>\n<td>1.0</td>\n<td>1.0</td>\n</tr>\n</tbody></table>\n<h2 id=\"增强的聚合-Cube和Grouping-和Rollup\"><a href=\"#增强的聚合-Cube和Grouping-和Rollup\" class=\"headerlink\" title=\"增强的聚合 Cube和Grouping 和Rollup\"></a>增强的聚合 Cube和Grouping 和Rollup</h2><p>这几个分析函数通常用于OLAP中，不能累加，而且需要根据不同维度上钻和下钻的指标统计，比如，分小时、天、月的UV数。</p>\n<p><strong>GROUPING SETS</strong><br>在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行UNION ALL,<br>其中的GROUPING__ID，表示结果属于哪一个分组集合。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"keyword\">count</span>(user_id) <span class=\"keyword\">as</span> pv,</span><br><span class=\"line\">    GROUPING__ID </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,sales</span><br><span class=\"line\"><span class=\"keyword\">GROUPING</span> <span class=\"keyword\">SETS</span>(user_type,sales) </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> </span><br><span class=\"line\">    GROUPING__ID;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"keyword\">count</span>(user_id) <span class=\"keyword\">as</span> pv,</span><br><span class=\"line\">    GROUPING__ID </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,sales</span><br><span class=\"line\"><span class=\"keyword\">GROUPING</span> <span class=\"keyword\">SETS</span>(user_type,sales,(user_type,sales)) </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> </span><br><span class=\"line\">    GROUPING__ID;</span><br></pre></td></tr></table></figure>\n\n<p><strong>CUBE</strong><br>根据GROUP BY的维度的所有组合进行聚合。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"keyword\">count</span>(user_id) <span class=\"keyword\">as</span> pv,</span><br><span class=\"line\">    GROUPING__ID </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,sales</span><br><span class=\"line\"><span class=\"keyword\">WITH</span> <span class=\"keyword\">CUBE</span> </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> </span><br><span class=\"line\">    GROUPING__ID;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>user_type</th>\n<th>log_time</th>\n<th>pv</th>\n<th>grouping__id</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NULL</td>\n<td>NULL</td>\n<td>10</td>\n<td>0</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-01</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-02</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-03</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-04</td>\n<td>2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-05</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-06</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-07</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>2019-07-17</td>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>new</td>\n<td>NULL</td>\n<td>7</td>\n<td>1</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-01</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-04</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-05</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-06</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-07</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>new</td>\n<td>2019-07-17</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>old</td>\n<td>NULL</td>\n<td>3</td>\n<td>1</td>\n</tr>\n<tr>\n<td>old</td>\n<td>2019-07-02</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>old</td>\n<td>2019-07-03</td>\n<td>2</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<p><strong>ROLLUP</strong><br>是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合。</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    user_type,</span><br><span class=\"line\">    sales,</span><br><span class=\"line\">    <span class=\"keyword\">count</span>(user_id) <span class=\"keyword\">as</span> pv,</span><br><span class=\"line\">    GROUPING__ID </span><br><span class=\"line\"><span class=\"keyword\">from</span> </span><br><span class=\"line\">    order_detail</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> </span><br><span class=\"line\">    user_type,sales</span><br><span class=\"line\"><span class=\"keyword\">WITH</span> <span class=\"keyword\">ROLLUP</span> </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> </span><br><span class=\"line\">    GROUPING__ID;</span><br></pre></td></tr></table></figure>\n\n<p>题：</p>\n<ol>\n<li><p>设备总共使用天数和最早、最晚使用的用户和时间</p>\n</li>\n<li><p>设备连续使用最长的天数</p>\n</li>\n<li><p>前三天的销售额，后三天的销售额？</p>\n</li>\n<li><p>每5分钟统计前一小时的在线人数</p>\n</li>\n</ol>\n<hr>\n<p>参考：</p>\n<ol>\n<li><strong><a href=\"https://blog.csdn.net/scgaliguodong123_/article/details/60881166\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/scgaliguodong123_/article/details/60881166</a></strong> </li>\n<li><strong><a href=\"https://blog.csdn.net/scgaliguodong123_/article/details/60135385\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/scgaliguodong123_/article/details/60135385</a></strong></li>\n<li><strong><a href=\"https://dacoolbaby.iteye.com/blog/1960373\" target=\"_blank\" rel=\"noopener\">https://dacoolbaby.iteye.com/blog/1960373</a></strong></li>\n<li><strong><a href=\"https://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/questions/30809097/sum-over-a-date-range-per-group-in-hive</a></strong></li>\n<li><a href=\"https://blog.csdn.net/Abysscarry/article/details/81408265\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/Abysscarry/article/details/81408265</a></li>\n<li><a href=\"http://lxw1234.com/archives/2015/07/367.htm\" target=\"_blank\" rel=\"noopener\">http://lxw1234.com/archives/2015/07/367.htm</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics</a></li>\n</ol>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjy01ec5l0008m5878li7p6sf","category_id":"cjy01ec5j0005m5873mww3gt3","_id":"cjy01ec5q000em587nesx4mer"},{"post_id":"cjy01ec5f0003m58781hbbf8i","category_id":"cjy01ec5j0005m5873mww3gt3","_id":"cjy01ec5r000im587mcpaz3pd"},{"post_id":"cjy01ec5m0009m587iihmd9p3","category_id":"cjy01ec5j0005m5873mww3gt3","_id":"cjy01ec5r000km587t63fj3x6"},{"post_id":"cjy01ec5h0004m587axe0b376","category_id":"cjy01ec5j0005m5873mww3gt3","_id":"cjy01ec5s000mm587mmjyvlnh"},{"post_id":"cjy01ec5l0007m587xk3ocp4b","category_id":"cjy01ec5j0005m5873mww3gt3","_id":"cjy01ec5s000nm5871nx0xzsn"}],"PostTag":[{"post_id":"cjy01ec5l0008m5878li7p6sf","tag_id":"cjy01ec5k0006m5873phnh7zp","_id":"cjy01ec5q000cm587s8p73eyw"},{"post_id":"cjy01ec5f0003m58781hbbf8i","tag_id":"cjy01ec5k0006m5873phnh7zp","_id":"cjy01ec5q000dm5873ctiici6"},{"post_id":"cjy01ec5m0009m587iihmd9p3","tag_id":"cjy01ec5k0006m5873phnh7zp","_id":"cjy01ec5r000hm587adklzq9f"},{"post_id":"cjy01ec5h0004m587axe0b376","tag_id":"cjy01ec5k0006m5873phnh7zp","_id":"cjy01ec5r000jm587k0nn2rzv"},{"post_id":"cjy01ec5l0007m587xk3ocp4b","tag_id":"cjy01ec5k0006m5873phnh7zp","_id":"cjy01ec5s000lm58729yonxlc"}],"Tag":[{"name":"sql","_id":"cjy01ec5k0006m5873phnh7zp"}]}}